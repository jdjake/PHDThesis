%!TEX root = ../main.tex

\subsection{Summary of Proof} To conclude the introductory sections of the paper, we summarize the various methods which occur in the proof of Theorem \ref{CpVersionOfTheorem}. In Lemma \ref{lowerboundlemma} of Section \ref{PrelimSetup}, we prove that
%
\begin{equation} \label{nonoppositeinequality}
    \sup\nolimits_R \| m_R(P) \|_{L^p(M) \to L^p(M)} \gtrsim C_p(m).
\end{equation}
%
We obtain \eqref{nonoppositeinequality} via a relatively simple argument, as might be expected from the discussion in Section \ref{transference}. The main difficulty is obtaining the opposite inequality
%
\begin{equation} \label{oppositeinequality}
    \sup\nolimits_R \| m_R(P) \|_{L^p(M) \to L^p(M)} \lesssim C_p(m).
\end{equation}
%
Unlike in the study of Euclidean multipliers, the behavior of the multipliers $m_R(P)$ on a \emph{compact} manifold for $R \lesssim 1$ is relatively benign. In Lemma \ref{lowjLemma} of Section \ref{PrelimSetup} that same section, another simple argument shows
%
\begin{equation} \label{TrivialLowFrequencyBound}
    \sup\nolimits_{R \leq 1} \left\| m_R(P) \right\|_{L^p(M) \to L^p(M)} \lesssim C_p(m).
\end{equation}
%
In that section we also setup notation for the rest of the paper. As a consequence of Proposition \ref{TjbLemma}, stated at the end of Section \ref{PrelimSetup},% but proved throughout Sections \ref{estimatesforwavepackets}, \ref{regime1firstsection} and \ref{regime2finalsection},
we can conclude that
%
\begin{equation} \label{dyadicMainReulst}
    \sup\nolimits_{R \geq 1} \| m_R(P) \|_{L^p(M) \to L^p(M)} \lesssim C_p(m),
\end{equation}
%
The upper bound \eqref{dyadicMainReulst} requires a more in depth analysis than \eqref{TrivialLowFrequencyBound}. Using the fact that $m$ is regulated, %so that the Fourier inversion formula can be interpreted in a pointwise sense,
we apply the Fourier inversion formula to write
%
\begin{equation}
    m_R(P) = \int_{-\infty}^\infty R \widehat{m}(Rt) e^{2 \pi i t P}\; dt,
\end{equation}
%
where $e^{2 \pi i t P}$ are wave propogators, which as $t$ varies, give 
%
%\[ e^{2 \pi i t P} = \sum\nolimits_\lambda e^{2 \pi i t \lambda} \mathcal{Q}_\lambda \]
%
solutions to the half-wave equation $\partial_t = 2 \pi i P$ on $M$. Studying $m_R(P)$ thus reduces to studying certain 'weighted averages' of the propagators $\{ e^{2 \pi i t P} \}$. We prove Proposition \ref{TjbLemma} using several new estimates for understanding these averages, including:
%
\begin{itemize}
    \item[(A)] Quasi-orthogonality estimates for averages of solutions to the half-wave equation on $M$, discussed in Section \ref{estimatesforwavepackets}, which arise from a connection between the theory of pseudodifferential operators whose principal symbols satisfy the curvature condition of Theorem \ref{CpVersionOfTheorem}, and Finsler metrics on the manifold $M$.

    \item[(B)] Variants of the density-decomposition arguments first used in \cite{HeoandNazarovandSeeger}, described in Section \ref{regime1firstsection}, which apply the quasi-orthogonality estimates obtained in Section \ref{estimatesforwavepackets} with a geometric argument which controls the `small time behavior' of solutions to the half-wave equation.

    \item[(C)] A new strategy to reduce the `large time behavior' of the half-wave equation to an endpoint local smoothing inequality for the half-wave equation on $M$, described in Section \ref{regime2finalsection}.
\end{itemize}
%
Equations \eqref{TrivialLowFrequencyBound} and \eqref{dyadicMainReulst} immediately imply \eqref{oppositeinequality}, which together with \eqref{nonoppositeinequality} completes the proof of Theorem \ref{CpVersionOfTheorem}.


\section{Preliminary Setup} \label{PrelimSetup}

%
%
%\[ M_R = 2 \int_0^\infty R \widehat{h}(Rt) \cos(2 \pi t P)\; dt, \]
%
%where
%
%\[ \cos(2 \pi t P) = \sum_\lambda \cos(2 \pi t \lambda) \mathcal{P}_\lambda \]
%
%is the Fourier multiplier giving solutions to the wave equation
%
%\[ \partial_t^2 u + (2 \pi P)^2 = 0. \]
%
%
%
%We begin by listing some fixed time bounds for the propogators $\{ e^{2 \pi i t P} \}$. The propogators are unitary, so for $t \in \RR$,
%
%\[ \| e^{2 \pi i t P} f \|_{L^2(S^d)} = \| f \|_{L^2(S^d)}. \]
%
%For $1 < p < \infty$, we have
%
%\[ \| e^{2 \pi i t P} f \|_{L^p(S^d)} \lesssim_{p,t_0} \| f \|_{L^p_{s_p}(S^d)} \quad\text{uniformly for $t > 0$}, \]
%
%which follows from Corollary 6.2.3 of \cite{Sogge}, and the periodicity of the wave propogators $\{ e^{2 \pi i t P} \}$.
%

We now begin with the details of the proof of Theorem \ref{CpVersionOfTheorem}, following the path laid out at the end of the introduction. To begin with, we prove the lower bound \eqref{nonoppositeinequality}.

\begin{lemma} \label{lowerboundlemma}
    Suppose $M$ is a compact manifold of dimension $d$, and $P$ is a classical elliptic self-adjoint pseudodifferential operator of order one on $M$ satisfying the assumptions of Theorem \ref{CpVersionOfTheorem}. Then for $1 < p < 2(d-1)/(d+1)$, and for any regulated function $m: [0,\infty) \to \CC$,
    %
    \begin{equation}
        \sup\nolimits_R \| m_R(P) \|_{L^p(M) \to L^p(M)} \gtrsim C_p(m).
    \end{equation}
\end{lemma}
\begin{proof}
Define the quasiradial Fourier multiplier $m( p(x_0,D) )$ on $\RR^d$ by
%
\begin{equation}
    m( p(x_0,D) ) f(x) = \int_{\RR^d} m(p(x_0,\xi)) \widehat{f}(\xi) e^{2 \pi i \xi \cdot x}\; d\xi.
\end{equation}
%
A result of Mitjagin \cite{Mitjagin} %\footnote{Mitjagin's original proof in \cite{Mitjagin} was published in German. See \cite{KenigStantonTomas} for a proof of the inequality written in the English mathematics literature.}
states that for any regulated $m$, and any $x_0 \in M$,
%
\begin{equation}
\label{mitjagininequality}
    \sup\nolimits_R \big\| m_R(P) \big\|_{L^p(M) \to L^p(M)} \gtrsim \big\| m \big( p(x_0,D) \big) \big\|_{L^p(\RR^d) \to L^p(\RR^d)}.
\end{equation}
%
Theorem 1.1 of \cite{KimQuasiradial} implies that
%
\begin{equation} \label{kiminequality}
    \big\| m \big( p(x_0,D) \big) \big\|_{L^p(\RR^d) \to L^p(\RR^d)} \sim C_p(m) \quad\text{and}\quad \big\| m\big( |D| \big) \big\|_{L^p(\RR^d) \to L^p(\RR^d)} \sim C_p(m).
\end{equation}
%
Putting together \eqref{mitjagininequality} and \eqref{kiminequality} then proves the claim. %, and \eqref{heonasainequality} proves the claim.
\end{proof}

In the remainder of the article, we focus on proving the upper bound \eqref{oppositeinequality}. For this purpose, we may assume without loss of generality that $\text{supp}(m) \subset [1,2]$. Let us briefly justify why. 
 Suppose we have proved
%
\begin{equation} \label{specialcasedyadicCp}
    \sup\nolimits_R \| m_R(P) \|_{L^p \to L^p} \sim C_p(m) \quad\text{if $\text{supp}(m) \subset [1,2]$.}
\end{equation}
%
Let $m$ be an arbitrary function whose support is a compact subset of $(0,\infty)$. Fix $\chi \in C_c^\infty(0,\infty)$ with $\text{supp}(\chi) \subset [1,2]$ and with $\sum \chi( \cdot / 2^j ) = 1$. If we set $m_j(\lambda) = \chi(\lambda) m(2^j \lambda)$, then $\text{supp}(m_j) \subset [1,2]$ for all $j$, and $m(\lambda) = \sum m_j(\lambda/2^j)$. Then
%
\begin{equation}
    \widehat{m}_j(t) = \widehat{\chi} * 2^{-j} \widehat{m}( \cdot / 2^j ),
\end{equation}
%
% ( int_t [<t>^a | int_s chi^(s) 2^{-j} m^((t-s)/2^j) |]^p )^{1/p}
%   << int_s ( int_t | <t>^a chi^(s) 2^{-j} m^((t-s)/2^j) |^p )^{1/p}
%   = 2^j int_s chi^(2^j s) ( int_t | <2^j t>^a m^(t-s) |^p )^{1/p}
%   = 2^j int_s chi^(2^j s) ( int_t |[<2^j t>^a / <t-s>^a ] <t - s>^a m^(t-s)|^p )^{1/p}
%   = 2^j int_s chi^(2^j s) [max_t <2^j t>^a / <t - s>^a] C_p(m)
%   << C_p(m) 2^j int_s chi^(2^j s) 2^{ja} <s>^a
%   << C_p(m) 2^{ja}
%
% s = 0: <2^j t>^a / <t>^a      2^{ja}
% Works also for s <= 1
%       For s >= 1      (2^j s)^a
%   = C_p(m) 2^j int_s chi^(2^j s)
and the rapid decay of $\widehat{\chi}$ implies that $C_p(m_j) \lesssim_j C_p(m)$. Thus \eqref{specialcasedyadicCp} implies
%
\begin{equation}
\begin{split}
    &\sup\nolimits_R \| m_j(P/2^j R) \|_{L^p(M) \to L^p(M)}\\
    &\quad\quad= \sup\nolimits_R \| m_j(P/R) \|_{L^p(M) \to L^p(M)} \lesssim_j C_p(m_j) \lesssim C_p(m).
\end{split}
\end{equation}
%
Now the triangle inequality, summing over finitely many $j$, implies
%
\begin{equation}
    \sup\nolimits_R \| m(P/R) \|_{L^p(M) \to L^p(M)} \lesssim \sum\nolimits_j \sup\nolimits_R \| m_j(P/2^j R) \|_{L^p(M) \to L^p(M)} \lesssim C_p(m),
\end{equation}
%
which proves Theorem \ref{CpVersionOfTheorem} in general.

Given a multiplier $m$ supported on $[1/2,2]$, we define $T_R = m(P/R)$. We fix some geometric constant $\varepsilon_M \in (0,1)$, matching the constant given in the statement of Theorem \ref{TjbLemma}. Our goal is then to prove inequalities \eqref{TrivialLowFrequencyBound} and \eqref{dyadicMainReulst}. Proving \eqref{TrivialLowFrequencyBound} is simple because the operators $T_R$ are smoothing operators, uniformly for $0 < R < 1$, and $M$ is a compact manifold.

\begin{lemma} \label{lowjLemma}
    Let $M$ be a compact $d$-dimensional manifold, let $P$ be a classical elliptic self-adjoint pseudodifferential operator of order one. If $1 < p < 2d/(d+1)$, and if $m: [0,\infty) \to \CC$ is a regulated function with $\text{supp}(m) \subset [1/2,2]$, then
    %
    \begin{equation}
        \sup\nolimits_{R \leq 1} \| T_R \|_{L^p(M) \to L^p(M)} \lesssim C_p(m).
    \end{equation}
\end{lemma}
\begin{proof}
    Let $T_R = m_R(P)$. The set $\Lambda_P \cap [0,2]$ is finite. For each $\lambda \in \Lambda_P$, choose a finite orthonormal basis $\mathcal{E}_\lambda$. Then we can write
    %
    \begin{equation}
        T_R = \sum\nolimits_{\lambda \in \Lambda_P} \sum\nolimits_{e \in \mathcal{E}_\lambda} \langle f, e \rangle e.
    \end{equation}
    %
    Since $\mathcal{V}_\lambda \subset C^\infty(M)$, H\"{o}lder's inequality implies
    %
    \begin{equation}
    \begin{split}
        \| \langle f, e \rangle e \|_{L^p(M)} &\leq \| f \|_{L^p(M)} \| e \|_{L^{p'}(M)} \| e \|_{L^p(M)} \lesssim_\lambda \| f \|_{L^p(M)}.
    \end{split}
    \end{equation}
    %
    But this means that
    %
    \begin{equation}
        \left\| T_R f \right\|_{L^p(M)} \leq \sum\nolimits_{\lambda \in \Lambda_P \cap [0,2]} \sum\nolimits_{e \in \mathcal{E}_\lambda} |m(\lambda/R)| \| \langle f, e \rangle e \|_{L^p(M)} \lesssim \| m \|_{L^\infty[0,\infty)}.
    \end{equation}
    %
    The proof is completed by noting that for $1 < p < 2d/(d+1)$, the Sobolev embedding theorem guarantees that $\| m \|_{L^\infty[0,\infty)} \lesssim C_p(m)$.
\end{proof}

Lemma \ref{lowjLemma} is relatively simple to prove because the operators $\{ T_R: 0 \leq R \leq 1 \}$ are supported on a common, finite dimensional subspace of $L^2(M)$. We thus did not have to perform any analysis of the interactions between different eigenfunctions, because the triangle inequality is efficient enough to obtain a finite bound. For $R > 1$, things are not so simple, and so proving \eqref{dyadicMainReulst} requires a more refined analysis of multipliers than \eqref{TrivialLowFrequencyBound}. A standard method, originally due to H\"{o}rmander \cite{Hormander2}, is to apply the Fourier inversion formula to write, for a regulated function $h: \RR \to \CC$,
%
\begin{equation}
    h(P) = \int_{-\infty}^\infty \widehat{h}(t) e^{2 \pi i t P}\; dt,
\end{equation}
%
where $e^{2 \pi i t P}$ is the spectral multiplier operator on $M$ which, as $t$ varies, gives solutions to the half-wave equation $\partial_t = 2 \pi i P$ on $M$. Thus we conclude that
%
\begin{equation}
    T_R = \int_{-\infty}^\infty R \widehat{m}(R t) e^{2 \pi i t P}\; dt.
\end{equation}
%
Writing $T_R$ in this form, we are  lead to obtain estimates for averages of the wave equation tested against a relatively non-smooth function $R \widehat{m}(Rt)$. % We begin pursue these estimates in the next section of the paper. In the remainder of this section, we introduce the required notation to carry out this task.

Fix a bump function $q \in C_c^\infty(\RR)$ with $\supp(q) \subset [1/2,4]$ and $q(\lambda) = 1$ for $\lambda \in [1,2]$, and define $Q_R = q(P/R)$. % has range contained in the finite dimensional subspace $V_R$ of $C^\infty(M)$ spanned by eigenfunctions of $P$ with eigenvalues in $[2^{j-2},2^{j+2}]$. Since $P$ is elliptic, it is often a useful heuristic that elements of $V_R$ are `frequency localized' at a scale $R$.
We write
%
\begin{equation}
    T_R = Q_R \circ T_R \circ Q_R = \int_{\RR} R \widehat{m}(R t) (Q_R \circ e^{2 \pi i t P} \circ Q_R)\; dt,
\end{equation}
%
and view the operators $(Q_R \circ e^{2 \pi i t P} \circ Q_R)$ as `frequency localized' wave propagators.

Replacing the operator $P$ by $aP + b$ for appropriate $a,b \in \RR$, we may assume without loss of generality that all eigenvalues of $P$ are integers. It follows that $e^{2 \pi i (t + n) P} = e^{2 \pi i t P}$ for any $t \in \RR$ and $n \in \ZZ$. Let $I_0$ denote the interval $[-1/2,1/2]$. We may then write
%
\begin{equation}
    T_R = \int_{I_0} b_R(t) (Q_R \circ e^{2 \pi i tP} \circ Q_R)\; dt,
\end{equation}
%
where $b_R: I_0 \to \CC$ is the periodic function
%
\begin{equation}
    b_R(t) = \sum\nolimits_{n \in \ZZ} R \widehat{m}(R (t + n)).
\end{equation}
% Sphere 1, but we scale is by 1/2pi
% Sphere of radius 1/2 pi, has sectional curvatures kappa = 2 pi
% so 1/4
We split our analysis of $T_R$ into two regimes: regime $\text{I}$ and regime $\text{II}$. In regime $\text{I}$, we analyze the behaviour of the wave equation over times $0 \leq |t| \leq \varepsilon_M$ by decomposing this time interval into length $1/R$ pieces, and analyzing the interactions of the wave equations between the different intervals. In regime $\text{II}$, we analyze the behaviour of the wave equation over times $\varepsilon_M \leq |t| \leq 1$. Here we need not perform such a decomposition, since the boundedness of $C_p(m)$ gives better control on the function $b_R$ over these times.
%Heuristically, the result is just a discretization of the condition that $C_p(m) < \infty$, but using the additional fact that $b_j$ was obtained from a periodization of the Fourier transform of a function with `frequency support' on an annulus of radius $2^j$, and thus locally constant at a scale $1/2^j$. This allows us to replace $L^p$ norms with $L^1$ norms on intervals of length $1/2^j$ without incurring any loss.

\begin{lemma} \label{decompositionLemma}
    Fix $\varepsilon > 0$. Let $\mathcal{T}_R = \ZZ/R \cap [-\varepsilon, \varepsilon]$ and define $I_t = [t - 1/R, t + 1/R]$. For a function $m: [0,\infty) \to \CC$, define a periodic function $b: I_0 \to \CC$ by setting
    %
    \begin{equation}
        b(t) = \sum\nolimits_{n \in \ZZ} R \widehat{m}(R(t + n)).
    \end{equation}
    %
    Then we can write $b = \left( \sum\nolimits_{t_0 \in \mathcal{T}_R} b_{t_0}^I \right) + b^{II}$, where
    %
    \begin{equation} \supp(b_{t_0}^I) \subset I_{t_0} \quad\text{and}\quad \supp(b_R^{II}) \subset I_0 \smallsetminus [-\varepsilon,\varepsilon].
    \end{equation}
    %
    Moreover, we have
    %
    \begin{equation} \label{DKAPDKAWIODJAWOI}
        \left( \sum\nolimits_{t_0 \in \mathcal{T}_R} \Big[ \| b^I_{t_0} \|_{L^p(I_0)} \langle R t_0 \rangle^{\alpha(p)} \Big]^p \right)^{1/p} \lesssim R^{1/p'} C_p(m)
    \end{equation}
        %
        and
        %
    \begin{equation} \label{DWAIOJDAOIWDJWAIODJIOJD}
        \| b^{II} \|_{L^p(I_0)} \lesssim R^{1/p' - \alpha(p)} C_p(m).
    \end{equation}
\end{lemma}

The proof is a simple calculation which we relegate to the appendix.

%\begin{remark} \label{constantspecificationremark}
%    \normalfont
%    The choice of $\varepsilon_M$ is made so that the geometric arguments of Section \ref{estimatesforwavepackets} work. Namely, $\varepsilon_M$ needs to be small enough so that geodesics normal to a family of hypersurfaces contained within a set $U$ with diameter $\varepsilon_M$ do not intersect. If all sectional curvatures of the manifold $M$ are bounded by $\kappa$, then we may pick $\varepsilon_M$ to be any value smaller than $0.2 \pi \kappa^{-1/2}$.  Given our choice of $\varepsilon_M$, we then pick $C_M$ to be any constant greater than or equal to $\log(2 / \varepsilon_M)$. This is chosen precisely so that for $j > C_M$, $\varepsilon_M \geq 2^{1-j}$, so that the support restriction of $b_j^{II}$ is possible given the assumption on the supports of the functions $\{ b_j^I \}$.
%    In particular, the sphere with radius $1/2\pi$ (and unit period geodesic flow) has constant sectional curvature $(2 \pi)^2$, so here we can choose $\varepsilon_M$ to be any value smaller than $0.1$. In particular, we can choose $\varepsilon_M = 0.05$ and $C_M = 2$.
%\end{remark}
The following proposition, a kind of $L^p$ square root cancellation bound, implies \eqref{dyadicMainReulst} once we take Lemma \ref{decompositionLemma} into account. % and comparing \eqref{DKAPDKAWIODJAWOI} with \eqref{ejqwoifjeoifjwqoifjwqoi} and \eqref{DWAIOJDAOIWDJWAIODJIOJD} with \eqref{DPOIJAOIWDJQWIOFJQOIVJIEOVNFNJNVNV},
Since we already proved \eqref{TrivialLowFrequencyBound}, proving this proposition completes the proof of Theorem \ref{CpVersionOfTheorem}.

\begin{prop} \label{TjbLemma}
    Let $P$ be a classical elliptic self-adjoint pseudodifferential operator of order one on a compact manifold $M$ of dimension $d$ satisfying the assumptions of Theorem \ref{CpVersionOfTheorem}. Fix $R > 0$ and suppose $1 < p < 2 (d-1)/(d+1)$. Then there exists $\varepsilon_M > 0$ with the following property. Consider any function $b: I_0 \to \CC$, and suppose we can write $b = \sum\nolimits_{t_0 \in \mathcal{T}_R} b_{t_0}^I + b^{II}$, where $\supp(b_{t_0}^I) \subset I_{t_0}$ and $\supp(b_R^{II}) \subset I_0 \smallsetminus [-\varepsilon_M,\varepsilon_M]$. Define operators $T^I = \sum\nolimits_{t_0 \in \mathcal{T}_R} T^I_{t_0}$ and $T^{II}$, where
    %
    \[ T_{t_0}^I = \int b_{t_0}^I(t) ( Q_R \circ e^{2 \pi i tP} \circ Q_R )\; dt\ \ \text{and}\ \ T^{II} = \int b^{II}(t) ( Q_R \circ e^{2 \pi i tP} \circ Q_R)\; dt. \]
    %
    Then
    %
    \begin{equation} \label{ejqwoifjeoifjwqoifjwqoi}
        \| T^I \|_{L^p \to L^p} \lesssim R^{-1/p'} \left( \sum\nolimits_{t_0 \in \mathcal{T}_R} \Big[ \| b^I_{t_0} \|_{L^p(I_0)} \langle R t_0 \rangle^{\alpha(p)} \Big]^{p} \right)^{1/p}
    \end{equation}
    %
    and
    %
    \begin{equation} \label{DPOIJAOIWDJQWIOFJQOIVJIEOVNFNJNVNV}
        \| T^{II} \|_{L^p \to L^p} \lesssim R^{\alpha(p) - 1/p'} \| b^{II} \|_{L^p(I_0)}.
    \end{equation}
\end{prop}

Proposition \ref{TjbLemma} splits the main bound of the paper into two regimes: regime $\text{I}$ and regime $\text{II}$. Noting that we require weaker bounds in \eqref{DPOIJAOIWDJQWIOFJQOIVJIEOVNFNJNVNV} than in \eqref{ejqwoifjeoifjwqoifjwqoi}, the operator $T^{II}$ will not require as refined an analysis as for the operator $T^I$, and we obtain bounds on $T^{II}$ by a reduction to an endpoint local smoothing inequality in Section \ref{regime2finalsection}. On the other hand, to obtain more refined estimates for the $L^p$ norms of quantities of the form $f = T^I u$, we consider a decompositions of the form $u = \sum_{x_0} u_{x_0}$, where $u_{x_0}: M \to \CC$ is supported on a ball $B(x,1/R)$ of radius $1/R$ centered at $x_0$. We then have $\smash{f = \sum\nolimits_{(x_0,t_0)} f_{x_0,t_0}}$, where $f_{x_0,t_0} = T^I_{t_0} u_{x_0}$. To control $f$ we must establish an $L^p$ square root cancellation bound for the functions $\{ f_{x_0,t_0} \}$. In the next section, we study the $L^2$ quasi-orthogonality of these functions, which we use as a starting point to obtain the required square root cancellation.

% Finsler geometry arises in our problem because the principal symbol $p: T^* M \to [0,\infty)$ acts as a Minkowski norm on the cotangent spaces $T^* M$%, and thus induces a Finsler metric on $M$ by taking the dual Minkowski norm. % $F(x,v) = \sup\nolimits_{\xi \in S_x^*} \xi(v)$. % A very similar theory of geodesics and curvature can be developed for Finsler manifolds as for Riemannian manifolds. The main difference is that a geodesic travelling from one point $p$ to another point $q$ need not be a geodesic when the orientation of the geodesic is reversed. Thus the induced distance function $d_+$ on a Finsler manifold \emph{need not be symmetric}.

% given a homogeneous function $F: TM \to [0,\infty)$ which induces a smoothly varying \emph{Minkowski norm} on the tangent spaces of $M$. 

%A similar theory of geodesics and curvature can be developed for Finsler manifolds, and the geodesic flow on $T^* M$ induced by the Finsler metric $M$ is precisely the Hamiltonian flow induced by the function $p$. The most important difference arising in our calculations between Riemannian and Finsler geometry is that the shortest geodesic travelling from a point $x_0 \in M$ to a point $x_1 \in M$ need not agree with the shortest geodesic travelling from $x_1$ to $x_0$; we call the former a \emph{forward geodesic} from $x_0$ to $x_1$, and the latter a \emph{backward geodesic} for $x_0$ to $x_1$, and denote the lengths of these geodesics by $d_+(x_0,x_1)$ and $d_-(x_0,x_1)$ respectively; the functions $d_+$ and $d_-$ obey the triangle inequality, but are \emph{not symmetric}. We make no assumption that the reader of this paper is familiar with Finsler geometry in this paper, describing the necessary results we need, and giving explicit citations for those who wish to know more details. 

\begin{comment}

Other than this theory, the only non-standard topic in Finsler geometry we will use is a theory of \emph{approximations to normal coordinates}. In Finsler geometry, for each $x \in M$ we can use geodesics to define a diffeomorphism of a neighborhood of the origin in $T_x M$ into $M$. This map will be smooth away from the origin. However, unlike in Riemannian geometry, normal coordinates on a Finsler manifold are in general only $C^1$ at the origin, which can cause issues. Fortunately in our argument we will only need to consider an approximate normal coordinate system which is $C^\infty$, based on a method of Douglas and Thomas \cite{Douglas,Thomas}. One can find a more modern exposition and extension of the method in \cite{Pfeifer}. For each $(x,v) \in TM - 0$, and each $y \in T_p M$, we consider the system of ordinary differential equations
%
\begin{align*}
    \frac{d^2 c^i}{dt^2} = - \sum\nolimits_{a,b} \gamma^i_{ab}(c,s) \dot{c}^a \dot{c}^b \quad\text{and}\quad \dot{s} = - \sum\nolimits_{a,b} \gamma^i_{ab}(c,s) \dot{c}^a s^b,
\end{align*}
%
where $c(0) = x$, $\dot{c}(0) = y$, and $s(0) = v$. If $y$ is suitably close to the origin, we can guarantee that $c$ and $s$ exist on $[0,1]$ with $s(t) \neq 0$ for all $t \in [0,1]$. Thus the coefficients of the ordinary differential equation are \emph{smooth} on a neighbourhood of the trajectories of $c$ and $s$, which gives us \emph{smooth dependence on the initial data}. By compactness, in some pre-compact coordinate system $(x,U)$, we can find an open ball $B \subset \RR^d$ such that for all $x \in U$, all $w \in B$, and all $v \in T_x M$ with $F(x,v) = 1$, the property above is true. But if $SM = \{ (x,v) \in TM : F(x,v) = 1 \}$ denotes the sphere bundle, this means we can find an open precompact set $U \subset SM \otimes TM$ containing $SM \times 0$ and a smooth map $F: U \to M$ such that for each $(x,v) \in SM$, $F(x,v,\cdot)$ is a $C^\infty$ diffeomorphism. The inverse is thus a coordinate system $y_{x,v}$. It has the property that the geodesic starting at $x$ with tangent vector will be mapped to a straight line in the coordinate system, but not necessarily the other geodesics starting at $x$. Thus in this coordinate system $G(0,v) = 0$.
% N^a_b(0,v)
% Thus
% 0 = - \sum\nolimits_{a,b} \gamma^i_{ab}(tv,v) v^a v^b \quad\text{for $1 \leq i \leq d$},

we can find $B_x \subset T_x M$ such that this is true for all $y \in B_x$ and $v$ with $F(x,v) = 1$, and in coordinates, the. But this implies that the coefficients of the ordinary differential equations have \emph{smooth coefficients}, and so we get smooth initial dependence on the data. Choosing an open precompact set $U \subset \bigcup_x \{ x \} \times B_x \times S_x$, we get a smooth map $F: U \to M$ which restricts to a diffeomorphism $F_{x,v}: B_x$ for each $x \in M$ and $v \in T_x M$ with $F(x,v) = 1$.

A proof of these facts using more modern notation is given in Theorem 7 of \cite{Pfeifer}.

\end{comment}

\section{Quasi-Orthogonality Estimates For Solutions to the Half-Wave Equation Obtained From High-Frequency Wave Packets} \label{estimatesforwavepackets}

The discussion at the end of Section \ref{PrelimSetup} motivates us to consider estimates for functions obtained by taking averages of the wave equation over a small time interval, with initial conditions localized to a particular part of space. In this section, we study the $L^2$ orthogonality of such quantities. We do not exploit periodicity of the Hamiltonian flow in this section since we are only dealing with estimates for the half wave-equation for \emph{small times}. Our results here thus hold for any manifold $M$, and any operator $P$ whose principal symbol has cospheres with non-vanishing Gaussian curvature.

In order to prove the required quasi-orthogonality estimates of the functions $\{ f_{x_0,t_0} \}$, we find a new connection between \emph{Finsler geometry} on the manifold $M$ and the behaviour of the operator $P$. We will not assume any knowledge of Finsler geometry in the sequel, describing results from the literature needed when required and trying to relate the tools we use to their Riemannian analogues. Moreover, in the case where $p(x,\xi) = \sqrt{\sum g^{jk}(x) \xi_j \xi_k}$ for some Riemannian metric $g$ on $M$, the Finsler geometry we study will simply be the Riemannian geometry on $M$ given by the metric $g$. We begin this section by briefly outlining the relevant concepts of Finsler geometry required to state Theorem \ref{theMainEstimatesForWave}, relegating more precise details to Subsection \ref{critpointsection} where Finsler geometry arises more explicitly in our proofs.

%In this section, we briefly describe the Finsler geometry that will come into play in our argument, mainly relying on results in \cite{BaoChern}. %, giving explicit citations in the literature for those who wish to read the results in more detail.
%This geometry will only come into play in the next two sections, and only seriously so in the second such section.

For our purposes, Finsler geometry is very akin to Riemannian geometry, though instead of a smoothly varying \emph{inner product} being given on the tangent spaces of $M$, in Finsler geometry we are given a smoothly varying \emph{vector space norm} $F: TM \to [0,\infty)$ on the tangent spaces of $M$. Many results in Riemannian geometry carry over to the Finsler setting. In particular, we can define the length of a curve $c: I \to M$ via it's derivative $c': I \to TM$ by the formula
%
\begin{equation}
    L(c) = \int_I F(c'),
\end{equation}
%
and thus get a theory of metric, geodesics, and curvature as in the Riemannian setting. The main quirk of the theory for us is that the metric induced from this length function is not symmetric. Indeed, let
%
\begin{equation}
    d_+(p_0,p_1) = \inf \big\{ L(c) : c(0) = p_0\ \text{and}\ c(1) = p_1 \big\},
\end{equation}
%
and set $d_-(p_0,p_1) = d_+(p_1,p_0)$, i.e. so that
%
\begin{equation}
    d_+(p_0,p_1) = \inf \big\{ L(c) : c(0) = p_1\ \text{and}\ c(1) = p_0 \big\}.
\end{equation}
%
In the Riemannian setting, $d_+$ and $d_-$ are both equal to the usual Riemannian metric, but on a general Finsler manifold one has $d_+ \neq d_-$, and these functions are distinct quasi-metrics on $M$ (though a compactness argument shows $d_M^+(x_0,x_1) \sim d_M^-(x_0,x_1)$). Geodesics from a point $p_0$ to a point $p_1$ need not be geodesics from $p_1$ to $p_0$ when reversed. A metric can be obtained by setting $d_M = (d_M^+ + d_M^-) / 2$, and we will use this as the canonical metric on $M$ in what follows.

Finsler geometry arises here because, if $p: T^* M \to [0,\infty)$ is a homogeneous function satisfying the curvature assumptions of Theorem \ref{CpVersionOfTheorem}, then a Finsler metric arises by taking the `dual norm' of $p$, i.e. setting
%
\begin{equation}
    F(x,v) = \sup \big\{ \xi(v) : \xi \in T^*_x M\ \text{and}\ p(x,\xi) = 1 \big\}.
\end{equation}
%
Thus $p$ induces quasimetrics $d_+$ and $d_-$ on $M$ via the metric $F$ as defined above. We now use these quasimetrics to state Proposition \ref{theMainEstimatesForWave}.

\begin{prop} \label{theMainEstimatesForWave}
    Let $M$ be a compact manifold of dimension $d$, and let $P$ be a classical elliptic self-adjoint pseudodifferential operator of order one whose principal symbol satisfies the curvature assumptions of Theorem \ref{CpVersionOfTheorem}. Then there exists $\varepsilon_M > 0$ such that for all $R \geq 0$, the following estimates hold:
    %
    \begin{itemize}%[leftmargin=8mm]
        \item (Pointwise Estimates) Fix  $|t_0| \leq \varepsilon_M$ and $x_0 \in M$. Consider any two measurable functions $c: \RR \to \CC$ and $u: M \to \CC$, with $\| c \|_{L^1(\RR)} \leq 1$ and $\| u \|_{L^1(M)} \leq 1$, and with $\supp(c) \subset I_{t_0}$ and $\supp(u) \subset B(x_0,1/R)$. Define $S: M \to \CC$ by setting
        %
        \begin{equation}
            S = \int c(t) (Q_R \circ e^{2 \pi i t P} \circ Q_R) \{ u \}\; dt.
        \end{equation}
        %
        Then for any $K \geq 0$, and any $x \in M$,
        %
        \begin{equation}
            |S(x)| \lesssim_K \frac{R^d}{\langle R d_M(x_0,x) \rangle^{\frac{d-1}{2}}} \max\nolimits_{\pm} \Big\langle R \big| t_0 \pm d_M^\pm(x,x_0) \big| \Big\rangle^{-K}. 
        \end{equation}

        \item (Quasi-Orthogonality Estimates) Fix $|t_0 - t_1| \leq \varepsilon_M$, and $x_0, x_1 \in M$. Consider any two pairs of functions $c_0,c_1: \RR \to \CC$ and $u_0,u_1: M \to \CC$ such that, for each $\nu \in \{ 0, 1 \}$, $\| c_\nu \|_{L^1(\RR)} \leq 1$, $\| u_\nu \|_{L^1(M)} \leq 1$, $\supp(c_\nu) \subset I_{t_\nu}$, and $\supp(u_\nu) \subset B(x_\nu,1/R)$. Define $S_\nu: M \to \CC$ by setting
        %
        \begin{equation}
            S_\nu = \int c_\nu(t) (Q_R \circ e^{2 \pi i t P} \circ Q_R) \{ u_\nu \}\; dt.
        \end{equation}
        %
        Then for any $K \geq 0$,
        %
        \begin{equation} \label{AWOICJAWOIVJEAO120321903120938}
            \left| \langle S_0, S_1 \rangle \right| \lesssim_K \frac{R^d}{\langle R d_M(x_0,x_1) \rangle^{\frac{d-1}{2}}} \max\nolimits_{\pm} \Big\langle R \big| (t_0 - t_1) \pm d^\pm(x_0,x_1) \big| \Big\rangle^{-K}.
        \end{equation}
    \end{itemize}
\end{prop}

%\begin{remark} The choice of balls $B(x_0,1/R)$ in the statement above is somewhat arbitrary. We assume we have fixed some choice of open sets $B(x,\delta)$ in $M$ for each $x \in M$ and $0 < \delta < 1$, such that for each coordinate chart $F: U \to \RR^d$ and each compact set $K \subset U$, there exists $C > 1$ such that for all $x \in K$ and $0 < \delta < 1$,
%%
%\begin{equation}
%    B(F(x), C^{-1} \delta) \subset F(B(x,\delta)) \subset B(F(x), C \delta),
%\end{equation}
%
%and where the balls in $\RR^d$ are the usual Euclidean balls. The particular choice given will only effect the magnitude of $\varepsilon_M$ and the implicit constants in the statement of the proposition. Fixing an arbitrary Riemannian metric on $M$ and then letting $B(x,\delta)$ be the resulting metric balls will suffice for our purposes.
%\end{remark}

%    \begin{figure}[h]
%        \centering
%        \includegraphics[width=0.3\textwidth]{TangenciesOnSphere.png}
%    \end{figure}

\begin{remark}
Estimate \eqref{AWOICJAWOIVJEAO120321903120938} is a variable coefficient analogue of Lemma 3.3 of \cite{HeoandNazarovandSeeger}. The pointwise estimate tells us that the function $S$ is concentrated on a geodesic annulus of radius $|t_0|$ centered at $x_0$ and thickness $O(1/R)$. %The annulus has thickness $O(1/R)$, and on this annulus the function $S$ has magnitude at most $O(R^{\frac{d+1}{2}} |t_0|^{- \frac{d-1}{2}})$.
The quasi-orthogonality estimate tells us that the two functions $S_0$ and $S_1$ are only significantly correlated with one another if the two annuli on which the majority of the support of $S_0$ and $S_1$ lie are internally or externally tangent to one another, depending on whether $t_0$ and $t_1$ have the same or opposite sign respectively. %, and then
%
%\begin{equation} \label{DIOAWJDOIWJAFOIJWAFcCw}
%    |\langle S_0, S_1 \rangle| \lesssim R^{\frac{d+1}{2}} |t_0 - t_1|^{- \frac{d-1}{2}}.
%\end{equation}
%
%, though the upper bounds here look superficially different, because here we are using the half wave equation to define our functions $\{ S_\nu \}$, whereas in \cite{HeoandNazarovandSeeger} the analogous functions are simply defined by taking smooth functions adapted to certain annuli. Normalizing and rescaling appropriately causes the bounds to match.
\end{remark}

%We denote the induced metric on $M$ by taking the lengths of forward geodesics between points by $d_M^+: M \times M \to [0,\infty)$, and the induced metric by taking the lengths of backward geodesics by $d_M^-: M \times M \to [0,\infty)$.

% Thus $S_x^*$ has everywhere positive scalar curvature by continuity.\footnote{This argument is a higher dimensional variant of the argument of Chapter 2, Theorem 4.2 of \cite{HeinzHopf}.} But then Hadamard's ovaloid theorem (see Chapter 4, Theorem 2.1 of \cite{HeinzHopf}) implies that for each $x \in M$, the interior of $S_x^*$ is strictly convex. Thus $p$ is a strictly convex function in the $\xi$ variable.

%For each $x \in M$, define $\| \cdot \|_x^2$ to be the Legendre transform of $p(x,\cdot)^2$. Then $\| \cdot \|$ is a smoothly varying family of strictly convex norms on $TM$, and thus gives a Finsler metric on $M$. Moreover, the geodesic flow with respect to this metric is precisely the Hamiltonian flow induced by the principal symbol $p$, since $p$ is precisely the dual Finsler metric on $T^* M$ to the Finsler metric $\| \cdot \|$ on $TM$. %(the usual process of taking the Legendre transform of the Euler-Lagrange equations defining geodesics is precisely the inverse of the procedure we have used to define the metric on $M$ from the function $p$).
%Slightly abusing notation, we will sometimes denote the principal symbol $p: T^* M \to [0,\infty)$ by $\| \cdot \|$, the distinction between the norm $\| \cdot \|$ being clear depending on whether it is applied to a tangent vector or covector. We denote the induced metric on $M$ by taking the lengths of forward geodesics between points by $d_M^+: M \times M \to [0,\infty)$, and the induced metric by taking the lengths of backward geodesics by $d_M^-: M \times M \to [0,\infty)$.

\begin{proof}[Proof of Proposition \ref{theMainEstimatesForWave}]

To simplify notation, in the following proof we will suppress the use of $R$ as an index, for instance, writing $Q$ for $Q_R$. For both the pointwise and quasi-orthogonality estimates, we want to consider the operators in coordinates, so we can use the \emph{Lax-H\"{o}rmander Parametrix} to understand the wave propagators in terms of various oscillatory integrals, though we use a slight variant of the usual parametrix which will help us in the stationary phase arguments which occur when manipulating the resulting oscillatory integrals.

Start by covering $M$ by a finite family of suitably small open sets $\{ V_\alpha \}$, such that for each $\alpha$, there is a coordinate chart $U_\alpha$ compactly containing $V_\alpha$ and with $N(V_\alpha, 1.1 \varepsilon_M) \subset U_\alpha$. Let $\{ \eta_\alpha \}$ be a partition of unity subordinate to $\{ V_\alpha \}$. It will be convenient to define $V_\alpha^* = N(V_\alpha, 0.01 \varepsilon_M )$ for each $\alpha$. The next lemma allows us to approximate the operator $Q$, and the propagators $e^{2\pi i t P}$ with operators which have more explicit representations in the coordinate system $\{ U_\alpha \}$, with an error negligible to the results of Proposition \ref{theMainEstimatesForWave}.

% TODO: Diagram Here?

\pagebreak[3]

\begin{lemma} \label{pseudodifferentialCoordinateLemma}
    Suppose $\varepsilon_M$ is suitably small, depending on $M$ and $P$. For each $\alpha$, and $|t| \leq \varepsilon_M$, there exists Schwartz operators $Q_\alpha$ and $W_\alpha(t)$, each with kernels supported on $U_\alpha \times V^*_\alpha$, such that the following holds:
    %
    \begin{itemize}%[leftmargin=8mm]
    \setlength\itemsep{0.5em}
        \item For any $u: M \to \CC$ with $\| u \|_{L^1(M)} \leq 1$ and $\text{supp}(u) \subset V_\alpha^*$,
        %
        \begin{equation}
            \text{supp}(Q_{\alpha} u) \subset N(\text{supp}(u), 0.01 \varepsilon_M),
        \end{equation}
        %
        \begin{equation}
            \supp(W_{\alpha}(t) u) \subset N(\supp(u), 1.01 \varepsilon_M ),
        \end{equation}
        %
        %
        \begin{equation}
            \| (Q - Q_{\alpha}) u \|_{L^\infty(M)} \lesssim_N R^{-N} \quad\text{for all $N \geq 0$},
        \end{equation}
        %
        and
        %
        \begin{equation}
            \left\| \big( Q_{\alpha} \circ ( e^{2 \pi i t P} - W_{\alpha}(t) ) \circ Q_{\alpha} \big) \{ u \} \right\|_{L^\infty(M)} \lesssim_N R^{-N} \quad\text{for all $N \geq 0$}.
        \end{equation}

        \item In the coordinate system of $U_\alpha$, the operator $Q_{\alpha}$ is a pseudo-differential operator of order zero given by a symbol $\sigma_{\alpha}(x,\xi)$, where
        %
        \begin{equation}
            \supp(\sigma_{\alpha}) \subset \{ \xi \in \RR^d: R/8 \leq |\xi| \leq 8R \},
        \end{equation}
        %
        and $\sigma_{\alpha}$ satisfies derivative estimates of the form
        %
        \begin{equation}
            |\partial^\lambda_x \partial^\kappa_\xi \sigma_{\alpha}(x,\xi)| \lesssim_{\lambda,\kappa} R^{-|\kappa|}.
        \end{equation}

        \item In the coordinate system $U_\alpha$, the operator $W_\alpha(t)$ has a kernel $W_\alpha(t,x,y)$ with an oscillatory integral representation
        %
        \begin{equation}
            W_\alpha(t,x,y) = \int s(t,x,y,\xi) e^{2 \pi i [ \phi(x,y,\xi) + t p(y,\xi) ]}\; d\xi,
        \end{equation}
        %
        where the function $s$ is compactly supported in $U_\alpha$, such that
        %
        \begin{equation}
            \supp_{t,x,y}(s) \subset \{ (t,x,y) : |x - y| \leq C |t| \}
        \end{equation}
        %
        for some constant $C > 0$, suc that
        %
        \begin{equation}
            \supp_\xi(s) \subset \{ \xi \in \RR^d : R/8 \leq |\xi| \leq 8R \}, 
        \end{equation}
        %
        and such that the function $s$ satisfies derivative estimates of the form
        %
        \begin{equation}
            | \partial_{t,x,y}^\lambda \partial_\xi^\kappa s | \lesssim_{\lambda, \kappa} R^{- |\kappa|}.
        \end{equation}
        %
        The phase function $\phi$ is smooth and homogeneous of degree one in the $\xi$ variable, and solves the \emph{eikonal equation}
        %
        \begin{equation}
            p \left( x, \nabla_x \phi(x,y,\xi) \right) = p(y,\xi),
        \end{equation}
        %
        subject to the constraint that $\phi(x,y,\xi) = 0$ if $\xi \cdot (x - y) = 0$.
        % \in \Sigma(y,\xi)$, where
        %
        %\[ \Sigma(y,\xi) = \{ \exp_y(t \eta) : \eta \in T_y^* M\ \text{and}\ \langle \xi, \eta \rangle_g = 0 \}. \]
    \end{itemize}
\end{lemma}

We relegate the proof of Lemma \ref{pseudodifferentialCoordinateLemma} to the appendix, the proof being a technical and mostly conventional calculation involving a manipulation of oscillatory integrals using integration by parts.

Let us now proceed with the proof of the pointwise bounds in Proposition \ref{theMainEstimatesForWave} using Lemma \ref{pseudodifferentialCoordinateLemma}. Given $u: M \to \CC$, write $u = \sum_\alpha u_\alpha$, where $u_\alpha = \eta_\alpha u$. Lemma \ref{pseudodifferentialCoordinateLemma} implies that if we define
%
\begin{equation}
    S_\alpha = \int c(t) (Q_{j,\alpha} \circ W_{j,\alpha}(t) \circ Q_{j,\alpha}) \{ u_\alpha \}\; dt,
\end{equation}
%
then for all $N \geq 0$,
%
\begin{equation} \label{parametrixerrroestimate}
    \left\| S - \textstyle\sum\nolimits_\alpha S_\alpha \right\|_{L^\infty(M)} \lesssim_N R^{-N}.
\end{equation}
%
This error is negligible to the pointwise bounds we want to obtain in Proposition \ref{theMainEstimatesForWave} if we choose $N \geq K - \frac{d+1}{2}$, since the compactness of $M$ implies that $d_M(x,x_0) \lesssim 1$ for all $x \in M$, and so
%
\begin{equation}
    R^{-N} \lesssim R^{\left( \frac{d+1}{2} - K \right)} \lesssim \frac{R^{d}}{\langle R d_M(x,x_0) \rangle^{\frac{d-1}{2}}} \big\langle R \big| |t_0| \pm d_M^\pm(x,x_0) \big| \big\rangle^{-K}.
\end{equation}
%
We bound each of the functions $\{ S_\alpha \}$ separately, combining the estimates using the triangle inequality. We continue by expanding out the implicit integrals in the definition of $S_\alpha$. In the coordinate system $U_\alpha$, we can write
%
\begin{equation} \label{SalphaDefinition}
\begin{split}
    S_\alpha(x) &= \int c(t) \sigma(x,\eta) e^{2 \pi i \eta \cdot (x - y)}\\[-6 pt]
    &\quad\quad\quad s(t,y,z,\xi) e^{2 \pi i [ \phi(y,z,\xi) + t p(z,\xi) ]}\\
    &\quad\quad\quad\quad \sigma(z,\theta) e^{2 \pi i \theta \cdot (z - w)} (\eta_\alpha u)(w)\\
    &\quad\quad\quad\quad\quad dt\; dy\; dz\; dw\; d\theta\; d\xi\; d\eta.
\end{split}
\end{equation}
%
The integral in \eqref{SalphaDefinition} looks highly complicated, but can be simplified considerably by noticing that most variables are quite highly localized. In particular, oscillation in the $\eta$ variable implies that the amplitude is negligible unless $|x - y| \lesssim 1/R$, oscillation in the $\theta$ variable implies that the amplitude is negligible unless $|z - w| \lesssim 1/R$, and the support of $u$ implies that $|w - x_0| \lesssim 1/R$. Define
%
\begin{equation} \label{DIOAJVIOEJAV8318923}
    k_1(t,x,z,\xi) = \int \sigma(x,\eta) s(t,y,z,\xi) e^{2 \pi i [ \eta \cdot (x - y) + \phi(y,z,\xi) - \phi(x,z,\xi) ]}\; dy\; d\eta,
\end{equation}
%
and
%
\begin{equation} \label{DWAIOCWOAIJFAIO213123}
\begin{split}
    k_2(t,\xi) &= \int k_1(t,x,z,\xi) \sigma(z,\theta) (\eta_\alpha u)(w)\\
    &\quad\quad\quad e^{2 \pi i [ \theta \cdot (z - w) + \phi(x,z,\xi) - \phi(x,x_0,\xi) + t p(z,\xi) - t p(x_0,\xi) ]}\; d\theta\; dw,
\end{split}
\end{equation}
%
and then set
%
\begin{equation} \label{oaisdjoai9091390}
\begin{split}
    a(x,\xi) &= \int c(t) k_2(t,R \xi) e^{2 \pi i [ (t - t_0) p(x_0, R \xi) ]} \; dt\; dz,
\end{split}
\end{equation}
%
so that $\text{supp}_\xi(a) \subset \{ \xi : 1/8 \leq |\xi| \leq 8 \}$, and
%
\begin{equation}
    S_\alpha(x) = R^d \int a(x, \xi) e^{2 \pi i R [ \phi(x,x_0,\xi) + t_0 p(x_0,\xi) ]}\; d\xi.
\end{equation}
%
Integrating by parts in $\eta$ and $\theta$ in \eqref{DIOAJVIOEJAV8318923} and \eqref{DWAIOCWOAIJFAIO213123} gives that for all multi-indices $\alpha$,
%
\begin{equation} \label{ioqejfoieqjf13412}
    |\partial_\xi^\alpha k_1(t,x,z,\xi)| \lesssim_\alpha R^{-|\alpha|} \quad\text{and}\quad |\partial_\xi^\alpha k_2(z,\xi)| \lesssim_{\alpha} R^{-|\alpha|}.
\end{equation}
%
Using the bounds in \eqref{ioqejfoieqjf13412} with the fact that $\text{supp}(c)$ is contained in a $O(1/R)$ neighborhood of $t_0$ in \eqref{oaisdjoai9091390} then implies $|\partial_\xi^\alpha a(x,\xi)| \lesssim_\alpha 1$ for all $\alpha$.
%
%We now find $\lambda: V_\alpha^* \times S^{d-1} \to (0,\infty)$ such that
%
%\[ p(x_0, \lambda(x_0,\xi) \xi ) = 1, \]
%
%and switch coordinate systems. If $\tilde{a}(x,\rho, \xi) = a(x, \rho \lambda(x_0) \xi ) (J_\xi \lambda)(x_0,\xi)$,
%a smooth family of homogeneous diffeomorphisms $F_{x_0}: \RR^d \to \RR^d$ for $x_0 \in V_\alpha^*$ such that $p(x_0,F_{x_0}(\xi)) = |\xi|$ for all $x \in \RR^d$. Then if $\tilde{a}(x,\rho, \eta) = a(x, \rho F_{x_0}(\eta) ) JF_{x_0}(\eta)$,
%then a change of variables $\xi = \lambda(x_0,\xi) \rho \eta$ for $|\eta| = 1$ gives that
%
%\begin{align*} 
%    &R^{d} \int a(x,\xi) e^{2 \pi i R [ \phi(x,x_0,\xi) + t_0 p(x_0,\xi) ]}\\
%    &\quad\quad\quad = R^d \int_0^\infty \rho^{d-1} \int_{|\eta| = 1} \tilde{a}(x,\rho,\eta) e^{2 \pi i R \rho [ \lambda(x_0,\eta) \phi(x, x_0, \eta) + t_0 ]}\; d\eta\; d\rho.
%\end{align*}
%

We now account for angular oscillation of the integral by working in a kind of 'polar coordinate' system. First we find $\lambda: V_\alpha^* \times S^{d-1} \to (0,\infty)$ such that for all $|\xi| = 1$,
%
\begin{equation}
    p(x_0, \lambda(x_0,\xi) \xi) = 1.
\end{equation}
%
If $\tilde{a}(x,\rho, \eta) = a(x, \rho \lambda(x_0) \xi ) \det [ \lambda(x_0,\xi) I + \xi (\nabla_\xi \lambda)(x_0,\xi)^T ]$, then
%
% F_{x_0}(xi) = L(x_0,xi) xi
% DF = L(x_0,xi) I + D_xi lambda
%
\begin{equation}
    S_\alpha(x) = R^d \int_0^\infty \rho^{d-1} \int_{|\xi| = 1} \tilde{a}(x,\rho, \xi) e^{2 \pi i R \rho [ t_0 + \phi(x, x_0, \lambda(x_0,\xi) \xi) ]}\; d\xi\; d\rho.
\end{equation}
%
Define $\Phi: S^{d-1} \to \RR$ by setting $\Phi(\xi) = \phi(x,x_0, \lambda(x_0,\xi) \xi)$.    
%
%\[ \text{Hess}_\xi(\Phi) = \text{Hess}_\xi(\phi) + t_0 \text{Hess}_\xi(p). \]
%
%For any multi-index $\alpha$,
%
%\[ |\partial_\xi^\alpha \{ \phi(x,x_0,\xi) - (x - x_0) \cdot \xi \}| \lesssim |x - x_0|^2 |\xi|^{1 - |\alpha|}, \]
%
%and so $\text{Hess}_\xi(\Phi)$ differs from $t_0 \text{Hess}_\xi(p)$ by a matrix all of whose entries are $O(|x - x_0|^2)$. The cosphere curvature assumption on $p$ implies that
%
%\[ |\text{Det}(\text{Hess}_{\xi} p)(x,\xi)| \sim 1. \]
%
%On the support of $s$, $|x - x_0| \lesssim |t|$. Thus if $\varepsilon_M$ is chosen appropriately small,
%
%\[ |\text{Det}(\text{Hess}_{\xi} \Phi)|  \gtrsim t^{d-1}. \]
%
%We claim that $(\nabla_\xi \Phi)(x,x_0,\xi) = 0$ if and only if $x$ lies on the characteristic curve through $x_0$ with cotangent vector $\xi$, and then
%
%To prove this, we rely on the property that, if we define the Hamiltonian flow $\alpha: (-\varepsilon_M, \varepsilon_M) \times T^* V_\alpha^* \to U_\alpha$ generated by $p$, then $\phi(x,x_0,\xi) = t p(x_0,\xi)$ if $x = \alpha(t,x',\xi')$, where $(x' - x) \cdot \xi = 0$ and $p(x',\xi') = p(x_0,\xi)$.
%
% For t_0 = 0, the geodesic from x_0 to x with covextor xi is the
% unique critical point.
%
% nabla_xi phi(x,x_0,xi) = Proj_{xi^Perp}(x - x_0) + O( |x - x_0|^2 )
% nabla_xi Phi = Proj_{xi^Perp}(x - x_0) + t_0 p_xi(x_0,xi) + O( |x - x_0|^2 )
%
% For each t_0, x_0, x, exists a unique xi
% t_0 = 0, then we pick xi going from x_0 to x.
%
% If we rewrite in radial coordinates
%       (t_0,r,x^,x_0)
% Provided that D_{x^} nabla_xi Phi is invertible, this is possible
%
%       phi(x,x_0,xi)
%
%   (D_{x^} nabla_xi) phi(x,x_0,xi) = D_xi [nabla_{x^} phi(x,x_0,xi)]
%                
%
%
%                   p(x, Nab_r phi, Nab_{x^} phi ) = p(x_0,xi)
%
%
% f(x,y) = 0 -> y = g(x) possible if D_yf is invertible
%
% (t(xi) + t_0) p(x_0,xi)
%
% Certainly the geodesic from x_0 to x with covector xi_0 minimizes t(xi) p(x_0,xi)
%   This would also work if p(x_0,xi) is maximized for |xi| = 1 at xi_0
%   Otherwise 
%
% Derivative is (t(xi) + t_0) ∇L(xi) + ∇t(xi) L(xi) = 0
%
% Action is (d_v L)(x,v) v
% Principle of Least Action: A(c) = int (d_v L)(c_x,c_v) c_v
%       Motion is critical point of action.
%
% H(x,xi) = xi * v - L(x,v)
%
% Action then becomes int c_xi c_v = int c_xi p_xi(c_x,c_xi)
%                                  = int p(c_x,c_xi)
%
% Given P as our Hamiltonian
%       P: T^*M -> R
%       P = Legendre Transform of L : TM -> R
%       L = Legendre Transform of P
%
%       If D_xi p(x,xi) = v
%       L(x,v) = v * xi - p(x,xi)
%
% p: R^n x R^n -> R
% psi: R^{n-1} -> R
%       where p(0,eta) = 0 and (Nabla_eta p)(0,eta) * e_n != 0
%       where (Nabla_x psi)(0) = eta'
%
% Then we can solve p(x, Nabla_x phi) = 0
%   such that phi(x',0) = psi(x')
%   and Nabla_x phi(0) = eta
%
% By the implicit function theorem, the assumptions
% uniquely determine d_x phi(x',0) = omega(x')
% in a neighborhood of 0.
%
% We start by finding a Lagrangian section of T^*M,
% which is subset of Z(p), and which contains
% (x',0,omega(x')) for all x' in a neighborhood of 0.
%
% Since Z(p) is a coisotropic submanifold, it is
% foliated by integral curves of the Hamiltonian
% vector field X_p. Find a Lagrangian manifold of R^n
% contained in Z(p), passing through (0,eta)
%
% By assumption, in a neighborhood of
% (0,eta) each of these integral curves of X_p
% projects onto a curve on R^n which is transverse
% to R^{n-1} x {0}. If we let S be the union of all
% the characteristics passing through (x',0,omega(x'))
% then S will therefore be a section of R^n.
%
% The collection (x',omega(x')) is a Lagrangian
% section of R^{n-1} x R^{n-1}, and thus is an
% isotropic submanifold M of R^n x R^n
%
% Thus TM is contained in (TM)^perp
% And the integral curves are perpendicular to
% TM since M is contained in Z(p), which
% implies the union of the curves is Lagrangian
%
% But now we've defined S, we can find phi such
% that d_x phi is S. 
%
%
% So in particular, if phi(x',0) = 0
% So vector field is perpendicular to R^{n-1}
% 
% c(0) = (q,p)
%
% F(c_1(t),y,xi) = int dF(c_1) c_1'
%           = int dF(c_1){ p_xi(c) }
%           = int c_2 { p_xi(c) }
%           = - int p(c)
%           = - t p(y,xi)
%
%The fact that $\Sigma_{x_0}$ has non-vanishing curvature means that 
%
%\[ \text{Det} ( \text{Hess}_{\hat{\eta}} \Phi ) \gtrsim |x - x_0|^{d-1}. \]
%In particular, if $\varepsilon_M$ is chosen suitably small, then the error terms are negligible. 
%
%Now the oscillatory integral has two critical points, at values $\eta$ such that $F_{x_0}(\eta)$ points directly towards or away the flow from $x_0$ to $x$. Thus the integral above can be written as
%
We claim that, in the $\xi$ variable, $\Phi$ has exactly two critical points $|\xi^+|^{-1} \xi^+$ and $|\xi^-|^{-1} \xi^-$, where $\xi^+ \in S_{x_0}^*$ is the covector corresponding to the forward geodesic from $x_0$ to $x$, and $\xi^- \in S_{x_0}^*$ is the covector corresponding to the backward geodesic from $x_0$ to $x$. Moreover,
%
\begin{equation}
    \Phi(|\xi^+|^{-1} \xi^+) = d_M^+(x_0,x) \quad\text{and}\quad \Phi(|\xi^-|^{-1} \xi^-) = - d_M^-(x_0,x),
\end{equation}
%
and the Hessian at each of these points is non-degenerate, with each eigenvalue of the Hessian having magnitude exceeding a constant multiple of $d_M^{\pm}(x_0,x)$. We prove that these properties hold for $\Phi$ in Proposition \ref{triangleLemma} of the following section, via a series of geometric arguments. It then follows from the principle of stationary phase that
%
\begin{equation}
    S_\alpha(x) = \sum_{\pm} \frac{R^{d}}{\left\langle R d_M^{\pm}(x_0,x) \right\rangle^{\frac{d-1}{2}}} \int_0^\infty \rho^{\frac{d-1}{2}} a_{\pm}(x,\rho) e^{2 \pi i R \rho [ t_0 \pm d_M^{\pm}(x_0,x)]}\; d\rho,
\end{equation}
%
where $a_{\pm}$ is supported on $|\rho| \sim 1$, and for all $\alpha$, $|\partial_\rho^\alpha a_{\pm}| \lesssim_\alpha 1$. Integrating by parts in the $\rho$ variable if $t_0 \pm d_M^{\pm}(x_0,x)$ is large, we conclude that
%
% t_0 - d^
%
% 
% 
%
\begin{equation} \label{finaloscillatoryintegralbound}
\begin{split}
    |S_\alpha(x)| \lesssim \frac{R^{d}}{\left\langle R d_M(x_0,x) \right\rangle^{\frac{d-1}{2}}} \sum_{\pm} \big\langle R |t_0 \pm d_M(x_0,x)| \big\rangle^{-K}.
\end{split}
\end{equation}
%
Combining \eqref{parametrixerrroestimate} and \eqref{finaloscillatoryintegralbound} completes the proof of the pointwise bounds.

The quasi-orthogonality arguments are obtained by a largely analogous method, and so we only sketch the proof. One major difference is that we can use the self-adjointness of the operators $Q$, and the unitary group structure of $\{ e^{2 \pi i t P} \}$, to write
%
\begin{equation}
\begin{split}
    \langle S_0, S_1 \rangle &= \int c_0(t) c_1(s) \big\langle (Q \circ e^{2 \pi i t P} \circ Q) \{ u_0 \}, (Q \circ e^{2 \pi i s P} \circ Q) \{ u_1 \} \big\rangle\\
    &= \int c_0(t) c_1(s) \big\langle (Q^2 \circ e^{2 \pi i (t - s) P} \circ Q^2) \{ u_0 \}, u_1 \big\rangle\\
    &= \int c(t) \big\langle (Q^2 \circ e^{2 \pi i t P} \circ Q^2) \{ u_0 \}, u_1 \big\rangle,
\end{split}
\end{equation}
%
where $c(t) = \int c_0(u) c_1(u - t)\; du$, by Young's inequality, satisfies
%
\begin{equation}
    \| c \|_{L^1(\RR)} \lesssim \| c_0 \|_{L^1(\RR)} \| c_1 \|_{L^1(\RR)} \leq 1
\end{equation}
%
and $\supp(c) \subset [ (t_0 - t_1) - 4/R, (t_0 - t_1) + 4/R]$. After this, one proceeds exactly as in the proof of the pointwise estimate. We write the inner product as
%
\begin{equation}
    \sum\nolimits_\alpha \int c(t) \big\langle (Q^2 \circ e^{2 \pi i t P} \circ Q^2) \{ \eta_\alpha u_0 \}, u_1 \big\rangle.
\end{equation}
%
Then we use Lemma \ref{pseudodifferentialCoordinateLemma} to replace $Q^2 \circ e^{2 \pi i tP} \circ Q^2$ with $Q_{\alpha}^2 \circ W_{\alpha}(t) \circ Q_{\alpha}^2$ using Lemma \ref{pseudodifferentialCoordinateLemma}, modulo a negligible error. The integral
%
\begin{equation}
    \sum\nolimits_\alpha \int c(t) \big\langle (Q_{\alpha}^2 \circ W_{\alpha}(t) \circ Q_{\alpha}^2) \{ \eta_\alpha u_0 \}, u_1 \big\rangle
\end{equation}
%
is then only non-zero if both the supports of $u_0$ and $u_1$ are compactly contained in $U_\alpha$. Thus we can switch to the coordinate system of $U_\alpha$, in which we can express the inner product by oscillatory integrals of the exact same kind as those occurring in the pointwise estimate. Integrating away the highly localized variables as in the pointwise case, and then applying stationary phase in polar coordinates proves the required estimates.
\end{proof}

\subsection{Some Facts About Finsler geometry} \label{BriefIntroduction}

All that remains to conclude the proof of the quasi-orthogonality estimates is to verify the required properties of the critical points of the phase function $\Phi$ which occurs in the proof of Proposition \ref{theMainEstimatesForWave}. To do this we exploit the Finsler geometry on the manifold, and so before we carry out this task we take out the time to precisely introduce the concepts of Finsler geometry that occur in the proof. We will work in a particular coordinate system $U_0$, so we introduce the concepts in coordinates to keep things concrete.

 %This subsection may be safely skipped if one only wants to follow the proof of Proposition \ref{theMainEstimatesForWave} in the special case where $p$ is induced by a Riemannian metric as in \eqref{pgivesRiemannian}, and thus gives $M$ the structure of a Riemannian manifold.

Let $U_0 \subset \RR^d$ be an open set. Then we may identify the tangent space $T U_0$ and cotangent space $T^* U_0$ with $U_0 \times \RR^d$, and each fibre $T_x U_0$ and $T^*_x U_0$ with $\RR^d$, though as is standard in differential we use upper indices to denote the coordinates of vectors, and lower indices to denote the coordinates of covectors. A \emph{Finsler metric} on $U_0$ is a homogeneous function $F: T U_0 \to [0,\infty)$ which is smooth on $TU_0 - 0$, and strictly convex, in the sense that the Hessian matrix with entries
%
\begin{equation} \label{FinslerMetricCoefficients}
    g_{ij}(x,v) = \frac{1}{2} \frac{\partial^2 F^2}{\partial v^i \partial v^j}(x,v)
\end{equation}
%
is positive definite for all $x \in U_0$ and $v \in T_x U_0 - \{ 0 \}$.% giving rise to an inner product on $T_x U_0$ which approximates the Finsler metric near $(x,v)$ up to second order in the $v$ variable.

%For a fixed tangent vector $T = (x,v)$, the coefficients $g_{jk}(x,v)$ then give rise to an inner product on $T_x M$ that approximates the Finsler metric up to second order at $(x,v)$, which we will denote by $g_T$. As alluded to before, Riemannian tools normally defined on the vector bundle $TM$ often have analogues in the setting of Finsler manifolds, but instead defined on the vector bundle $\mathscr{E} = \pi^*(TM)$ over $TM$. In particular, the $g_T$ form a smoothly varying inner product on the fibres of $\mathscr{E}$.

Given a metric $F$ on $M$, we define a dual metric $F_*: T^*M \to [0,\infty)$ by setting $F_*(x,\xi) = \sup \{ \xi(v) : F(x,v) = 1 \}$. Then $F_*$ is also strictly convex, so that for each $x \in U_0$ and each covector $\xi \in \RR^d - \{ 0 \}$, the Hessian matrix with entries
%
\begin{equation}
    g^{ij}(x,\xi) = \frac{1}{2} \frac{\partial^2 F_*^2}{\partial \xi_i \partial \xi_j}(x,\xi)
\end{equation}
%
is positive definite. For $x \in U_0$, the \emph{Legendre transform} is then defined to the smooth map $\mathcal{L}_x: T_x U_0 - \{ 0 \} \to T^*_x U_0 - \{ 0 \}$ given by the formula $(\mathcal{L}_x v)_i = \sum_j g_{ij}(x,v) v^j$, defined so that $\sum (\mathcal{L}_x v)_j w^j = \sum g_{ij}(x,v) v^j w^j$ for any vector $w \in \RR^d$. Then the matrix with entries $g_{ij}(x,v)$ is the inverse of the matrix with entries $g^{ij}(x, \mathcal{L}_x v)$, so that for a covector $\xi \in \RR^d - \{ 0 \}$, $(\mathcal{L}_x^{-1} \xi)^i = \sum g^{ij}(x,\xi) \xi_j$. The Legendre transform is the Finsler variant of the musical isomorphism in Riemannian geometry, though the Legendre transform is in general only \emph{homogeneous}, rather than linear.

Let us prove in more detail that if $p: T^*M \to [0,\infty)$ is homogenous and satisfies the assumptions of Theorem \ref{CpVersionOfTheorem}, the function $F(x,v) = \sup \{ \xi(v): p(x,\xi) = 1 \}$ gives a Finsler metric on $M$. Working in a coordinate system, we may assume $M = U_0$ is an open subset of $\RR^d$ like above. For each $x \in U_0$, the cosphere $S_x^* = \{ \xi \in T_x^* M : p(x,\xi) = 1 \}$ has non-vanishing Gaussian curvature. Thus all principal curvatures of $S_x^*$ must actually be \emph{positive}, since $S_x^*$ is a compact hypersurface in $T^*_x M$. This follows from a simple modification of an argument found in Chapter 2 of \cite{HeinzHopf}. Indeed, if we fix an arbitrary point $v_0 \in T_x^*M$, and consider the smallest closed ball $B \subset T_x^* M$ centered at $v_0$ and containing $S_x^*$, then the sphere $\partial B$ must share the same tangent plane as $S_x^*$ at some point. All principal curvatures of $\partial B$ are positive, and at this point all principal curvatures of $S_x^*$ must be greater than the principal curvatures of $\partial B$, since $S_x^*$ curves away faster than $\partial B$ in all directions. By continuity, we conclude that the principal curvatures are everywhere positive.  Thus for each $x \in M$ and $\xi \in T_x^* M - \{ 0 \}$, the coefficients $g^{ij}(x,\xi) = (1/2) (\partial^2 p^2 / \partial \xi_i \partial \xi_j)$ form a positive-definite matrix. But inverting the procedure of the last paragraph shows that the dual norm $F(x,v) = \sup\nolimits_{\xi \in S_x^*} \xi(v)$ is also strictly convex, and thus gives a Finsler metric on $M$ with $F_* = p$. %(Chapter 14 of \cite{BaoChern}). 

 % and give rise to an inner product on $T_x M$ that best approximates the Finsler metric to second order at $(x,v)$.%, i.e. so that if $F(x,v) = F(x,w) = 1$, then
%
%\[ \left| F(x,w) - ( \sum g_{jk}(x,v) w^j w^k )^{1/2} \right| \lesssim |v - w|^3 \]

 %For each $v \in T_x M$, the coefficients $g_{jk}(x,v)$ define a Riemannian metric on $T_x M$ which approximates the Finsler metric to second order in a neighbourhood of $v$. % i.e. the functions $w \mapsto F(x,w)$ and $w \mapsto ( \sum g_{jk}(x,v) w^j w^k )^{1/2}$ agree up to second order in a neighborhood of $v$.

%The closest analogue to the Levi-Civita connection on Finsler manifolds is the \emph{Chern connection}. To discuss the connection, let $\bar{T}$ be the \emph{distinguished section} of $\mathscr{E}$, the section given by $\bar{T}(x,v) = F(v)^{-1} v$. For two sections $X$ and $Y$ of $\mathscr{E}$, the Chern connection gives a section $\nabla_X Y$ of $\mathscr{E}$ which is (a) $C^\infty(TM)$-linear in $X$ and $\RR$-linear in $Y$ (b) \emph{torsion free}, in the sense that $\nabla_X Y - \nabla_Y X = [X,Y]$, and (c) \emph{almost metric compatable} which is a somewhat technical to state fully, but for our purposes implies that for any sections $X$ and $Y$ and $Z$ of $\mathscr{E}$, when evaluating $X \{ g_{\bar{T}}(Y,Z) \}$ at a point in $TM$ where $X$, $Y$, or $Z$ is a multiple of $\bar{T}$, one has
%
% X { g_{Tbar}(Y,Z) } = g_{Tbar}( Nabla_X Y, Z ) + g_{Tbar} ( Y, Nabla_X Z ) + 2 C_{Tbar}(Nabla_X Tbar, Y, Z )
%
%\begin{equation} \label{almostmetriccompatibility}
%    X \{ g_{\bar{T}}(Y,Z) \} = g_{\bar{T}}(\nabla_X Y, Z) + g_{\bar{T}}(Y, \nabla_X Z).
%\end{equation}
%
%%Note that the Chern connection is \emph{not metric compatible}, i.e. \eqref{almostmetriccompatibility} does not hold for arbitrary $X$, $Y$, and $Z$. There does not exist a torsion free, metric compatible affine connection on the bundle $\mathscr{E}$ unless $M$ is a Riemannian manifold. As in Riemannian geometry, the value of $\nabla_X Y$ at some point $(x,v) \in TM$ depends only on the behaviour of $Y$ along some curve $c: I \to TM$ with $c(0) = (x,v)$ and $(\pi \circ c)'(0) = X(x,v)$, and we will sometime abuse notation by writing $\nabla_X Y$ if $Y$ is only defined along such a curve.

%The analogue of the Riemann curvature tensor is the \emph{first Chern curvature tensor} $R$, a $(1,3)$ tensor defined over $\mathscr{E}$ such for three sections $X$, $Y$, and $Z$ of $\mathscr{E}$, $R(X,Y) Z$ is the section defined by
%
%\begin{equation}
%    R(X,Y) Z = \nabla_X \nabla_Y Z - \nabla_Y \nabla_X Z - \nabla_{[X,Y]} Z.
%\end{equation}
%
%Given a section $X$ of $\mathscr{E}$, the \emph{flag curvature} $K(\bar{T},X)$ is defined by the formula
%
%\begin{equation}
%    K({\bar{T},X) = \frac{g_{\bar{T}}(R(X,\bar{T}) \bar{T}, X)}{g_{\bar{T}}(X,X)} - g_{\bar{T}}(X,\bar{T})^2},
%\end{equation}
%
%which generalizes the \emph{sectional curvature} from Riemannian geometry. By homogeneity, compactness, and continuity, on any compact Finsler manifold $M$, there exists constants $\delta$ and $\Delta$ such that for all sections $X$ of $\mathscr{E}$, $\delta \leq K(\bar{T},X) \leq \Delta$.

The theory of geodesics on Finsler manifolds is similar to the Riemannian case, except for the interesting quirk that a geodesic from a point $p$ to a point $q$ need not necesarily be a geodesic when considered as a curve from $q$ to $p$, and so we must consider \emph{forward} and \emph{backward} geodesics. We define the length of a curve $c: I \to U_0$ by the formula $L(c) = \int_I F(c,\dot{c})\; dt$, and use this to define the \emph{forward distance} $d_+: U_0 \times U_0 \to [0,\infty)$ by taking the infima of paths between points. This function is a \emph{quasi-metric}, as it is not necessarily symmetric. We define the \emph{backward distance} $d_-: U_0 \times U_0 \to [0,\infty)$ by setting $d_-(p,q) = d_+(q,p)$. A \emph{constant speed speed forward geodesic} on a Finsler manifold is a curve $c: I \to U_0$ which satisfies the geodesic equation
%$\nabla_{\bar{T}} \bar{T}$ vanishes when restricted the lifted curve $c': I \to TM$.
%
% Nabla_T T = T^a [partial_a T^b] e_b + T^a T^b N_b^c e_c
%           = T^a [ partial_a T^b + T^c N_c^b(T) ] e_b
%           = d^2c^a / dt^2 = - T^j T^k Gamma_{jk}^a
%
\begin{equation} \label{geodesicequation}
    \ddot{c}^a = - \sum\nolimits_{j,k} \gamma^a_{jk}(c,\dot{c}) \dot{c}^j \dot{c}^k \quad\text{for $1 \leq i \leq d$},
\end{equation}
%
where $\gamma^a_{jk} = \sum_i (g^{ai}/2) ( \partial_{x_k} g_{ij} + \partial_{x_j} g_{ik} - \partial_{x_i} g_{jk} )$ are the \emph{formal Christoffel symbols}. % defined by
%
%\[ \Gamma^a_{jk} = \sum\nolimits_i \frac{g^{ai}}{2} \left( \frac{\delta g_{ij}}{\delta x^k} + \frac{\delta g_{ik}}{\delta x^j} - \frac{\delta g_{jk}}{\delta x^i} \right), \]
%
%and where $\delta / \delta x^a = \partial / \partial x^a - \sum_b N^b_a \partial / \partial v^b$
%Such a curve is precisely one which is an extremal of the length functional $L$.
We call the reversal of such a geodesic a \emph{backward geodesic}. A curve $c$ is a geodesic if and only if it's Legendre transform $(x,\xi) = \mathcal{L}(c,\dot{c})$ satisfies
%
\begin{equation} \label{FStarHamilton}
    x'(t) = (\partial_\xi F_*)(x,\xi) \quad\text{and}\quad \xi'(t) = - (\partial_x F_*)(x,\xi).
\end{equation}
%
The equations \eqref{FStarHamilton} are first order ordinary differential equations induced by the Hamiltonian vector field $( \partial_\xi F_*, - \partial_x F_* )$ on $T^* U_0$. Note that in our situation, the dual norm $F_*$ is the principal symbol $p$ of the operator $P$ we are studying, and so \eqref{FStarHamilton} is precisely the Hamiltonian flow alluded to in \eqref{qoiJIOJdoiwajfoiawjfoi}. Sufficiently short geodesics on a Finsler manifold are length minimizing. In particular, there exists $r > 0$ such that if $c$ is a geodesic between two points $x_0$ and $x_1$, and $L(c) < r$, then $d_M^+(x_0,x_1) = L(c)$.

%The analogue of the Riemann curvature tensor on a Finsler manifold is the \emph{$hh$-curvature tensor} $R$, a $(1,3)$ tensor defined on $E$ such that for three sections $X,Y,Z$ of $E$, $R(X,Y) Z$ is a section of $E$, which we define in the appendix. The analogue of sectional curvature in Finsler geometry is the \emph{flag curvature}. To define the curvature, let $T$ be the section of $E = \pi^*(TM)$ given by $T(x,v) = F(v)^{-1} v$, %and let $\omega$ be the section of $E^* = \pi^*(T^* M)$ given in coordinates by $\omega(x,v) = \sum (\partial F / \partial v^j)(x,v) dx^j$.
%This section is often called the \emph{distinguished section} in the literature. Given a section $X$ of $E$, we define the flag curvature $K(T,X)$ to be the function on $TM$ given by
%
%\[ K(T,X) = \frac{g(R(X,T) T, X)}{g(X,X) - g(X,T)^2} \]
%
%where $R$ is the \emph{$hh$-Curvature tensor}, a $(1,3)$ tensor defined on $E$ such that for three sections $X,Y,Z$ of $E$, $R(X,Y) Z$ is the section of $E$ given in coordinates by
%
%\[ R(X,Y) Z = \sum\nolimits_{i,j,k,l} Z^j X^k Y^l \left( \frac{\delta \Gamma^i_{jl}}{\delta x^k} - \frac{\delta \Gamma^i_{jk}}{\delta x^L} + \Gamma^i_{hk} \Gamma^h_{jl} - \Gamma^i_{hl} \Gamma^h_{jk} \right) \frac{\partial}{\partial x^i} \]
%
%

The last piece of technology we must consider before our analysis of critical points are \emph{variation formulas} for arclength. Consider a function $A: (-\varepsilon,\varepsilon) \times [0,1] \to U_0$, such that for each $s$, the function $t \mapsto A(s,t)$ is a constant speed geodesic, and such that all such geodesics begin at the same point, i.e. so that $A(s,0)$ is independent of $s$. Define the vector field $V(s) = (\partial_s A)(s,1)$ and $T(s) = (\partial_t A)(s,1)$. Then if $R(s)$ gives the arclength of the geodesic $t \mapsto A(s,t)$, the first variation formula for energy tells us
%
\begin{equation} \label{FirstVariationFormula2}
    R(s) R'(s) = \sum\nolimits_{i,j} g_{ij}( A(s,1), T(s) ) V^i(s) T^j(s).
\end{equation}
%
Details can be found in Chapter 5 of \cite{BaoChern}. We will consider \emph{second variations}, i.e. estimates on $R''$, but we will not need the second variation formula to obtain the estimates we need, relying only on differentiating the equation \eqref{FirstVariationFormula2} rather than explicitly working with Jacobi fields. % We note that the denominator in \eqref{FirstVariationFormula} is precisely the length of the vector $T$, which by the Gauss Lemma for Finsler manifolds (see Lemma 6.1.1 of \cite{BaoChern}) is precisely $R(s)$. Thus we can rewrite \eqref{FirstVariationFormula} as
%
%\begin{equation} \label{FirstVariationFormula2}
%    R(s) R'(s) = \sum g_{ij}( A(s,1), T(s) ) V^i(s) T^j(s).
%\end{equation}

\begin{remark}
    The second variation formula for arclength in Riemannian geometry has an exact analogue in Finsler geometry via a generalization of the theory of Jacobi fields. Sectional curvature is replaced with what is called \emph{flag curvature}, which controls the growth of Jacobi fields. One could in principle use such quantities to obtain more explicit constants in Theorem \ref{triangleLemma}, though we have no such need of these constants here.
\end{remark}

% which we shall view as a vector in the bundle $\mathscr{E}$ in the fibre over $(a(s,t), \partial_t a(s,t))$. By a similar process, we can think of the distinguished section $\bar{T}$ on $\mathscr{E}$ as a function of $t$ and $s$, and then $\bar{T}$ is just the unit vector at $a(s,t)$ pointing in the same direction as $(\partial_t a)(s,t)$. If we let $L(s)$ be the length of the curve $t \mapsto a(s,t)$, then the \emph{second variation formula} says that $L''(0) = V \{ g_{\bar{T}}(V,T) \}|_0^l$, i.e. the difference between $V \{ g_{\bar{T}}(V,T) \} = g_{\bar{T}}( \nabla_V V, T ) + g_{\bar{T}}( V, \nabla_V T )$ evaluated at $s = 0$ and $t = l$, and evaluated at $s = 0$ and $t = 0$. In our setup $a(s,0)$ will be independent of $s$, so $V$ and $\nabla_V V$ vanish at the origin, and so it suffices to evaluate $V \{ g_{\bar{T}}(V,T) \}$ when $s = 0$ and $t = l$.

% g( Nabla_V V,  TBar ) + g( V, Nabla_V T )
%   = V { g(V,T) }

% Energy of a constant speed geodesic of length l and speed k is int_0^{l/k} k^2 = lk
%
% In our case the curves have length L(s) and speed L(s) / l
% and so the energy is equal to L(s)^2 / l
%
% The first derivative of energy is 2 L(s) L'(s) / l
% And the second derivative is 2 L(s) L''(s) / l = - g_T( V, Nabla_T V )
% L''(s) = - 1/2

% By almost metric compatibility, we can write
% g( Nabla_V V, T ) = V { g(V,T) } - g( V, Nabla_V T ).
%
% Let us start by bounding V { g(V,T) }.
%
% If E = exp_x, then because a(s,t) = exp_x( t( cos(s) v + sin(s) w ) )
%
% V = partial_s a = E( t( cos(s) v + sin(s) w ) ) { t ( cos(s) w - sin(s) v ) }
%
% By the Gauss Lemma for Finsler manifolds,
%       g(V,T) = g( t ( cos(s) w - sin(s) v ), v )
%              = - t sin(s)
% Thus V { g(V,T) } = - t cos(s)
%
% On the other hand, to calculate g(V, Nabla_V T), write this quantity as f(t). We note that since V(0) = 0, f(0) = 0.
% Moreover by almost metric compatibility and being torsion free,
%    f' = T { g(V, Nabla_V T) } = g( Nabla_T V, Nabla_V T ) + g(V, Nabla_T Nabla_V T )
%                               = | Nabla_T V |^2 - g( R(V,T)T, V )
%                               = | Nabla_T V |^2 - K(T,V)                 [ Here we used the fact that V is verywhere orthogonal to T ]
%
%
%
% Define a(t) = f(t) / |V|^2 = g(V, Nabla_V T) / |V|^2
%
% a'(t) = f'(t) / |V|^2 - 2 f(t)^2 / |V|^4 
%       = f'(t) / |V|^2 - 2 a(t)^2
% a'(t) + 2 a(t)^2 = f'(t) / |V|^2 = | Nabla_T V|^2 / |V|^2 - K(T,V)
%
% < Nabla_T V, Nabla_T V > >= < Nabla_T V, V/|V| >^2 = < Nabla_T V, V>^2 / |V|^2 = |V|^2 a(t)^2
%
% so
%
% a'(t) + a(t)^2 >= - K(T,V) >= -Delta
%
% We can use this to establish an upper bound on a(t), and thus on f(t), provided
% we have good bounds on |V|. But we do have such bounds, which completes the proof.
%
% To get the lower bound, we exploit the fact that 
%
% < Nabla_V T, V >(b) = < Nabla_E T, E >(b)
%
% for the parallel vector field E with E(b) = V(b). If we define a(t) = < Nabla_E T, E > / |E|^2, then
%
%  a'(t) = T { (Hess r)(E,E) } / |E|^2
%        = Nabla_T {Hess r} (E,E) / |E|^2
%        = - < R(E,T)T, E > / |E|^2 - (Hess^2 r)(E,E) / |E|^2
%       <= - K(T,E) - a(t)^2
%
% So a'(t) + a(t)^2 <= - K(T,E) <= - delta




% Let
%
% (Hess r)(X,Y) = < Nabla_X T, Y >
%
% (Hess^2 r)(X,Y) = < nabla_X T, Nabla_Y T >
%
% (L_T Hess r)(X,Y) - (Hess^2 r)(X,Y) = - < R(X,T) T, Y >
%
% Proof
%
% L_T { (Hess r)(X,Y) } = T < Nabla_X T, Y >
%                       = < Nabla_T Nabla_X T, Y > + < Nabla_X T, Nabla_T Y >
%                       = - < R(X,T) T, Y > + < Nabla_{[T,X]} T, Y > + < Nabla_X T, Nabla_T Y >
%                       = - < R(X,T) T, Y > + (Hess r)(L_T X, Y) + < Nabla_X T, Nabla_T Y >
%                       = - < R(X,T) T, Y > + (Hess r)(L_T X, Y) + < Nabla_X T, Nabla_Y T > + (Hess r)(X, L_T Y)
%                       = - < R(X,T) T, Y > + (Hess^2 r)(X,Y) + (Hess r)(L_T X, Y) + (Hess r)(X, L_T Y)
%
% Subtracting off and using the product rules gives the result.
%
%
% [Nabla_T (Hess r)](X,Y) + (Hess^2 r)(X,Y) = - <R(X,T)T, Y>
%
% Proof
%
% We write
%
% - < R(X,T) T, Y > = < Nabla_T Nabla_X T - Nabla_{[T,X]} T, Y > 
%                   = T < Nabla_X T, Y > - < Nabla_X T, Nabla_T Y > - < Nabla_{[T,X]} T, Y >
%                   = T { (Hess r)(X,Y) } - (Hess r)(X, Nabla_T Y) - (Hess r)(L_T X, Y)
%                   = T { (Hess r)(X,Y) } - (Hess r)(X, Nabla_T Y)  - (Hess r)( Nabla_T X, Y ) + (Hess r)(Nabla_X T, Y)
%                   = Nabla_T {Hess r} (X,Y) + (Hess r)(Nabla_X T, Y)
%                   = Nabla_T {Hess r} (X,Y) + (Hess^2 r)(X,Y)

















%   then
%
%   a'(t) = T < Nabla_E T, E >
%         = < Nabla_T Nabla_E T, E >
%         = < R(T,E) T, E > + < Nabla_{[T,E]} T, E >
%         = < R(T,E) T, E > - < Nabla_T [T,E], E >
%         = < R(T,E) T, E > + T < Nabla_E T, E >

%         = < R(T,E) T, E > + < Nabla_T [T,E], E >
%         = < R(T,E) T, E >
%
%
%         = - |E|^2 K(T,E) - <Nabla_E T, Nabla_E T>
%


%
% T < Nabla_E T, E > = < nabla_E T, Nabla_E T > - K(T,E)
%   Proof: T < Nabla_E T, E > = < Nabla_T Nabla_E T, E >
%                             = < Nabla_{[E,T]} T, E > - K(T,E) |E|^2
%                             = < Nabla_T [E,T], E > - K(T,E) |E|^2
%                             = T < [E,T], E > - K(T,E) |E|^2 
%                           
%               Nabla_{[E,T]} T = Nabla_T [E,T] + [ [E,T], T]
%                               = Nabla_T [E,T] - [ T, [E,T] ]
%                               = Nabla_T [E,T]
%
% 2 [T,[E,T]] + [ E, [T,T] ] - [ T, [T,E] ] = 0

%                               = Nabla_T [E,T] + (  T,[E,T]    )

% we get
%
% a'(t) = < Nabla_T Nabla_E T, E 
%       = T < Nabla_E T, E >
%       = - K(T,E) - |Nabla_E T|^2

%
% T < Nabla_E T, E > + Hess^2 r(E,E) = - K(T,E)
%
% [Hess^2 r](E,E) = < Nabla_E T, nabla_E T >
%
% T < Nabla_E T, E > + |Nabla_E T|^2 = - K(T,E)


%
% for any parallel vector field E with E(b) = V(b). If we define a(t) = < Nabla_E T, E > / |E|^2 then
%
% a'(t) = < Nabla_T Nabla_E T, E > / |E|^2
%       = - K(T,E) + < Nabla_{[T,E]} T, E > / |E|^2
%


% a'(t) = < Nabla_T Nabla_E T, E > / |E|^2 + < Nabla_{[T,E]} T, E > / |E|^2 = - K(T,E) + < Nabla_{[T,E]} T, E > / |E|^2 <= -delta + < Nabla_{[T,E]} T, E > / |E|^2
%
% Since [T,E] = Nabla_T E - Nabla_E T = - Nabla_E T
%
% a'(t) <= - delta - < Nabla_{Nabla_E T} T, E >






% Let M_j^i = partial_j E_*^i.
% Then E_* = M_j^i dx^j o partial_i
% Thus Nabla E_* = (partial_k M_j^i) dx^k o dx^j o partial_i + M_j^i Gamma^l_{ik} dx^k o dx^j o partial_l
%                = [ (partial_k M_j^i) + M_j^l Gamma^i_{lk} ] dx^k o dx^j o partial_i
%
% And so (Nabla E_*)(v,v) = v^a v^b [ (partial_a M_b^i) + M_b^l Gamma_{la}^i ] e_i
%
% On the other hand,
%       Nabla_V V = V^a Nabla_{e_a} V
%                 = V^a [ [partial_a V^i] e_i  + V^b Gamma_{ab}^l e_l]
%                 = [ V^a partial_a V^i + V^a V^b Gamma_{ab}^i ] e_i
%
% Now V^a = partial_s exp^a( t(w + sv) ) = tv^b M_b^a
%
% and so
%       Nabla_V V = [ V^j partial_j V^i + V^j V^k Gamma_{jk}^i  ] e_i
%                 = t^2 V^a v^b [ partial_j M_b^i + M_b^l Gamma_{la}^i ] e_i
%                 = t^2 (Nabla E_*)(V,v)

%
% T g(D_V V, T) = g(D_T D_V V, T) = g(D_V D_T V, T) + g( R(T,V) V, T )
%                                 = g(D_V D_T V, T) - g( R(T,V) T, V )
%                                 = g(D_V D_T V, T) - g( D_T D_T V, V )
%                                 = g(D_V D_T V, T) + |D_T V|^2 - T { g( D_T V, V ) }

% T^2 g(D_V V, T) = g(D_T D_T D_V V, T)

%Like in Riemannian manifolds,
% Let us write this equation as $\ddot{c} + G(c,\dot{c}) = 0$. The exact formula for $\gamma^i_{jk}$ in terms of the Finsler metric are not needed in our arguments, but one can find more details in Sections 2.3 and 5.3 of \cite{BaoChern}. If $c$ solves \eqref{geodesicequation}, then $F(c,\dot{c})$ is a constant, justifying the adjective `unit speed'. 
%suitably short geodesics are length minimizing. Thus for suitably small forward geodesics $c: [a,b] \to M$ we have $d_+(c(a), c(b)) = L(c)$.

\subsection{Analysis of Critical Points} \label{critpointsection}

In this section, we now classify the critical points of the kinds of functions that arose in Proposition \ref{theMainEstimatesForWave}.

\pagebreak[3]

\begin{prop} \label{triangleLemma}
    Fix a bounded open set $U_0 \subset \RR^d$. Consider a Finsler metric $F: U_0 \times \RR^d \to [0,\infty)$ on $U_0$, and it's dual metric $F_*: U_0 \times \RR^d \to [0,\infty)$, which extends to a Finsler metric on an open set containing the closure of $U_0$. Fix a suitably small constant $r > 0$. Let $U$ be an open subset of $U_0$ with diameter at most $r$ which is geodesically convex (any two points are joined by a minimizing geodesic). Let $\phi: U \times U \times \RR^d \to \RR$ solve the eikonal equation
    %
    \begin{equation} \label{eikonalequation}
        F_* ( x, \nabla_x \phi(x,y,\xi) ) = F_*(y,\xi),
    \end{equation}
    %
    such that $\phi(x,y,\xi) = 0$ for $x \in H(y,\xi)$, where $H(y, \xi) = \{ x \in U : \xi \cdot (x - y) = 0 \}$. For each $x_0,x_1 \in U$, let $S_{x_0}^* = \{ \xi \in \RR^d: F_*(x_0,\xi) = 1 \}$ be the cosphere at $x_0$, and define $\Psi: S_{x_0}^* \to \RR$ by setting
    %
    \begin{equation}
        \Psi(\xi) = \phi(x_1,x_0,\xi).
    \end{equation}
    %
    Then the function $\Psi$ has exactly two critical points, at $\xi^+$ and $\xi^-$, where the Legendre transform of $\xi^+$ is the tangent vector of the forward geodesic from $x_0$ to $x_1$, and the Legendre transform of $\xi^-$ is the tangent vector of the backward geodesic from $x_1$ to $x_0$. Moreover,
    %
    \begin{equation}
        \Psi(\xi^+) = d_M^+(x_0,x_1) \quad\text{and}\quad \Psi(\xi^-) = - d_M^-(x_0,x_1),
    \end{equation}
    %
    and the Hessians $H_+$ and $H_-$ of $\Psi$ at these critical points, viewed as quadratic maps from $T_{\xi_{\pm}} S_{x_0}^* \to \RR$ satisfy
    %
    \begin{equation}
        H_+(\zeta) \geq C d_M^+(x_0,x_1) |\zeta| \quad\text{and}\quad H_-(\zeta) \leq - C d_M^-(x_0,x_1) |\zeta|,
    \end{equation}
    %
    where the implicit constant is uniform in $x_0$ and $x_1$.
\end{prop}

If $\Psi$ is as above, then the function $\Phi: S^{d-1} \to \RR$ obtained by setting $\Phi(\xi) = \phi( x, x_0, F_*(x,\xi)^{-1} \xi  )$ is precisely the kind of function that arose as a phase in Proposition \ref{theMainEstimatesForWave}, where $F_*$ was the principal symbol $p$ of the pseudodifferential operator we were considering. Since critical points and the Hessians of maps at critical points are stable under diffeomorphisms, %(see Chapter 5, Problem 17 of \cite{Spivak1}),
and the map $\xi \mapsto F_*(x,\xi)^{-1} \xi$ is a diffeomorphism from $S_{x_0}^*$ to $S^{d-1}$, classifying the critical points of the map $\Psi$ implies the required properties of the map $\Phi$ used in Proposition \ref{theMainEstimatesForWave}.

% then, because critical points and the Hessians of maps at critical points are stable under diffeor

%Define a function $\lambda: U \times S^{d-1} \to \RR$ by setting $\lambda(x,v) = F_*( x,v )^{-1}$ and then define $\Phi: S^{d-1} \to \RR$ by setting $\Phi(\xi) = \phi(x,x_0,\lambda(x_0,\xi) \xi)$. Let $\xi^+$ and $\xi^-$ are the two points in $S_{x_0}^*$ such that $\mathcal{L}^{-1}(x_0,\xi^+)$ is the unit tangent vector of the forward geodesic from $x_0$ to $x$, and $-\mathcal{L}^{-1}(x_0,\xi^-)$ is the unit tangent vector to the backward geodesic from $x_0$ to $x$. In this subsection, we will verify that the only critical points of $\Phi$ occur at $|\xi^+|^{-1} \xi^+$ and $|\xi^-|^{-1} \xi^-$, that $\Phi( |\xi^+|^{-1} \xi^+) = d_M^+(x_0,x)$ and $\Phi( |\xi^-|^{-1} \xi^-) = - d_M^-(x_0,x)$, and that if we let $V_+$ and $V_-$ be the subspaces of $\RR^d$ which give the tangent spaces of $S^{d-1}$ at each critical point, then the Hessians $H_{\pm} : V_{\pm} \to \RR$ of $\Phi$ at each critical point are non-degenerate, with $|H_{\pm}(\omega)| \gtrsim d_M^{\pm}(x_0,x) |\omega|$, where the implicit constant is locally uniform in $x_0$ and $x$. These properties precisely prove the properties we needed to analyze the function $\Phi$ occuring in Proposition \ref{theMainEstimatesForWave}, where $F_*$ is the principal symbol $p$ of the pseudodifferential operator we are considering.

%We will find it more geometrically natural to work with the function $\Psi: S_{x_0}^* \to \RR$ given by $\Psi(\xi) = \phi(x,x_0,\xi)$. Then $\Phi( \xi ) = \Psi( \lambda(x_0,\xi) \xi )$. The map $\xi \mapsto \lambda(x_0,\xi) \xi$ is a diffeomorphism from $S^{d-1}$ to $S_{x_0}^*$. Both critical points . Thus it suffices to prove that the only critical points of $\Psi$ occur at $\xi^+$ and $\xi^-$, that $\Psi(\xi_{\pm}) = \pm d_M^{\pm}(x_0,x)$, and that the Hessian of $\Psi$ is appropriately non-degenerate.

%It is simpler to define $\Sigma(x_0,\xi)$ when there is a Riemannian metric $g$ on $U_0$ such that $F(x,v)^2 = \sum g_{ij}(x) v^i v^j$. In this case we take a system of normal coordinates around $x_0$, and define $\Sigma(x_0,\xi)$ to be the hyperplane through the origin in these coordinates perpendicular to $\xi$.  More precisely, we consider the exponential map $\exp: W \to U$, where $W$ is an open subset of $U_0 \times \RR^d$ containing $U_0 \times \{ 0 \}$, defined so that for each $x_0 \in U_0$ and each $u \in \RR^d$, the curve $c(t) = \exp(x_0,tv)$ is a geodesic where it is well defined, i.e. solving the system
%
%\[ \ddot{c}^a = - \sum\nolimits_{j,k} \Gamma^{a}_{jk}(c) \dot{c}^j \dot{c}^k, \]
%
%with the initial conditions $c(0) = x_0$ and $c'(0) = v$. For each $\xi \in \RR^d - \{ 0 \}$, we consider the hyperplane $H_\xi \subset \RR^d$ with normal vector $\xi$, and then define $\Sigma(x_0,\xi) = \exp(x_0,H_\xi)$, which is a hypersurface perpendicular to $\xi$ since $D_v \exp(x_0,0)$ is the identity matrix. % In other words, $\Sigma(x_0,\xi)$ is the hypersurface which, in normal coordinates at $x_0$, forms a hyperplane normal to $\xi$. Since normal coordinates are smooth, and smoothly depend on $x_0$, the family of hypersurfaces $\Sigma(x_0,\xi)$ is then a smooth function of $x_0$ and $\xi$.

%We would like to use the same definition in the Finsler setting. However, the exponential map on a Finsler manifold is in general only everywhere $C^1$. %\footnote[1]{In general the exponential map is only $C^1$ unless $M$ is a \emph{Berwald Finsler manifold}, as in \cite{AkbarZadeh}.}
% Thus the surfaces $\Sigma(x_0,\xi)$ will only be $C^1$, and thus the phase $\phi$ will only be $C^1$, and thus not be smooth enough to yield a parametrix for the half-wave equation.
%
%This argument is not significantly more complex in the Finsler manifold setting than if the operator $P$ gave $M$ a Riemannian metric rather than just a Finsler metric (e.g. for the operators associated with zonal multipliers on the sphere). But we must adapt certain results in the Riemannian manifold literature to the Finsler manifold setting, which does require some extra proof. We relegate these proofs to the appendix when necessary, mentioning citations for the required result in the Riemannian manifold literature that we are adapting.
%A fix is provided by using a kind of normal coordinates at a point adapted to a particular direction, defined in terms of the \emph{autoparallel exponential map} introduced in \cite{Pfeifer}. %, though related to older methods of Douglas and Thomas \cite{Douglas,Thomas}.
%This exponential map is given by $\text{Exp}: W \to U \times \RR^d$, where $W$ is an open subset of $U_0 \times \RR^d \times \RR^d$ containing $U_0 \times \{ 0 \} \times \{ 0 \}$, and smooth away from $U_0 \times \RR^d \times \{ 0 \}$, such that for each $x_0 \in V$, and each $(v,\xi) \in \RR^d \times \RR^d$, the pair of curves $(c(t), s(t)) = \text{Exp}(x_0,tv,\xi)$ solves the system
%%
%\[ \ddot{c}^a = - \sum\nolimits_{j,k} \Gamma^a_{jk}(c,s) \dot{c}^j \dot{c}^k \quad\text{and}\quad \dot{s}^a = - \sum\nolimits_{j,k} \Gamma^a_{jk}(c,s) \dot{c}^j s^k, \]
%
%such that $c(0) = x_0$, $\dot{c}(0) = v$, and where $s(0) = \mathcal{L}^{-1}(x_0,\xi)$. Because of the choice of initial conditions here, if we let $\pi: U \times \RR^d \to U$ be the standard projection map, then $D_v(\pi \circ \text{Exp})(x_0, 0, \xi)$ is the identity, and so the map $G_{x_0,\xi} = (\pi \circ \text{Exp})(x_0,\cdot,\xi)$ is a diffeomorphism onto a neighborhood of $x_0$. % and so $E_{x_0,\xi} = (\pi \circ \exp)(x_0,\cdot,\xi)$ is a diffeomorphism in a neighborhood of the origin. If we assume $V$ to have small enough diameter, we may assume that $E_{x_0,\xi}$ is a diffeomorphism onto an open set containing $V$ for all $x_0$ and $\xi$. % We use this diffeomorphism to define a covector field $\omega$ such that $E_{x_0,\xi}^* \{ \omega \}$ is the constant covector field that everywhere takes the value $\xi$. Now we can obtain a Riemannian metric $h(x_0,\xi, \cdot)$ on $V$ via the coefficients $h^{ij}(x_0, \xi, x) = g^{ij}( x_0, \omega(x_0) )$, where $g^{ij}$ are the coefficients corresponding to Riemannian approximations of the dual metric $F_*$. We finally take a normal coordinate system $F_{x_0,\xi}$ for the metric $h$ and define 
%We define
%
%\[ \Sigma(x_0,\xi) = G_{x_0,\xi}(H_\xi), \]
%
%where $H_\xi$ is the hyperplane in $\RR^d$ perpendicular to $\xi$.

%We note that $G_{x_0,\xi}$ is in general only defined locally, which is why we must work with sufficiently small open sets $U \subset U_0$ such that for all $x_0 \in U$ and $\xi \in \RR^d$, the map $G_{x_0,\xi}$ is a diffeomorphism onto $U$. We will also assume that $U$ is small enough that each pair of points in $U$ is connected by a unique forward geodesic. This is an unproblematic assumption for the application of our arguments to Proposition \ref{theMainEstimatesForWave}, since there we were able to take an arbitrarily fine partition of unity before switching into coordinates.% Since $DE_{x_0,\xi}(0) = I$, the hypersurface $\Sigma(x_0,\xi)$ is perpendicular to $\xi$ at $x_0$, and this definition gives a smooth family of smooth hypersurfaces for any Finsler manifold.

%(TODO: does this formula look better in cotangent coordinates? ). We have need of using this exponential map is because the standard exponential map, defined in terms of geodesics, and which we can write in terms of the autoparallel exponential map as
%
%\[ (x_0,u) \mapsto \exp(x_0, u, u), \]
%
%fails to be smooth when $u = 0$ on a general Finsler manifold. On the other hand, the auto-parallel exponential map $\exp(x_0,u,v)$ is smooth away from $v = 0$. For each $x_0$ and $v \neq 0$, the inverse of the map map $u \mapsto \exp(x_0,u,v)$ gives a smooth coordinate system in a neicghborhood of $x_0$, and provides a substitute for normal coordinates, at least when in comes to geodesics pointing in the direction $v$. If 

%Now we can define $\Sigma(x_0,\xi)$ for $\xi \in T^*_{x_0} M - \{ 0 \}$. Let $v \in \RR^d$ be the Legendre transform of $\xi$ at $x_0$, i.e. the vector $v$ such that $\xi = \sum g_{ij}(x_0,v) v^j$, or equivalently, if we define
%
%\[ g^{ij} = \frac{1}{2} \frac{\partial^2 F_*}{\partial \xi^i \partial \xi^j}, \]
%
%then $v^i = \sum g^{ij}(x_0,\xi) \xi_j$. If we consider the coordinate system given by the inverse of $u \mapsto \exp(x_0,u,v)$, then the hypersurface $\Sigma(x_0,\xi)$ is then precisely the surface which in this coordinate system is the unique \emph{hyperplane} conormal to $\xi$.

%If we fix $\alpha$, then for each $x_0 \in U_\alpha$, and each $\xi \in T^*_{x_0} M$, we will also need to define an additional coordinate system $z(\cdot,x_0,\xi)$ defined in a neighborhood of $x$, which is an analogue of normal coordinates for Finsler manifolds called \emph{Douglas-Thompson normal coordinates}. Like normal coordinates, defined in terms of a system of ordinary differential equations. Namely, we have $z(x,x_0,\xi) = z$, if and only if the unique solution to the system
%
%\[ (c^i)''(t) = - \sum\nolimits_{a,b} \gamma^i_{ab}(c,s) (c^a)'(t) (c^b)'(t) \quad\text{and}\quad s'(t) = - \sum \gamma^i_{ab}(c,s) (c^a)'(t) s^b(t) \]
%
%with the initial conditions $c(0) = x_0$, $c'(0) = z$, and $s(0) = \xi$ has $c(1) = x$. If 

%\begin{remark}
%    Using the notation in Proposition \ref{theMainEstimatesForWave}, if $\alpha: S^{d-1} \to S_x^*$ is given by $\alpha(\xi) = \lambda(x_0,\xi) \xi$, then $(\Psi \circ \alpha)(\xi) = \Phi(\xi)$. Then $\alpha$ is a diffeomorphism, and so it follows from Proposition \ref{triangleLemma} that the critical points of $\Phi$ are equal to $\alpha^{-1}(\pm \xi_0)$, where $\xi_0$ is the unique covector in $S_x^*$ corresponding to the geodesic from $x_0$ to $x_1$. Since the Hessian of a function is a diffeomorphism invariant at critical points of a manifold, it also follows from Proposition \ref{theMainEstimatesForWave} that the Hessian of $\Phi$ at $\alpha^{-1}(\pm \xi_0)$ is non-degenerate, with each eigenvalue having magnitude exceeding a constant multiple of 

%    In the statement of Proposition \ref{triangleLemma}, we have applied a change of coordinates as compared to the application
%\end{remark}

In order to prove \ref{triangleLemma}, we rely on a geometric interpretation of $\Psi$ following from Hamilton-Jacobi theory.

\begin{lemma} \label{HamiltonLemma}
    Consider the setup to Proposition \ref{triangleLemma}. For any $\xi \in S_{x_0}^*$,
    %
    \[ |\Psi(\xi)| = \begin{cases} \text{the length of the shortest curve from $H(x_0,\xi)$ to $x_1$} & : \text{if}\ \Psi(\xi) > 0, \\ \text{the length of the shortest curve from $x_1$ to $H(x_0,\xi)$} & : \text{if}\ \Psi(\xi) < 0. \end{cases} \]
\end{lemma}
\begin{proof}
    We rely on a construction of $\phi$ from Proposition 3.7 of \cite{Treves2}, which we briefly describe. Fix $x_0$ and $\xi$. Then there is a unique covector field $\omega: H(x_0,\xi) \to \RR^d$ which is everywhere perpendicular to $H(x_0,\xi)$, with $\omega(x_0) = \xi$ and with $F_*(x,\omega(x)) = F_*(x_0,\xi)$ for all $x \in H(x_0,\xi)$. There exists a unique point $x(\xi) \in H(x_0,\xi)$ and a unique $t(\xi) \in \RR$, such that the unit speed geodesic $\gamma$ on $M$ with $\gamma(0) = x(\xi)$ and $\gamma'(0) = \mathcal{L}^{-1}( x(\xi), \omega(x(\xi)) )$ satisfies $\gamma( t(\xi) ) = x_1$. We then have $\Psi(\xi) = t(\xi)$. If $t(\xi)$ is negative, then $\gamma|_{[t(\xi),0]}$ is a geodesic from $x_1$ to $x_0$, and if $t(\xi)$ is positive, $\gamma|_{[0,t(\xi)]}$ is a geodesic from $x_0$ to $x_1$. Because $\gamma$ is a geodesic, the geometric interpretation then follows if $U$ is a suitably small neighborhood such that geodesics are length minimizing.
\end{proof}

% and relies on the fact discussed in the introduction that the Hamiltonian flow of the principal symbol $p$ of $P$ on $M$ is the geodesic flow of the Finsler metric induced by $p$ on $M$. Namely, for $\xi \in S_{x_0}$, the value $\psi(\xi)$ is the length of the unique geodesic starting at $x_1$ and ending at a point on $\Sigma(x_0,\xi)$, passing through $\Sigma(x_0,\xi)$ orthogonally. More precisely, there exists a unique point $x(\xi) \in \Sigma(x_0,\xi)$ and a unique value $t(\xi) \in \RR$, such that if $\eta = \eta(\xi)$ is the unique unit covector orthogonal to $\Sigma(x_0,\xi)$ at $x(\xi)$ and oriented outward from $\Sigma(x_0,\xi)$ in the same direction as $\xi$, then $x_1 = \exp_{x(\xi)}(t(\xi) \eta(\xi) )$, and we then have $\psi(\xi) = t(\xi)$.
%



%\begin{figure}[h]
%        \centering
%        \includegraphics[width=0.4\textwidth]{Geodesics.eps}
%\end{figure}

%\noindent That a unique point $x(\xi)$ exists with these properties follows if we choose the diameter of $V$ to be suitably small, depending on the Finsler metric $M$.

% to be smaller than the normal injectivity radius of $\Sigma(x_0,\xi)$ for each $\xi \in S_{x_0}$. That the point $x(\xi)$ lies in $U$ follows because $d(\Sigma(x_0,\xi), x_1) \leq d(x_0,x_1)$, and $N(x_1,d(x_0,x_1)) \subset U$. In particular, if all sectional curvatures of $M$ are upper bounded by $\kappa^2$, then $\psi$ exists if we choose the diameter of $U$ to be smaller then $(\pi/2) \kappa^{-1}$, since, by Lemma 2.3 of \cite{Grimaldi}, $(\pi/2) \kappa^{-1}$ is smaller than the normal injectivity radius of $\Sigma(x_0,\xi)$. In particular, this result is satisfied since the diameter of $U$ is bounded by $2.5 \varepsilon_M$, and $\varepsilon_M < 0.4 (\pi/2) \kappa^{-1}$.

It follows immediately from Lemma \ref{HamiltonLemma} that
%
\begin{equation}
    \Psi(\xi_+) = d_M^+(x_0,x_1) \quad\text{and}\quad \Psi(\xi_-) = - d_M^-(x_0,x_1).
\end{equation}
%
A simple geometric argument also shows $\Psi(\xi^+)$ is the maximum value of $\Psi$ on $S_{x_0}^*$, and $\Psi(\xi^-)$ is the minimum value on $S_{x_0}^*$, so that these two points are both critical. Indeed, the point $x_0$ lies in $H(x_0,\xi)$ for all $\xi \in \RR^d - \{ 0 \}$. Thus the shortest curve from $x_0$ to $x_1$ is always longer than the shortest curve from $H(x_0,\xi)$ to $x_1$. Similarily, the shortest curve from $x_1$ to $x_0$ is always longer than the shortest curve from $x_1$ to $H(x_0,\xi)$. Thus
%
\begin{equation}
    - d_M^-(x_0,x_1) \leq \Psi(\xi) \leq d_M^+(x_0,x_1),
\end{equation}
%
and so $\Psi(\xi^-) \leq \Psi(\xi) \leq \Psi(\xi^+)$ for all $\xi \in $. All that remains is to prove that $\xi^+$ and $\xi^-$ are the \emph{only} critical points of $\Psi$, and that these critical points are appropriately non-degenerate.

%In this first part of the argument, the constant $\varepsilon_M$ need only be smaller than the injectivity radius of the set $U$, which is positive since $U$ is precompact, but in the proof that $\pm \xi_0$ are nondegenerate critical points we may need to pick $\varepsilon_M$ even smaller.

In order to simplify proofs, we employ a structural symmetry to reduce the number of cases we need to analyze. Namely, if one defines the reverse Finsler metric $F_\rho(x,v) = F(x,-v)$, then $F_\rho^*(x,\xi) = F_*(x,-\xi)$, and so the associated function $\Psi^\rho$ which is the analogue of $\Psi$ for $F^\rho$ satisfies $\Psi^\rho(\xi) = -\Psi(-\xi)$. The critical points of $\Psi$ and $\Psi^\rho$ are thus directly related to one another, which allows us without loss of generality to study only points with $\Psi \leq 0$ (and thus only study geodesics beginning at $x_1$).

\begin{proof}[Proof that $\xi^+$ and $\xi^-$ are the only critical points]
    Fix $\xi^* \in T_{x_0} M - \{ \xi^{\pm} \}$. Using the notation defined above, let $x_* = x(\xi^*)$ and $t_* = t(\xi^*)$. Using the symmetry above, we may assume without loss of generality that $\Psi(\xi^*) \leq 0$. Since $\xi^-$ is not perpendicular to $H(x_0,\xi^*)$ at $x_0$, we have $x_* \neq x_0$. If $\Psi(\xi^*) < 0$, let $\gamma$ be the unique unit speed forward geodesic with $\gamma(0) = x_1$ and $\gamma(t_*) = x_*$. If $\Psi(\xi^*) = 0$, let $\gamma$ be the unique unit speed forward geodesic with $\gamma'(0)$ equal to the Legendre transform of $\xi^*$. Pick $\eta$ such that $\eta \cdot (x_* - x_0) \neq 0$, and then, for $t$ suitably close to $t_*$, define a smooth map
    %
    \begin{equation} \xi(t) = \frac{\xi^* + a(t) \eta}{F_*(x_0, \xi^* + a(t) \eta)}, \end{equation}
    %
    into $S_{x_0}^*$, where
    %
    \begin{equation} a(t) = - \frac{\xi^* \cdot ( \gamma(t) - x_0 )}{\eta \cdot ( \gamma(t) - x_0 )} \end{equation}
    %
    is defined so that $\gamma(t) \in H(x_0, \xi(t))$. Then $\Psi( \xi(t) ) = -t$, and differentiation at $t = t_*$ gives $D\Psi( \xi^* ) ( \xi'(t_*) ) = -1$. In particular, $D \Psi( \xi^* ) \neq 0$, so $\xi^*$ is not a critical point. \qedhere
    %
    % gamma(t_*) = x_*
    %   xi(t_*) = xi^*
    %
    %  xi(t) ( gamma(t) - x_0 ) = (t - t_*) [ xi'(t_*) [ gamma(t) - x_0 ] + xi^* gamma'(t) ]
    %   xi(t) = xi^* + a(t) eta
    %   xi(t) ( gamma(t) - x_0 ) = xi^* ( gamma(t) - x_0 ) + a(t) eta ( gamma(t) - x_0 )
    %                               = 0 if a(t) = - xi^* ( gamma(t) - x_0 ) / eta ( gamma(t) - x_0 )
    %   Well defined near xi^* if eta ( x_* - x_0 ) != 0
    %       Derivative in t is
    %           a'(0) = - [ [eta ( x_* - x_0 )] [xi^* gamma'(t_*)] / [ eta (x_* - x_0) ]^2 = - 1 / eta (x_* - x_0)
    %   
    %   Which is good provided that xi^* gamma'(t_*) != 0 and eta (x_* - x_0) != 0
    %
    %   And this is easy because xi^* gamma'(t_*) = 1
    %   So as long as eta is not a constant multiple of xi^* we're good.

    % Define xi(t) = xi^* + a xi^+
%    Let $v(t) = \mathcal{L}(\gamma(t), \gamma'(t))$.


    %
    %

%    For $t$ suitably close to $x_0$, we can find a smooth function $\xi(t) \in S_{x_0}^*$ with $\xi(0) = \xi^*$, and such that $\gamma(t) \in H(x_0, \xi(t))$ for each $t$. Using the fact that $\gamma|_{[t,t_*]}$ is a curve from $H(x_0,\xi(t))$ to $x_1$ of length $t_* - t$, we find that $\Psi(\xi(t)) \leq t_* - t$. Since $\Psi(\xi(0)) = t_*$, taking $t \to 0^+$ gives $D\Psi(\xi^*)(\xi'(0)) \leq -1$. In particular, $D\Psi(\xi^*) \neq 0$, and so $\xi^*$ is not a critical point. \qedhere
    \end{proof}

We now analyze the non-degeneracy of the critical points $\xi^+$ and $\xi^-$.








%It is here that the precise choice of surfaces $\Sigma(x_0,\xi)$ will aid us because to prove non-degeneracy one must look at the second-order behavior of the function $\Psi$, rather than in the last proof where it was only necessary to study first-order behavior of $\Psi$.

    % For v in T_p M, we get a linear isomorphism
    %
    % d exp_{p,v}: T_p M -> T_{exp_p(v)} M
    %
    % Given by differentiating, identifying
    % T_v T_p M with T_p M in the canonical way.
    %
    % d exp_p is *not* an isometry, but it does satisfy
    %
    % < d exp_{p,v}(v), d exp_{p,v}(w) > = < v, w >.
    %
    % Now define the Curvature Tensor
    %
    % R(X,Y)Z = Nabla_X Nabla_Y Z - Nabla_Y Nabla_X Z - Nabla_{[X,Y]} Z
    %
    % Consider alpha(s,t) = exp_p( (cos(s) v + sin(s) w) t )
    %
    % Let T = alpha_*(d_t), J(t) = alpha_*(d_s|_{(0,t)}).
    %
    % Then J is a Jacobi field, with J(t) being a tangent vector at exp_p( vt )
    %
    % J(0) = 0
    % J'(0) = w
    % J''(0) = R(T,J) T|_0 = 0
    % < J,J >''' = R(T,w) T
    %
    % Let f(t) = < J(t), J(t) >_g
    %
    % Then f(0) = < J(0), J(0) > = < 0, 0 > = 0
    %
    % And f'(0) = 2 < J'(0), J(0) > = 2 < w, 0 > = 0
    %
    % And f''(0) = 2 [ < J''(0), J(0) > + < J'(0), J'(0) > ]
    %            = 2 |w|_g^2
    % 
    % And f'''(0) = 2 [ < J'''(0), J(0) > + 3 < J''(0), J'(0) > ]
    %             = 6 < J''(0), w > = 0
    %
    % And f^(4)(0) = 2 < J^(4)(0), J(0) > + 5 < J'''(0), J'(0) > + 3 < J''(0), J''(0) >
    %              = 5 < J'''(0), J'(0) > 
    %              = 5 < R(T,w) T, w >
    %
    % So we conclude that | J(t) |^2 = t^2 + (5/24) < R(T,w) T, w > t^4
    %
    % For small s, s^{-1} [ alpha(t,s) - alpha(t,0) ] should be
    % close to J(t), so we are saying that
    % | alpha(t,s) - alpha(t,0) |^2 = s^2 [ t^2 + (5/24) < R(T,w) T, w > t^4 + O(t^5) ]
    % In particular, we can guarantee that | alpha(t,s) - alpha(t,0)| ~ st
    %
    % Now alpha(d,0) = x_0, and alpha(t,s) = x
    % and for |t - d| <= C s^2
    %
    % Now | alpha(d,s) - x_0 | ~ sd
    %
    % and | alpha(d,s) - x | <= C s^2
    %
    % so |x - x_0| ~ sd - Cs^2 ~ sd
    %
    % Argument why it must be critical: easy argument it's a maximum
    % because x_0 lies on all of the surfaces Sigma(x_0,xi), so
    % the geodesics from x_1 must hit Sigma(x_0,xi) - { x_0 } before
    % hitting x_0.


    % In particular alpha(t,s) - alpha(t,0) = s [ t^2 + (5/24) < R(T,w) T, w > t^4 + O(t^5) ]

    % 
    % exp_p( ( cos(s) v + sin(s) w ) t )
    % exp_p( ( v - O(s^2)v + sw + O(s^3)w ) t )

    % Consider |dexp_{p,v}(tw)|^2, where v and w are orthonormal. Then
    %
    %   |dexp_{p,v}(tw)|^2 = t^2 - [< R(w, v ) v, w > / 3] t^4 + O(t^5)
    %
    % By polarization identity
    %
    %   < x, y > = (1/2) [  |x + y|^2 - |x|^2 - |y|^2   ]
    %
    % < A(tw),  

\begin{proof}[Proof that $\xi^+$ and $\xi^-$ are non-degenerate]
    Using symmetry, it suffices without loss of generality to analyze the critical point $\xi^-$ rather than $\xi^+$. Let $H_-: T_{\xi^-} S_{x_0}^* \to \RR$ be the Hessian of $\Psi$ at $\xi^-$. Let $v_0 = \mathcal{L}_{x_0}^{-1} \xi^-$, and using the notation of the last argument, let $l = t(\xi^-)$. Consider a curve $\xi(a)$ valued in $S_{x_0}^*$ with $\xi(0) = \xi^-$ and $\zeta = \xi'(0)$ for some $\zeta \in T_{\xi^-}^* S_{x_0}$. Then the second derivative of $\Psi(\xi(a))$ at $a = 0$ is $H_-( \zeta )$, and so our proof would be complete if we could show that the function $L(a) = - \Psi( \xi(a) )$, which is the length of the shortest geodesic from $x_1$ to $H(x_0,\xi(a))$, satisfies $L''(0) \geq C l |\zeta|$, for a constant $C > 0$ uniform in $x_0$, $x_1$, and $\zeta$.

    Consider the partial function $\text{Exp}_{x_1}: \RR^d \to U_0$ obtained from the exponential map at $x_1$. If $r$ is chosen suitably small, there exists a neighborhood $V$ of the origin in $\RR^d$ such that $E = \text{Exp}_{x_1}|_V$ is a diffeomorphism between $V$ and $U$. Unlike in Riemannian manifolds, in general $E$ is only $C^1$ at the origin, though smooth on $V - \{ 0 \}$. However, for $|v| = 1$ and $t > 0$ we can write $E(tv) = (\pi \circ \varphi)( x_1, \mathcal{L}_{x_1} v, t)$, where the partial function $\varphi: (T^* U_0 - 0) \times \RR \to (T^* U_0 - 0)$ is the flow induced by the the Hamiltonian vector field $( \partial_\xi F_*, - \partial_x F_*)$ on $T^* U_0 - 0$, and $\pi$ is the projection map from $T^* U_0 - 0$ to $U_0$. Where defined, $\varphi$ is a smooth function since the Hamiltonian vector field is smooth, and so it follows by homogeneity and the precompactness of $U$ that the partial derivatives $(\partial^\alpha E_x)(v)$ are uniformly bounded for $v \neq 0$ and $x \in U$. It thus follows from the inverse function theorem that there exists a constant $A > 0$ such that for all $x_1$ and $x$ in $U$ with $x \neq x_1$,
    %
    \begin{equation} \label{GBounds}
        |\partial_j G_{x_1}(x)| \leq A \quad\text{and}\quad |(\partial_j \partial_k G_{x_1})(x)| \leq A.
    \end{equation}
    %
    We can also pick $A$ to be large enough that for all $x \in U$, and all $v,w \in \RR^d - \{ 0 \}$,
    %
    \begin{equation}
        A^{-1} |w|^2 \leq \sum\nolimits_{ij} g_{ij}(x,v) w^i w^j \leq A |w|^2.
    \end{equation}
    %
    and such that for all $x \in U$ and $v \in \RR^d - \{ 0 \}$,
    %
    \begin{equation} \label{gijbounds}
        |\partial_{x_k} g_{ij}(x,v)| \leq A.
    \end{equation}
    %
    %With notation now setup, we now prove $H_-$ is appropriately nondegenerate.

    Since $\zeta \in T_{\xi^-} S_{x_0}^*$, and $S_{x_0}^* = \{ \xi : F_*(x_0,\xi)^2 = 1 \}$, by Euler's homogeneous function theorem we have
    %
    \begin{equation} \label{DIOAWJDIOWAJDOIWAJ14}
        \sum\nolimits_{ij} g^{ij}(x_0,\xi^-) \xi'(0) \xi^-_j = \frac{1}{2} \sum\nolimits_{i,j} \frac{\partial^2 F_*^2}{\partial \xi_i \partial \xi_j}(x_0,\xi^-) \zeta_i \xi^-_j = \frac{1}{2} \sum \frac{\partial F_*^2}{\partial \xi_j}(x_0,\xi^-) \zeta_j = 0.
    \end{equation}
    %
    Differentiating $F_*(x_0,\xi(a))^2 = 1$ twice with respect to $a$ at $a = 0$ yields that
    %
    \begin{equation} \label{AIWODJWAO314213}
         \sum\nolimits_{i,j} g_{ij}(x_0,\xi^-) \xi''_i(0) \xi^-_j = - \sum\nolimits_{i,j} g_{ij}(x_0,\xi^-) \zeta^i \zeta^j
    \end{equation}
    %
    Define vectors $n(a)$ by setting $n^i(a) = \sum g^{ij}(x_0,\xi^-) \xi_j(a)$. Then $n(a)$ is the normal vector to $H(x_0, \xi(a))$ with respect to the inner product with coefficients $g_{ij}(x_0,v_0)$.
    %
%    \begin{equation}
%        H(x_0,\xi(a)) = \left\{ x : \sum\nolimits_{ij} g_{ij}(x_0,v_0) (x^i - x_0^i) n^j(a) = 0 \right\}.
%    \end{equation}
    %
    Let $u(a)$ be the orthogonal projection of $v_0$ onto the hyperplane $\{ v : \xi(a) \cdot v = 0 \}$ with respect to the inner product $g_{ij}(x_0,v_0)$. If we define
    %
    \begin{equation}
        c(a) = \sum g_{ij}(x_0,v_0) v_0^i n^j(a) = \sum g^{ij}(x_0,\xi^-) \xi^-_i \xi_j(a),
    \end{equation}
    %
    then $u(a) = v_0 - c(a) n(a)$. Note that \eqref{DIOAWJDIOWAJDOIWAJ14} and \eqref{AIWODJWAO314213} imply $c(0) = 1$, $c'(0) = 0$, and $c''(0) \leq - |\zeta|^2 / A$. Also $n(0) = v_0$ and $|n'(0)| \leq A |\zeta|$.
    %
    % 1 - c(a)^2 = 1 - ( 1 - a^2 |zeta|^2 / A )^2
    %            = 2 a^2 |zeta|^2 / A

    Let $x(s) = x_0 + s u(a)$ and let $R(s)$ be the length of the geodesic from $x_1$ to $x(s)$. To control $R(s)$, define $y(s) = G(x(s))$, and consider the variation $A(s,t) = E( t y(s) )$, defined so that $t \mapsto A(s,t)$ is the geodesic from $x_1$ to $x(s)$. The Gauss Lemma for Finsler manifolds (see Lemma 6.1.1 of \cite{BaoChern}) implies that
    %
    \begin{equation}
        A^{-1} R(s) \leq |y(s)| \leq A R(s).
    \end{equation}
    %
    Define $T(s) = (\partial_t A)(s,1)$ and $V(s) = (\partial_s A)(s,1)$.
    %
%    \begin{equation}
%        T(s) = (\partial_t A)(s,1) = \sum\nolimits_{i,j} \frac{\partial E^i}{\partial y^j}( y(s) ) y^j(s) e_i \quad\text{and}\quad V(s) = (\partial_s A)(s,1) = u(a).
%    \end{equation}
    %
    Then \eqref{FirstVariationFormula2} implies
    %
    \begin{equation} \label{RSFirstDerivativeEquation}
        R(s) R'(s) = \sum\nolimits_{i,j} g_{ij}( x(s), T(s) ) V^i(s) T^j(s).
    \end{equation}
    %
    Again, the Gauss Lemma implies
    %
    \begin{equation} \label{Idiawjdiwaj213123}
        A^{-1} R(s) \leq |T(s)| \leq A R(s).
    \end{equation}
    %
    We can write
    %
    \begin{equation}
        T^i(s) = \sum\nolimits_{i,j} (\partial_j E^i)( y(s)) y^j(s)
    \end{equation}
    %
    and
    %
    \begin{equation}
        V^i(s) = \sum\nolimits_{i,j,k} (\partial_j E^i) (y(s)) (\partial_k G^j)( x(s)) u^k(a) = u^i(a).
    \end{equation}
    %
    In particular, $T(0) = v_0$, so $R'(0) = \sum g_{ij}(x_0, v_0) u^i(a) v_0^j = 1 - c(a)^2$. Cauchy-Schwartz applied to \eqref{RSFirstDerivativeEquation} also tells us that
    %
    \begin{equation} \label{wdkawoidj141}
        |R'(s)| \lesssim_A |u(a)|.
    \end{equation}
    %
    Note that $u(0) = 0$, so if $a$ is small enough, then we have $|u(a)| \leq l/2$. Taylor's theorem applied to \eqref{wdkawoidj141}, noting $R(0) = l$ then gives that for $|s| \leq 1$,
    %
    \begin{equation} \label{RBound}
        l/2 \leq R(s) \leq 2l.
    \end{equation}
    %
    Differentiating \eqref{RSFirstDerivativeEquation} tells us that
    %
    \begin{equation} \label{FFFF213123}
    \begin{split}
        R'(s)^2 + R''(s) R(s) &= \sum\nolimits_{i,j,k} \Bigg[ (\partial_{x_k} g_{ij})( x(s), T(s) ) u^i(a) T^j(s) u^k(a) \Bigg]\\
        &\quad\quad\quad\quad + \Bigg[ (\partial_{v_k} g_{ij} )(x(s), T(s)) u^i(a) T^j(s) (\partial_s T^k)(s)   \Bigg]\\
        &\quad\quad\quad\quad\quad\quad + \Bigg[ g_{ij}( x(s), T(s) ) u^i(a) (\partial_s T^j)(s) \Bigg].
    \end{split}
    \end{equation}
    %
    Write the right hand side as $\text{I} + \text{II} + \text{III}$. Using \eqref{gijbounds}, \eqref{Idiawjdiwaj213123}, \eqref{RBound}, and the triangle inequality gives
    %
    \begin{equation} \label{IBound}
        |\text{I}| \lesssim R(s) |u(a)|^2 \lesssim l |u(a)|^2.
    \end{equation}
    %
    Since $\partial_{v_k} g_{ij} = (1/2) (\partial^3F^2 / \partial v_i \partial v_j \partial v_k)$, applying Euler's homogeneous function theorem when summing over $j$ implies that
    %
    \begin{equation} \label{IIBound}
        \text{II} = 0.
    \end{equation}
    %
    Applying Cauchy-Schwarz and \eqref{GBounds}, we find
    %
    \begin{equation} \label{IIIBound}
        |\text{III}| \lesssim |u(a)| |T'(s)| \lesssim |u(a)|^2 [|y(s)| + 1] \lesssim (l + 1) |u(a)|^2.
    \end{equation}
    %
    But now combining \eqref{IBound}, \eqref{IIBound}, \eqref{IIIBound}, and rearranging \eqref{FFFF213123} shows that
    %
    \begin{equation}
        |R''(s)| \lesssim l^{-1} |u(a)|^2 + (1 + l^{-1}) |u(a)|^2 \lesssim l^{-1} |u(a)|^2.
    \end{equation}
    %
    Taylor's theorem implies there exists $B > 0$ depending only on $A$ and $d$ such that
    %
    \begin{equation} \label{WADAWD21312}
        |R(s) - (R(0) + s R'(0))| \leq B l^{-1} |u(a)|^2 s^2.
    \end{equation}
    %
    Since $R(0) = $l, $R'(0) = 1 - c(a)^2$, $c(0) = 1$, $u(0) = 0$, $c'(0) = 0$, $|u'(0)| \leq A |\zeta|$, and $c''(0) \leq - |\zeta|^2 / A$, we conclude from \eqref{WADAWD21312} that as $a \to 0$, if $s > 0$ then
    %
    \begin{equation}
    \begin{split}
        R(-s) &\leq l - s R'(0) + B l^{-1} |u(a)|^2 s^2\\
        &\leq l - s ( A^{-1} |\zeta|^2 a^2 ) + A^2 B l^{-1} |\zeta|^2 s^2 a^2 + O(a^3 ( s + l^{-1} s^2)).
    \end{split}
    \end{equation}
    %
    % |u(a)|^2 = sum u^i(a)^2
    % Derivative is 2 u . u'(a) = 0
    % Derivative is 2 u' u' <= A^2 |zeta|^2
    For all $s$, $L(a) \leq R(s)$. Optimizing by picking $s = l / 2 A^3 B$ gives
    %
    \begin{equation}
        L(a) \leq R(-s) \leq l - l |\zeta|^2 a^2 / 4 A^4 B + O( a^3 ).
    \end{equation}
    %
    Taking $a \to 0$ and using that $L(0) = l$ gives that $L''(0) \leq - l |\zeta|^2 / 4 A^4 B$, so setting $C = 1/4 A^4 B$, we find we have proved what was required. \qedhere
    \begin{comment}

    % - 2 c c''

    \[ R(s) \leq l + a^2 s + B a^2 l^{-1} |\zeta|^2 s^2 + o( a^2 ) [l^{-1} s^2 + |s|]. \]
    %
    Optimizing by picking $s = - l |\zeta| / 2 B$ gives
    %
    \[ R(s) \leq l - \frac{la^2}{4 B |\zeta|^2} \]


    % t = 1 / 2 B |zeta|^2

    % l a^2 [ - 1 + B t |zeta|^2 ]

    Picking $a^2 s + B a l^{-1} |\zeta|^2 \leq - l a^2$
    % t^2 B l a |zeta|^2 - t a^2 l
    % l a t ( t B |zeta|^2 - a )

    % - t a^2 l + t^2 B l |zeta|^2


    %
    \[ A^2 |u(a)| |(\partial_j T)(s)| \]

    $A^2 |u(a)| |(\partial_j T)(s)| \leq A^2 |u(a)| [ A |u(a)|^2  ]$

    $A |u(a)|$

    % First term is  << R(s) |u(a)|^2
    %
    %
    % Second term is 0 because of homogeneity
    %
    % Third term is by Cauchy Schwarz is << |u(a)| |partial_s T| << |u(a)| [ |y(s)| |u(a)| + |u(a)| ]
    %
    % So in total, << |u(a)|^2  if R(s) << 1
    %
    %
    % But then rearranging gives that |R''(s)| << l^{-1} |u(a)|^2.



    Since $\bar{T}(0) = v_0$, if we let $b(a) = \sum g_{ij}(x_0,v_0) n^i(a) v_0^j$ then
    %
    \[ B(0) = \sum g_{ij}(x_0, v_0) u^i(a) v_0^j = \sum g_{ij}(x_0,v_0) ( v_0^i - c(a) n^i(a) ) v_0^j = 1 - c(a) b(a). \]
    %
    We have
    %
    \[ B'(s) = g_s( u(a), 
      ) \]

    %
    \[ T(s) = DE( v_s ) \{ v_s \} \]


    The variation fields $T = \partial_t a(s,t)$ and $V = \partial_s a(s,t)$ are given by
    %
    \[ T(s,t) = DE( t v_s ) \{ v_s \} \quad\text{and}\quad V(s,t) = t DE( t v_s ) DG( x_0 + s u(a) ) u(a) . \]
    %
    Note that $V(s,1) = u(a)$. If we let $\bar{T} = T(s,t) / F(  )$ be the unit vector with respect to the Finsler metric pointing in the direction of $T$, then $\bar{T}(0,1) = w_0$

    and so by the Gauss Lemma for Finsler manifolds (see Lemma 6.1.1 of \cite{BaoChern}), if we define $g_{ij}(s,t) = g_{ij}( a(s,t) , T(s,0))$, then
    %
    \[ \sum g_{ij}(s,t) T^i V^j = t \sum g_{ij}(s,0) v_s^i [DG(x_0 + s u(a)) u(a)]^j  \]

    %
    %
    %
    %
    %
    % a(t,s) = Exp( t G(x_0 + s u(a)) )
    %
    % T = DExp( t G(x_0 + s u(a)) ) { G(x_0 + s u(a)) }
    % V = DExp( t G(x_0 + s u(a)) ) { t DG(x_0 + s u(a)) u(a) }
    %   = t u(a)
    %
    % < T, V > = g_0 < G(x_0 + su(a)), t DG(x_0 + su(a)) u(a) >
    %          = t g_0 < x(s), x'(s) >          HAVE TO NORMALIZE TODO
    %
    % < TBar , V > = t g_0 < x(s), x'(s) > / |x(s)|_s
    %
    %  Where x(s) = G(x_0 + s u(a)) 
    %
    % So V < T, V > = t [ g_s < x'(s), x'(s) > / |x(s)| + g_s < x(s), x''(s) ] > / |x(s)|_s - 2 < x(s), x'(s) >^2 / |x(s)|_s^2 ]
    %
    % So we need to estimate < x'(s), x'(s) > / |x(s)|
    %
    %       <<      < u(a), u(a) > / l = O( a^2 / l )
    %
    % And    < x(s), x''(s) / |x(s)| 
    %
    %       << A a^2
    %
    %
    % And    < x(s), x'(s) >^2 / |x(s)|^2
    %
    %       << a^2 / l
    %   
    %
    %
    %
    %
    %
    %
    %       So if L(s) is the length of the geodesic from x1 to x_0 + s u(a)
    %
    %           Then L(0) = l
    %               L'(0) = < Tbar, V > = < v_0 , DG(x0) { u(a) } > = < v_0, DG(x_0) { w_0 - b(a) n(a) } >
    %                     = - a^2 / 2
    %
    %               L''(s) = V < Tbar, V > is bounded by (A + 1/l) a^2
    %
    %
    %           Thus | L(s)- l + (a^2/2) s | <= (A + 1/l) a^2 s^2 / 2
    %           So L(s) >= l (-a^2/2) s - (A + 1/l) a^2 s^2 / 2
    %           Let s = - eps
    %           Then L >= l + (a^2/2) eps [ 1 - eps (A + 1/l) ] >= (a^2/2) eps [ 1 - 2 eps / l ]
    %
    % Assume that 1/l >= A, i.e. l <= 1/A
    %
    % Then pick eps = l / 4. Then L >= l + l a^2/ 16.
    %
    %
    %
    % x(s) = G(x_0 + s u(a))
    % x'(s) = DG(x0 + s u(a)) { u(a) }
    % x''(s) = D^2G(x0 + su(a)) { u(a), u(a) }
    %
    % Now since u(a) = ( w_0 - g( w_0, n(a) ) n(a) )
    %
    % n'(0) = zeta^flat
    % n''(0) = zeta''^flat
    %
    % F(x_0, xi(s))^2 = 1
    % F^2_{jk} zeta_j zeta_k + F^2_j xi'' = 0
    % Thus < zeta, zeta > + < xi'', xi^- > = 0
    %   So < xi'', xi^- > = -1
    %
    % If b(a) = g( w_0, n(a) )
    %
    % b(0) = 1
    % b'(0) = 0
    % b''(0) = -1
    %
    % u(a) = w_0 - b(a) n(a)
    % u'(0) = - n'(0)
    %       = - zeta^flat
    % u''(0) = - b''(0) n(0) - b(0) n''(0)
    %        = w_0 - n''(0)
    % So < u'', w_0 > = 2
    %
    % So < u(s), w_0 > = s^2 + O(s^3).
    % 
    % If we consider the variation c_t to be the geodesic from x_1 to x_0 + t u(s), then V(s) = u(s)
    %   L'(0) = < u(s), w_0 > = s^2 + O(s^3)
    %   L''(t) = V { <V,T> }
    %           Let x(t) = G( x_0 + t u(s) )
    %           Then Exp( x(t) ) = x_0 + u(s)
    %           Then the geodesic from x_1 to x_0 + t u(s) is   a -> Exp( a x(t) )
    %           And so T = D Exp( a x(t) ) x(t) / F(x_1, G(x_0 + t u(s)) )
    %           And so V = a D Exp( a x(t) ) x'(t)
    %           And so by Gauss Lemma, < T, V > = a < x(t), x'(t) > / F(x_1, G(x_0 + t u(s)))
    %                               V < T, V > =   [ < x'(t), x'(t) > + < x(t), x''(t) > ]   / F(x_1, G(x_0 + t u(s)))
    %                                         <= l^{-1} |x'(t)|^2 + |x''(t)|
    %           x''(t) = D^2G(x0 + t u(s)) { u(s), u(s) }
    %           x'(t) = DG(x0 + t u(s)) { u(s) }
    %           x(t) = G( x_0 + t u(s) )
    %
    %           We can obtain a bound |x''(t)| <= A |u(s)|^2 <= A s^2
    %           We can obtain a bound |x'(t)|^2 <= A |u(s)|^2 <= As^2
    %
    %
    %  Therefore |L(t) - t L'(0)| <= A l^2 s^2
    %  Thus L(t) <= t L'(0) + A l^2 t^2
    %             = t ( s^2 + O(s^3) ) + A l^2 t^2
    %           ts^2 + Al^2 t^2
    %
    %       Setting t = 
    %
    %  A quantity l should show up somewhere.

    Now $H(x_0, \xi(s)) = \{ s \}$

    Since $\mathcal{L}(x_0, v_0) = (x_0,\xi^-)$, if we define $n(s) = \mathcal{L}^{-1}(x_0,\xi(s))$ then
    %
    \[ \sum g_{x_0,v_1}(v_1, v'_0) = 0. \]


    By homogeneity, this is equal to
    %
    \[ \sum g^{jk}(x_0,\xi^-) \]

    % F_*( x_0, xi ) = 0
    % dF_*( x )



    Because $E$ is a diffeomorphism, the vector field $v(s) = E^{-1}(x_0 + sw)$ is a smooth function of $s$. To simplify notation, write $v_0$, $v_0'$, and $v_0''$ for $v(0)$, $v'(0)$, and $v''(0)$. We define a variation $a: (-\varepsilon,\varepsilon) \times [0,l] \to M$ by setting $\alpha(s,t) = (t / l) v(s)$, and then setting $a(s,t) = \text{Exp}( \alpha(s,t) )$. For each $s$, $t \mapsto a(s,t)$ is then a constant speed geodesic from $x_1$ to $x_0 + sw$.

    Unlike Riemannian manifolds, in general on Finsler manifolds the exponential map $\text{Exp}$ is only $C^1$ at the origin, and so it is not immediate that the variation $a$ is smooth when $t = 0$. But if we consider the flow $\varphi$ induced by the integral equations \eqref{FStarHamilton}, and we define the covector $\omega(s) = \mathcal{L}( x_1, v(s) )$, then $a(s,t)$ is equal to the projection of the flow $\varphi(t, \omega(s))$ onto $M$. Thus $a$ is smooth everywhere.

    % TODO TODO TODO TODO
    % We assume a bounded geometry condition,
    % so that the first and second derivatives of E^{-1}
    % are bounded uniformly.
    %
    % Thus v'(s) = Sum ( partial (E^-1) / partial x^j )( x_0 + sw ) w^j
    % And so v''_0 = Sum ( partial^2 (E^{-1}) / partial x^j partial x^k )(x_0 + 0w) w^j w^k
    % Thus <v',v'> = 1, and <v'', v> <= A
    %
    %
    %




    Let $L: (-\varepsilon,\varepsilon) \to (0,\infty)$ be the function that gives the arclength of the geodesic $t \mapsto a(s,t)$. Then $L(0) = l$, and since $t \mapsto a(s,0)$ is the shortest geodesic from $x_1$ to $H(x_0,\xi^-)$, we have $L'(0) = 0$. We apply the second variation formula to estimate $L''(0)$, which is equal to $V \{ g_{\bar{T}}( V, T ) \}$, evaluated at $s = 0$ and $t = l$.

    Note that $L(s) = F(x_1, v(s))$. Since $t \mapsto a(0,t)$ is the shortest geodesic from $x_1$ to the hyperplane $H(x_0,\xi_-)$, it must be true that $L'(0) = 0$. If we let $V(s,t) = \partial_s a(s,t)$, then the second variation formula implies $L''(0)$ is equal to $- g_T(V, \nabla_T V) / 2$ evaluated at $s = 0$ and $t = l$. We bound $g(V,\nabla_V T)$ by deriving some differential inequalities for quantities related to $g(V,\nabla_V T)$, inspired by calculations of \cite{Casado}.

    Let us start with a lower bound. Define
    %
    \begin{equation}
        a(t) = \frac{g_{\bar{T}}(V, \nabla_{\bar{T}} V)}{g_{\bar{T}}(V,V)}.
    \end{equation}
    %
    Almost metric compatibility and lack of torsion implies that
    %
    \begin{equation}
        a'(t) = \frac{g_{\bar{T}}(\nabla_{\bar{T}} V, \nabla_{\bar{T}} V) + g(V, \nabla_{\bar{T}} \nabla_V \bar{T})}{g_{\bar{T}}(V,V)} - 2 \frac{g_{\bar{T}}(\nabla_{\bar{T}} V, V)^2}{g_{\bar{T}}(V,V)^2}.
    \end{equation}
    %
    Cauchy-Schwarz implies that $g_{\bar{T}}(\nabla_{\bar{T}} V, \nabla_{\bar{T}} V) \geq g_{\bar{T}}( \nabla_{\bar{T}} V, V )^2 / g_{\bar{T}}(V,V)$, and so
    %
    \begin{equation}
        a'(t) \geq \frac{g(V, \nabla_{\bar{T}} \nabla_V \bar{T})}{g_{\bar{T}}(V,V)} - \frac{g_{\bar{T}}(\nabla_{\bar{T}} V, V)^2}{g_{\bar{T}}(V,V)^2} = \frac{g(V,\nabla_{\bar{T}} \nabla_V \bar{T})}{g_{\bar{T}}(V,V)} - a(t)^2.
    \end{equation}
    %
    Finally, we employ the bound $\nabla_{\bar{T}} \nabla_V \bar{T} = R({\bar{T}},V) {\bar{T}}$ and the definition of flag curvature to obtain the final bound $a'(t) + a(t)^2 \geq - K({\bar{T}},V) \geq - \Delta$. Using L'Hôpital's rule and the fact that $V = 0$ and $\nabla_{\bar{T}} V = v'_0 / l$ is nonvanishing when $t = 0$ and $s = 0$, we find that $a(t) \to \infty$ as $t \to 0$, which is only possible (by Proposition 3.10 of \cite{Casado}) if $a(t) \geq \cot_\Delta( t )$.

    On the other hand, consider a parallel vector field $E$ along the curve $t \mapsto a(s,t)$ with $E(l) = V(l)$. Thus $\nabla_{\bar{T}} E = 0$. When $t = l$ we have $g_{\bar{T}}( \nabla_{\bar{T}} V, V ) = g_{\bar{T}}( \nabla_V \bar{T}, V ) g_{\bar{T}}( \nabla_E \bar{T}, E )$. If we define 
    %
    \begin{equation}
        b(t) = \frac{g_{\bar{T}}( \nabla_E \bar{T}, E )}{g_{\bar{T}}(E,E)},
    \end{equation}
    %
    then
    %
    \begin{equation}
        b'(t) = \frac{g_{\bar{T}}(\nabla_{\bar{T}} \nabla_E \bar{T}, E)}{g_{\bar{T}}(E,E)}.
    \end{equation}
    %
    Permuting $\nabla_{\bar{T}}$ and $\nabla_E$ using the first Chern curvature tensor, we can write
    %
    \begin{equation}
        b'(t) = - K(\bar{T},E) - \frac{g_{\bar{T}}( \nabla_{\nabla_E \bar{T}} \bar{T}, E )}{g_{\bar{T}}(E,E)}.
    \end{equation}
    %
    Using the identity $g_{\bar{T}}( \nabla_A \bar{T}, B) = g_{\bar{T}}(A, \nabla_B \bar{T})$, a kind of 'mixed partials' identity, we conclude that $g_{\bar{T}}( \nabla_{\nabla_E \bar{T}} {\bar{T}}, E) = g_{\bar{T}}( \nabla_E \bar{T}, \nabla_E \bar{T} ) \geq g_{\bar{T}}( \nabla_E \bar{T}, E )^2 / g_{\bar{T}}( E, E )$, and thus that $b'(t) - b(t)^2 \leq - K(\bar{T},E) \leq - \delta$. But this is only possible by Proposition 3.10 of \cite{Casado} if $b(t) \leq \cot_\delta(t)$.

    Putting together the work of the two previous paragraphs, we have shown that
    %
    \begin{equation} \label{NablaVOverV}
        \cot_\Delta(t) \leq \frac{g_{\bar{T}}( \nabla_V \bar{T}, V )}{g_{\bar{T}}(V,V)} \leq \cot_\delta(t).
    \end{equation}
    %
    Since $g_{\bar{T}}(V,V) = 1$ when $t = l$ and $s = 0$, it follows that for these parameters
    %
    \[ \cot_\Delta(l) \leq g_{\bar{T}}(\nabla_V \bar{T}, V) \leq \cot_\delta(l). \]
    %
    % l^{-1} - Delta l / 3
    %
    To bound the order of growth of $g_{\bar{T}}(\nabla_V \bar{T}, V)$ and $g_{\bar{T}}(V,V)$, define $f(t) = g_{\bar{T}}(V,V) \csc_\delta^2(t)$. Then upper bounds on $g_{\bar{T}}(\nabla_{\bar{T}} V, V)$ imply that
    %
    \begin{equation}
        f'(t) = 2 g_{\bar{T}}(\nabla_{\bar{T}} V, V) \csc_\delta^2(t) - 2 g_{\bar{T}}(V,V) \csc_\delta^2(t) \cot_\delta(t) \leq 0.
    \end{equation}
    %
    Thus $f$ is non-increasing. It is simple to check from the fact that when $s = 0$ and $t = 0$, then $V = 0$ and $\nabla_{\bar{T}} V = v_0'$, so that $f(0) = l^{-2} g_{v_0}(v_0', v_0')$. Thus we conclude that
    %
    \begin{equation} \label{VUpper}
        g_{\bar{T}}(V,V) \leq l^{-2} \sin_\delta^2(t) g_{v_0}(v_0', v_0').
    \end{equation}
    %
    A similar method justifies the lower bound
    %
    \begin{equation} \label{VLower}
        g_{\bar{T}}(V,V) \geq l^{-2} \sin_\Delta^2(t) g_{v_0}(v_0', v_0').
    \end{equation}
    %
    Plugging in the bound $g_{\bar{T}}(V,V) = 1$ when $t = l$ and $s = 0$, we find that if $r = g_{v_0}(v_0',v_0')$, then $l^2 / \sin_\delta^2(l) \leq r \leq l^2 / \sin_\Delta^2(l)$. But this implies that
    %
    \[ \frac{\sin_\Delta^2(t)}{\sin_\delta^2(t)} \leq g_{\bar{T}}(V,V) \leq \frac{\sin_\delta^2(t)}{\sin_\Delta^2(t)}, \]
    %
    and thus that
    %
    \begin{equation} \label{boundsonNablaTV}
        \frac{\sin_\Delta(t) \cos_\Delta(t)}{\sin_\delta^2(t)} \leq g_{\bar{T}}(\nabla_V \bar{T}, V) \leq \frac{\sin_\delta(t) \cos_\delta(t)}{\sin_\Delta^2(t)}.
    \end{equation}
    %
    %
    % sin_Delta(l) = sin( sqrt(Delta) l ) / sqrt(Delta) = l - Delta l^3 / 6 + O(l^5)
    % sin_Delta'(l) = cos( sqrt(Delta) l ) = 1 - Delta l^2 / 2 + O(l^4)
    % sin_delta^2(l) = ( l - delta l^3 / 6 )^2 = l^2 - delta l^4 / 6 + O( l^6 )
    %
    % So the LHS is ( l - Delta l^3 / 6 + O(l^5) ) ( 1 - Delta l^2 / 2 + O(l^4) ) / ( l^2 - delta l^4 / 6 + O(l^6) )
    %               1/l - Delta / 2 - Delta l / 6 + O(l^3)   /  ( 1 - delta l^2 / 6 + O(l^6) )
    %               1/l - Delta / 2  - (Delta - delta) l / 6 + O(l^3)
    %
    % And the RHS is thus 1/l - delta / 2 + (Delta - delta) l / 6 + O(l^3)
    %
    Thus $g_{\bar{T}}(V,V) = 1 + O(l^2)$ as $l \to 0$, and $g_{\bar{T}}(\nabla_V \bar{T}, V) \sim l^{-1}$.
    % [ l^2 - delta l^4 / 6 + O(l^6) ] / [ l^2 - Delta l^4 / 6 + O(l^6) ]
    %   = ( 1 - delta l^2 / 6 + O(l^4) ) / ( 1 - Delta l^2 / 6 + O(l^4) )
    %   = [1 - delta l^2 / 6] (1 + Delta l^2 / 6) + O(l^4)
    %   = 1 + (Delta - delta) l^2 / 6 + O(l^4)

    % L''(0) = 2 < V, Nabla_V T > - { V < V, T > }
    %
    % g_T( Mv(s), Mv(s) )
    % = g_{v(s)}( v(s), v(s) ) = L(s)^2
    %
    %
    % < V, Nabla_V T > - < Nabla_V V, T>
    % 2 < V, Nabla_V T > - V { < V, T > }

    Now let's control $g_{\bar{T}}(\nabla_V V, \bar{T})$. If we let $A = \nabla_V V$, then $A$ satisfsies the differential equation
    %
    \[ \nabla_{\bar{T}} \nabla_{\bar{T}} A + R(A,\bar{T}) \bar{T} + Y = 0, \]
    %
    where $Y = (\nabla_{\bar{T}} R)(V,\bar{T}) V + (\nabla_V R)(V,\bar{T}) \bar{T} + 4 R(V,\bar{T}) (\nabla_V \bar{T})$. Then
    %
    \[ g_{\bar{T}}(Y,Y)^{1/2} \leq l^{-1} \sin_\delta(t) \]

    \[ \| Y \| \leq C \| V \|^2 \leq C_1 ( 1 + (\Delta - \delta) t^2 / 3 + C_2 t^4 ) \]
    %
    %
    % y'' + a y = b + c t^2 + d t^4
    %   Solution with y = y' = 0 is
    %       y = -2c/a^2 + b/a + cx^2/a + (2c/a^2 - b/a) cos( sqrt(a) x )
    %
    % Thus | Nabla_V V | <= l^{-1} [ C1 + C2 t^2 + C3 cos( sqrt(delta) x ) ]
    %  

    Now let's control $g_{\bar{T}}(\nabla_V V, \bar{T})$. For each $s$ and $t$, let $M: \RR^d \to \RR^d$ be the linear map given by $E_*( \alpha(s,t) )$, i.e. having coefficients $(\partial E^i / \partial x^j) ( \alpha(s,t) )$. Gauss' Lemma for Finsler manifolds (see Lemma 6.1.1 of \cite{BaoChern}), and that $\bar{T} = (l / L(s)) (\partial_t a)$, we find that for any $v \in \RR^d$,
    %
    \[ g_{\bar{T}}( M v, {\bar{T}} ) = [L(s) / l] g_{v(s)}( v, v(s) ). \]
    %
    To control $V \{ g_{\bar{T}}(V,\bar{T}) \}$, note $V = (t/l) M v'(s)$. Thus
    %
    \[ g_{\bar{T}}( V, {\bar{T}} ) = (L(s) / l) (t / l) g_{v(s)}( v'(s), v(s) ). \]
    %
    Note that $V \{ g_T(V,T) \}$ is the partial derivative of $g_{\bar{T}}(V,{\bar{T}})$ in the $s$ variable. Since $L'(0) = 0$, and $(\partial_s g_{v(s)}) (v'(s), v(s))) = 0$ by homogeneity, when $s = 0$ we have
    %
    \[ V \{ g_T(V,T) \} = t l^{-1} [g_{v_0}( v''_0, v_0 ) + g_{v_0}( v'_0, v'_0 )]. \]
    %
    Now $g_{v_0}(v_0', v_0') = 1$.

    %
    \[ w = (\partial_s a)(s,l) = M(s,l) v'(s) \]
    %
    and if we define $A_{s,t}: \RR^d \times \RR^d \to \RR^d$ to be the bilinear map given by the coefficients $(\partial^2 E^i / \partial x^j \partial x^k)(a(s,t))$, then
    %
    \[ 0 = (\partial_s^2 a)(s,l) = M(s,l) v''(s) + A_{s,l}( v'(s), v'(s) ). \]
    %
    Using Gauss' Lemma in reverse, we thus have
    %
    \[ g_{v_0}(v_0'', v_0) = g_T( M(0,l) v_0'', M(0,l) v_0 ) = - g_T( A_{0,l}(v_0', v_0'), T ). \]

    Now consider any curve $\xi(u)$ in $S_{x_0}^*$ with $\xi(0) = \xi^-$, and let $\omega = \xi'(0)$. Pick a unit vector $w$ as above at $x_0$ pointing in the same direction as $\omega$. For each $s$, the Euclidean distance from $x_0 + sw$ to the hyperplane $H(x_0, \xi(u))$ is $|s| |\xi(u) \cdot w| / |\xi(u)|$. Since the Euclidean metric is comparable with the Finsler metric, taking Taylor series in $u$ leads us to conclude that the length of the geodesic from $x_0 + sw$ to $H(x_0, \xi(u))$ is at most $C_0 |\omega| su$ for some universal constant $C_0$ as $s,u \to 0$. But the length of the geodesic from $x_1$ to $x_0 + sw$ is equal to $L(s)$, which is at least $l + C_1 s^2 / l$ for some universal constant $C_1$ as $s \to 0$. By the triangle inequality it follows that the length of the geodesic from $x_1$ to $H(x_0,\xi(u))$, which is equal to $\Psi(\xi(u))$, is at least $l + C_0 |\Omega| su - C_1 s^2$. If we optimize by choosing $s = 2 (C_0 / C_1) |\omega|^{1/2} l u$ then we find that
    %
    \begin{equation}
        l \geq - \Psi(\xi(u)) \geq l + |\omega| l [ 2 C_0^2 / C_1 - C_0^2 / C_1 ] u^2 = l + (C_0^2 / C_1) |\omega| l u^2.
    \end{equation}
    %
    Taking $u \to 0$ and noting $\Psi(\xi(0)) = l$, we conclude that $|H_-(\xi'(0))| \geq (C_0^2 / C_1) l |\omega| \gtrsim l |\omega|$. Since $l = d^-(x_0,x_1)$, we have proven what was required to be shown.

    % The plane is described by xi(u) * x = 0
    % The vector sw
    %
    % Translate : What is the distance from the origin to the plane xi(u) * x = - s ( xi(u) * w )
    %   It's |s| |xi(u) * w| / |xi(u)|
    %
    % Thus the distance from x_0 + sw to H(x_0, xi(u)) is |s| |xi(u) * w| / |xi(u)|
    %
    % But the distance from x_1 to x_0 + sw is L(s)
    %
    % and thus the distance from x_1 to H(x_0, xi(u)) is
    %   <= L(s) + |s| |xi(u) * w| / |xi(u)|
    %   <= l - C s^2 / l + s [ u |xi'(0)| |w| + O(u^2)] / (|xi(0)| + O(u))
    %   <= 1 - C s^2 / l + A su + O(su^2)
    %
    % We want to *minimize* this quantity by picking s appropriately
    % Picking s = e l u, we get
    %
    % <= l - (C e^2 - A e ) l u^2
    % <= l - e ( Ce - A ) l u^2
    %
    % Picking e >= A/C gives a bound of the form >= l - C l u^2
    %
    % And thus that H(\xi'(0)) >> l.
    %
    % But this proves the required nondegeneracy



% To get the lower bound, we exploit the fact that 
%
% < Nabla_V T, V >(b) = < Nabla_E T, E >(b)
%
% for the parallel vector field E with E(b) = V(b). If we define a(t) = < Nabla_E T, E > / |E|^2, then
%
%  a'(t) = T { (Hess r)(E,E) } / |E|^2
%        = Nabla_T {Hess r} (E,E) / |E|^2
%        = - < R(E,T)T, E > / |E|^2 - (Hess^2 r)(E,E) / |E|^2
%       <= - K(T,E) - a(t)^2
%
% So a'(t) + a(t)^2 <= - K(T,E) <= - delta





%
% On the other hand, to calculate g(V, Nabla_V T), write this quantity as f(t). We note that since V(0) = 0, f(0) = 0.
% Moreover by almost metric compatibility and being torsion free,
%    f' = T { g(V, Nabla_V T) } = g( Nabla_T V, Nabla_V T ) + g(V, Nabla_T Nabla_V T )
%                               = | Nabla_T V |^2 - g( R(V,T)T, V )
%                               = | Nabla_T V |^2 - K(T,V)                 [ Here we used the fact that V is verywhere orthogonal to T ]
%
%
%
% Define a(t) = f(t) / |V|^2 = g(V, Nabla_V T) / |V|^2
%
% a'(t) = f'(t) / |V|^2 - 2 f(t)^2 / |V|^4 
%       = f'(t) / |V|^2 - 2 a(t)^2
% a'(t) + 2 a(t)^2 = f'(t) / |V|^2 = | Nabla_T V|^2 / |V|^2 - K(T,V)
%
% < Nabla_T V, Nabla_T V > >= < Nabla_T V, V/|V| >^2 = < Nabla_T V, V>^2 / |V|^2 = |V|^2 a(t)^2
%
% so
%
% a'(t) + a(t)^2 >= - K(T,V) >= -Delta
%
% We can use this to establish an upper bound on a(t), and thus on f(t), provided
% we have good bounds on |V|. But we do have such bounds, which completes the proof.
%
% To get the lower bound, we exploit the fact that 
%
% < Nabla_V T, V >(b) = < Nabla_E T, E >(b)
%
% for the parallel vector field E with E(b) = V(b). If we define a(t) = < Nabla_E T, E > / |E|^2, then
%
%  a'(t) = T { (Hess r)(E,E) } / |E|^2
%        = Nabla_T {Hess r} (E,E) / |E|^2
%        = - < R(E,T)T, E > / |E|^2 - (Hess^2 r)(E,E) / |E|^2
%       <= - K(T,E) - a(t)^2
%
% So a'(t) + a(t)^2 <= - K(T,E) <= - delta



    % By almost metric compatibility, we can write
% g( Nabla_V V, T ) = V { g(V,T) } - g( V, Nabla_V T ).
%
% Let us start by bounding V { g(V,T) }.
%
% If E = exp_x, then because a(s,t) = exp_x( t( cos(s) v + sin(s) w ) )
%
% V = partial_s a = E( t( cos(s) v + sin(s) w ) ) { t ( cos(s) w - sin(s) v ) }
%
% By the Gauss Lemma for Finsler manifolds,
%       g(V,T) = g( t ( cos(s) w - sin(s) v ), v )
%              = - t sin(s)
% Thus V { g(V,T) } = - t cos(s)

\end{comment}

\begin{comment}

    Now let's control $V \{ g_T(V,T) \}$. For each $s$ and $t$, let $M: \RR^d \to \RR^d$ be the linear map given by $E_*( \alpha(s,t) )$, i.e. having coefficients $(\partial E^i / \partial x^j) ( \alpha(s,t) )$. Gauss' Lemma for Finsler manifolds (see Lemma 6.1.1 of \cite{BaoChern}) implies that for any $v \in \RR^d$,
    %
    \[ g_T( M v, T ) = g_{v(s)}( v, v(s) ). \]
    %
    To control $V \{ g_T(V,T) \}$, note $V = (t/l) M v'(s)$. Since $T = M v(s) / L(s)$, it follows that
    %
    \[ g_T( V,T ) = (t/l) \frac{g_{v(s)}( v'(s), v(s) )}{L(s)}. \]
    %
    Note that $V \{ g_T(V,T) \}$ is the partial derivative of $g_T(V,T)$ in the $s$ variable. Since $L'(0) = 0$, and $(\partial_s g_{v(s)}(v'(s), v(s))) = 0$ by homogeneity, when $s = 0$ and $t = l$,
    %
    \[ V \{ g_T(V,T) \} = \frac{g_{v_0}(v_0'', v_0) + g_{v_0}(v_0', v_0')}{l}. \]
    %
    We note that since $a(s,l) = x_0 + s w = E( v(s) )$, it follows that
    %
    \[ w = (\partial_s a)(s,l) = M(s,l) v'(s) \]
    %
    and if we define $A_{s,t}: \RR^d \times \RR^d \to \RR^d$ to be the bilinear map given by the coefficients $(\partial^2 E^i / \partial x^j \partial x^k)(a(s,t))$, then
    %
    \[ 0 = (\partial_s^2 a)(s,l) = M(s,l) v''(s) + A_{s,l}( v'(s), v'(s) ). \]
    %
    Using Gauss' Lemma in reverse, we thus have
    %
    \[ g_{v_0}(v_0'', v_0) = g_T( M(0,l) v_0'', M(0,l) v_0 ) = - g_T( A_{0,l}(v_0', v_0'), T ). \]
    %
    %
    % We want to know partial^2 E^i / partial x^j partial x^k
    %
    % THE PAPER gives bounds on
    %           Nabla d exp = Nabla { partial E^i / partial x^j (dx^j o e_i) }
    %                       = (partial^2_{jk} E^i) ( dx^k o dx^j o e_i )
    %                           + (partial_j E^i) Gamma_{ki}^a (dx^k o dx^j o e_a)
    %
    %           So we have to bound (partial_j E^i) Gamma_{ki}^a


\end{comment}

    %
    % Since a(s,l) = x_0 + sw
    %   and a(s,t) = E( alpha(s,t) )
    %   w = (partial_s a)(s,t_-) = DE^i( alpha(s,t_-) ) { v'(s) }
    %   0 = (partial_s^2 a)(s,t_-) = DE^i( alpha(s,t_-) ) { v''(s) } + D^2E^i( alpha(s,t_-) ) v'(s) v'(s)
    %   So
    %      g_{v(0)}(v'',v) = g_T( DE^i( alpha(s,t_-) ) { v''(s) } e_i , T )
    %                      = g_T( - D^2 E^i( alpha(s,t_-) ) v'(s) v'(s) e_i, T )
    %
    %
    %   We should be able to control v_0', since it is equal to (Nabla_T V)(0,0)
    %
    %   But how do we control D^2 E^i?
    %
    % 

    %
    %
    % We have V(0) = 0 and V'(0) = X'(0)

    % the theory of Jacobi fields should allow us to bound g_{X(0)}( X'(0), X'(0) )

    %
    % Now T(0) = X(0) / t_-
    % So derivatives of


    % Using Jacobi fields, we should be able to determine X'(0) since this is V'(0)
    
    % (1/2) partial_i F^2(x_1, X(s)) X'(s)_i
    % F(x_1,X(s)) F_i(x_1, X(s)) X'(s)_i

\begin{comment}

    But then when $s = 0$ we have
    %
    \[ V \{ g(V,T) \} = (t/t_-) \frac{g(v_0'', v_0) + g(v_0', v_0')}{t_-} = (t/t_-) \frac{g(v_0'', v_0) + t_-^2}{t_-}. \]
    %
    But now the Gauss Lemma in reverse tells us that
    %
    \[ g_T(v_0'', v_0) = BLAH \]

    But since $L(s)^2 = g(v(s), v(s))$, we conclude that $t_- L''(0) = g(v',v') + g(v,v'')$. Thus, evaluted at $t = t_-$ and $s = 0$, we have
    %
    \[ V \{ g(V,T) \} = t_- + \frac{g(v_0'', v_0)}{t_-} = L''(0) + t_- - \frac{g(v_0',v_0')}{t_-} \]

    viewed as a function of $t$ and $s$, we have
    % exp( X(s) t )
    % exp_*( X'(s) t )
    \[ g(V,T)(s,t) = g \big( X'(s) (t / t_-) , v(s) / |v(s)| \big) = (t/t_-) \frac{g( v'(s), v(s) )}{|v(s)|} \]


    Let $T : U_0 - \{ x_1 \} \to \RR^d$ be the smooth vector field given for each $t > 0$ and $v \in S_{x_1}$ by $T_{E(tv)} = E_*(v)$. Then $F(x, T_x) = 1$ for all $x \in U_0$.

\end{comment}




\begin{comment}

    Let $y: U \to T_{x_0} U$ be the inverse of $G_{x_0,\xi^+}$. Let $v_+$ be the inverse Legendre transform of $\xi^+$, and let $v: U \to \RR^d$ be a vector field on $U$ given by the expression $\sum v_+^j \partial / \partial y^j$, i.e. setting
    %
    \[ v(x) = Dy(x)^{-1} v_+. \]
    %
    Using $v$, define a Riemannian metric $h$ on $U$ with coefficients $h_{jk}(x) = g_{jk}(x, v(x))$. Since $h$ is then a smooth function of $x_0$ and $x_1$, we can pick a constant $\kappa > 0$, upper bounded locally uniformly in $x_0$ and $x_1$, such that all sectional curvatures of $h$ are bounded from below by $- \kappa^2$.

    %Consider the vector field $v: U \to \RR^d$ given by the inverse Legendre transform, i.e.
    %
    %\[ v^i(x) = \sum g^{ij}(x, \omega(x)) \omega_j(x). \]

    %Note that $\omega(x_0) = \xi^+$, and that if $\gamma: [0,t_+] \to U$ is the unit speed geodesic from $x_0$ to $x_1$, then $\gamma'(t)$ is given by the expression $\sum v_+^i \partial / \partial y^i$, i.e.
    %
    %\[ \gamma'(t) = Dy(\gamma(t))^{-1} v_+. \]
    %
    %This follows because $\gamma(t) = \exp( x_0, t v_+, \xi^+ )$, where $v_+^i = \sum h^{ij}(x_0) \xi^+_j$. Also pick $\kappa > 0$ so that all sectional curvatures of $h$ are bounded from below by $- \kappa^2$. By continuity, one can choose $\kappa$ to be bounded locally uniformly in $x_0$ and $x_1$.

    For each vector $\zeta \in T_{\xi^+} S_{x_0}^*$ with $\sum h^{jk}(x_0) \zeta_j \zeta_k = 1$, choose a curve $\xi: [0,1] \to S_{x_0}^*$ with $\xi(0) = \xi^+$ and $\xi'(0) = \zeta$. Then since $\Psi(\xi(0)) = \Psi(\xi^+) = t_+$, we have
    %
    \[ \Psi(\xi(s)) = t_+ + (H_+ \zeta) s^2 + O(s^3), \]
    %
    Our goal is thus to understand the second order behavior of the quantity $\Psi(\xi(s))$. If $\alpha \in T_{x_0}^* U$ is equal to $\xi''(0)$, then differentiating the relation $F_*^2(x_0,\xi(s)) = 1$ gives
    %
    \[ \sum h^{jk}(x_0) \zeta_j \xi^+_k = 0 \quad\text{and}\quad \sum h^{jk}(x_0) (\zeta_j \zeta_k + \alpha_j \xi^+_k) = 0. \]
    %
    Using these relations, we calculate that if
    %
    \[ a(s) = \sum h^{jk}(x_0) \xi_j(s) \xi_k(s) \quad\text{and}\quad b(s) = \sum h^{jk}(x_0) \xi_j(s) \xi^+_k, \]
    %
    then $a(0) = 1$, $a'(0) = 0$, $a''(0) = 0$, $b(0) = 0$, $b'(0) = 0$, and $b''(0) = -1$.
    %
%    \[ a(0) = \sum h^{jk}(x_0) \xi^+_j \xi^+_k = 1 \quad a'(0) = 2 \sum h^{jk}(x_0) \zeta_j \xi^+_k = 0 \]
    %
%    \[ a''(0) = 2 \sum h^{jk}(x_0) ( \zeta_j \zeta_k + \alpha_j \xi^+_k ) = 0. \]
    %
%    \[ b(0) = \sum h^{jk}(x_0) \xi^+_j \xi^+_k = 1 \quad b'(0) = \sum h^{jk}(x_0) \zeta_j \xi_k^+ = 0 \]
    %
%    \[ b''(0) = \sum h^{jk}(x_0) \alpha_j \xi_k^+ = - \sum h^{jk}(x_0) \zeta_j \zeta_k = -1. \]
    %
    Thus if $\theta(s)$ denotes the angle between $\xi(s)$ and $\xi^+_k$ with respect to the metric $h$, then $\cos \theta(s) = b(s) a(s)^{-1/2}$, and so we conclude that $\theta(0) = 0$ and $\theta'(0) = 1$.

    We will use some hyperbolic trigonometry, combined with Rauch's comparison theorem, to bound lengths with respect to the metric $h$. Consider a geodesic triangle in the hyperbolic plane $\mathbb{H}^2_\kappa$ of constant curvature $- \kappa^2$, with sidelengths $A$, $B$, $C$, and opposing angles $a$, $b$, and $c$, such that $A = t_+$, that $a = \pi/2$, and that $b = \pi/2 - \theta(s)$. This is enough information to completely determine all other characteristics of the triangle. Since $a = \pi/2$, Pythagoras' theorem for hyperbolic triangles says that
    %
    \[ \cosh(A / \kappa) = \cosh(B / \kappa) \cosh(C / \kappa). \]
    %
    The sine law for hyperbolic geometry then says that
    %
    \[ \frac{\sinh(A/\kappa)}{\sin(a)} = \frac{\sinh(B / \kappa)}{\sin(b)} = \frac{\sinh(C / \kappa)}{\sin(c)}. \]
    %
    If we consider $B = B(s)$ and $C = C(s)$ as functions of $s$, then these formulas imply that $B(0) = t_+$, $B'(0) = 0$, $B''(0) = - \kappa \tanh( t_+ / \kappa )$, $C(0) = 0$, and $C'(0) = \kappa \tanh(t_+/\kappa)$.

    We relate the study of triangles in $\mathbb{H}^2_\kappa$ by considering a diffeomorphism $J: \mathbb{H}^2_\kappa \to \Pi(s)$ which is a `normal coordinate system' at some point $P \in \mathbb{H}^2_\kappa$, in the sense that $J(P) = 0$, that the linear map $M : T_P \mathbb{H}^2_\kappa \to \Pi(s)$ given by the derivative of $J$ at $P$ is an isometry, and that for any geodesic $c: [0,1] \to \mathbb{H}^2_\kappa$ with $c(0) = P$, $J(c(t)) = t M[c'(0)]$, so that $J \circ c$ is a straight line through the origin.

    %
    %Similarily, if we consider $c(s)$, then $c(0) = 0$ and $c'(0) = \text{sech}(t_+/\kappa)$.
    %We now apply this calculation, with the help of the Rauch comparison theorem of Riemannian geometry, to upper bound the lengths of curves with respect to the metric $h$.
    %
    %
    % sinh(C/kappa) = sin(c) sinh(X)
    % C'(0) / kappa = c'(0) sinh(X)
    %
    % sin(b) = sinh(B/kappa) / sinh(X)
    %   sin(b) = sinh( X - s^2 [ 1/2 tanh(t_+ / kappa) ] ) / sinh(X)
    %          = sinh( X ) - s^2/2
    %  sin(b + s b') = sin(b) + s cos(b) b'
    %
    Define a vector $w(s) \in \RR^d$ by setting $w(s)^i = \sum h^{ij}(x_0) \xi_j(s)$. Then the $h(x_0)$ angle between $v_+$ and $w(s)$ is equal to $\theta(s)$.
    %
%    \[ \sum h_{ij}(x_0) v_+^i w^j(s) = \sum h^{ij}(x_0) \xi^+_i \xi_j(s) \]
    %
%    and
    %
%    \[ \sum h_{ij}(x_0) w(s)^j w(s)^k = \sum h^{ij}(x_0) \xi(s)_j \xi(s)_k, \]
    %
    Take the unique right angled geodesic triangle in $\mathbb{H}^2_\kappa$ with vertices $P$, $Q$, and $R$, with the property that $J(P) = 0$, that $J(Q) = t_+ v_+$, that $J(R)$ lies on the line $l(s)$ through the origin in $\Pi(s)$ perpendicular to $\xi(s)$, and that the triangle forms a right angle at the vertex $R$. If we let $A$, $B$, and $C$ be the lengths of the geodesics from $P$ to $Q$, from $Q$ to $R$, and from $R$ to $P$ respectively, and $a$, $b$, and $c$ be the opposing angles of these geodesics, then this triangle is precisely of the form considered in the previous paragraph.
    %
    % cosh(X) / cosh(B/kappa) = cosh(C/kappa)
    %
    % cosh(B/kappa) = cosh( X - s^2 tanh(X) / 2 )
    %               = cosh(X) - s^2 (1/2) sinh(X) tanh(X)
    %
    % cosh(C/kappa) = 1 + s^2 [(1/2) C'(0)^2 / kappa^2]
    %
    % cosh(X) / cosh(B/kappa) = 1 / [ 1 - s^2 (1 / 2) tanh(X)^2 ]
    %                         = 1 + s^2 (1 / 2) tanh(X)^2
    %
    % 
    % So C'(0)^2 / kappa^2 = tanh(X)^2
    %  C'(0) = tanh(X) kappa

    Let $\rho: [0,1] \to \mathbb{H}^2_\kappa$ parameterize the side of this triangle from $R$ to $Q$. Then the length of this geodesic is equal to $B$. If $\exp_h: T_{x_0} U \to U$ is the exponential map at $x_0$ for the metric $h$, consider the curve $r: [0,1] \to U$ given by $\exp_h \circ j \circ \rho$. Because the sectional curvatures of $h$ are bounded from below by $-\kappa^2$, Corollary 1.35 of \cite{Cheeger}, which follows immediately from the Rauch comparison theorem, implies that the length of $r$ with respect to $h$ is smaller than the length of $\rho$ with respect to the metric on $\mathbb{H}^2_\kappa$, i.e.
    %
    \[ \int_0^1 \sqrt{ \sum h_{jk}(r(t)) r'(t)^j r'(t)^k }\; dt \leq B. \]
    %
    Because the hyperbolic plane is negatively curved, if two sides of a triangle are straightened in normal coordinates, the third side curves inward, and so the angle between the tangent vector of the third side and the tangent to the side it approaches monotonically decreases. Thus the $h(x_0)$ angle between $(j \circ \rho)'(t)$ and $v_+$ is bounded for all $t \in [0,1]$ by the $h(x_0)$ angle between $(j \circ \rho)'(0)$ and $v_+$, which is $\theta(s) = s + O(s^2)$. If we let $z: U \to T_{x_0} U$ be the inverse of $\exp_h$, then by the Gauss lemma, for each $u \in [0,t_+]$, if $x = \exp_h(u v_+)$, then the $h(x)$ angle between $Dz(x)^{-1} v_+$ and $Dz(x)^{-1} (j \circ \rho)'(t)$ is also bounded by $\theta(s)$ for each $t$. Since the maps $\exp_h$ and $G_{x_0,\xi^+}$ agree on the line $u \mapsto u v_+$, it follows that $Dz(x)^{-1} v_+ = v(x)$. By smoothness, since the curve $r(t)$ lies at a distance at most $C = O(s)$ from the line $u \mapsto \exp_h(u v_+)$ at all times, it follows that the $h(r(t))$ angle between $r'(t)$ and $v(r(t))$ is $O(s)$. Since the norm corresponding to the metric $h(x)$ approximates the Finsler metric $F(x,v)$ up to second order for $v$ in a conic neighborhood of $v(x)$, this means that
    %
    \[ F(r(t), r'(t) ) \leq \sqrt{ \sum h_{jk}(r(t)) r'(t)^j r'(t)^k } + O \big(s^3 |r'(t)| \big). \]
    %
    Thus it follows that
    %
    \[ \int_0^1 F(r(t),r'(t)) \leq B + O(s^3). \]
    %
    We claim that the point $r(0)$ lies a distance $O(s^3)$ from the surface $\Sigma(x_0,\xi)$. This follows because $r(0)$ lies a distance $O(s)$ from the point $x_0$ on the surface $\exp_h(\Pi(s))$, and 


     from which it follows that the distance from $\Sigma(x_0,\xi)$ to $x_1$ is bounded by $B + O(s^3)$. Thus
    %
    \[ \Psi(\xi) \leq B + O(s^3) = t_+ - \kappa \tanh(t_+ / \kappa) s^2 + O(t_+ s^3). \]
    %
    This means that $H_+ \zeta \leq - \kappa \tanh(t_+ / \kappa)$, and so $|H_+ \zeta| \gtrsim t_+$, completing the proof. \qedhere

    \end{comment}

    \begin{comment}
    Now because the curve $\rho$ curves inward, by the Gauss lemma the angle between the vector $c'(t)$ and $v^+$ at the point $c(t)$ with respect to the metric $h$ is bounded by $\theta(s)$ for all $t \in [0,a]$. Using the fact that
    %
    \[ Dg_{x_0,\xi}(x) = I - \sum\nolimits_k \Gamma^i_{jk}(x_0, v_+) x_k + O(x^2), \]
    %
    % If we let c_0 be the unit speed geodesic from x_0 to x_1
    % omega(c_0(t)) is the Legendre transform of c_0'(t).
    %       Since c_0 is a geodesic,
    %    

    Since the curve $\gamma: [0,t_+] \to U$ given by $\gamma(t) = G_{x_0,\xi^+}( t v^+ )$ is the unit speed forward geodesic from $x_0$ to $x$, 


     We will prove $\Psi(\xi(s)) \leq t_+ ( 1 - c_0 s^2)$
    %
    \[ \Psi(\xi(s)) \leq t_+ ( 1 - c_0 s^2 ) \]
    %
    for some $c_0 > 0$. Since $\Psi(\xi(0)) = t_+$, this implies that
    %
    \[ \Psi(\xi(s)) - \Psi(\xi(0)) \leq - c_0 t_+ s^2. \]
    %
    Taking $t \to 0^+$ then proves that $H_+(\zeta) \leq - c_0 t_+$. By homogeneity, we find that for all $\zeta \in T_{\xi^+} S_{x_0}^*$, $|H_+(\zeta)| \gtrsim t_+ |\zeta|$, thus completing the proof of Proposition \ref{triangleLemma}.






    We have thus given $\Pi$ two different metrics, and we denote the length of curves with respect to these two metrics by using $L_M$ and $L_{\mathbb{H}^2_\kappa}$. There is a unique point $y(s)$ on the line through the origin in $\Pi$ perpendicular to $\xi(s)$, and curve $\gamma$ in $\Pi$ from $y(s)$ to $x_1$, which is a geodesic in the $\mathbb{H}^2_\kappa$ metric perpendicular to $\xi(s)$ at $y(s)$. Let $L(s) = L_{\mathbb{H}^2_\kappa}(\gamma)$. The geodesic $\alpha$ from $x(s)$ to $x_1$ is the minimal length geodesic starting on $\Sigma(x_0,\xi(s))$ and ending at $x_1$. Since $\Pi$ is contained in $\Sigma(x_0,\xi(s))$, we conclude that
    %
    \[ L_M(\alpha) \leq L_M(\gamma). \]
    %
    If $p$ is a Riemannian manifold, then Corollary 1.35 of \cite{Cheeger} (which follows from the Rauch comparison theorem) implies that
    %
    \[ L_M(\gamma) \leq L_{\mathbb{H}^2_\kappa}(\gamma). \]
    %
    For more general Finsler metrics, this remains true but requires a slightly adjusted version of Corollary 1.35 of \cite{Cheeger}, and we prove this as Lemma BLAH in the appendix. Since $\Psi(\xi(s)) = L_M(\alpha)$ and $L(s) = L_{\mathbb{H}^2_\kappa}(\gamma)$, we conclude that $\Psi(\xi(s)) \leq L(s)$. In the next paragraph we use some hyperbolic trigonometry to obtain a bound
    %
    \[ L(s) \leq t_0 (1 - s^2/4), \]
    %
    which will complete the proof of the bound for $\Psi(\xi(s))$.

    The curve $\gamma$ forms one side of a right-angled geodesic triangle, with the other sides being given in normal coordinates with the line from $0$ to $y(s)$ and from $0$ to $t_0 e_1$. If we dilate the triangle by a factor $1/ \kappa$, we obtain a geodesic triangle in the standard hyperbolic plane of constant curvature $-1$, with the same angles, but with a dilated set of side-lengths, indicated in the following diagram:

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.3\textwidth]{triangle.eps}
    \end{figure}

    The hyperbolic sine law (See Section 1.6 of \cite{Katok}) says that for a geodesic triangle in $\mathbb{H}^2$ with side-lengths $A$, $B$, and $C$, and opposing angles $a$, $b$, and $c$,
    %
    \begin{equation}
        \frac{\sinh(A)}{\sin(a)} = \frac{\sinh(B)}{\sin(b)} = \frac{\sinh(C)}{\sin(c)}.
    \end{equation}
    %
    Using the diagram above, we thus conclude that
    %
    \begin{equation}
        \frac{\sinh(L(s) / \kappa)}{\sin(\pi/2 - s)} = \frac{\sinh(t_0 / \kappa)}{\sin(\pi/2)} = \sinh(t_0 / \kappa).
    \end{equation}
    %
    Since $L(0) = t_0$, taking Taylor series in the equation above in the $s$ variable about $s = 0$ gives that $L'(0) = 0$, and $L''(0) = - \kappa \tanh(t_0 / \kappa)$. We have
    %
    \begin{equation}
        \kappa \tanh(t_0 / \kappa) \geq 2 t_0 / 3 \quad\text{for}\quad t_0 \leq \kappa / 10,
    \end{equation}
    %
    and so for suitably small $s$, we have
    %
    \begin{equation}
        L(s) \leq t_0 ( 1 - s^2 / 4 ).
    \end{equation}
    %
    Thus $\Psi(\xi(s)) \leq L(s) \leq t_0 (1 - s^2 / 4)$, as was required to be shown.




    %
    \[ \overline{\partial_j} F_*^2(x_0,\xi(t)) \xi'_j(t) \]

    %
%    \[ \frac{\psi(\xi(s)) - \psi(\xi(0))}{s^2} \leq - c t_+. \]
    %
%    Taking $s \to 0$ gives that $\partial_s^2 \{ \psi \circ \xi \}(0) \leq -c t_+ / 2$. But this means that the bilinear form on $T_{\xi^+} S_{x_0}^*$ given by the Hessian $H$ of $\psi$ at $\xi^+$ is negative definite, i.e.
    %
%    \[ H(\zeta,\zeta) \leq - c t_+ / 2, \]
    %
%    which proves the required non-degeneracy condition.

    The Riemannian metric $h(x_0,\xi, \cdot)$ is a smooth function of $x_0$ and $\xi$. It follows by continuity and compactness that we can choose $\kappa > 0$ such that all sectional curvatures of $h$ are bounded from below by $- \kappa^2$ for all $x_0 \in V$ and $\xi \in S_{x_0}^*$. Let $\exp(x_0,\xi,\cdot): B \to V$ be the exponential map around $x_0$ for the metric $h$.

    %To aid with our computations, we choose to work in the coordinates centered at $x_0$ given by the inverse of the map $u \mapsto \exp(x_0, u, \xi^+)$. The advantage of working in this coordinate system is that then $x_0 = 0$, the forward geodesic from $x_0$ to $x_1$ is just a straight line of length $t_+$, and $\Sigma(x_0,\xi^+)$ is the hyperplane conormal to $\xi^+$. Rotating our coordinate system, we may assuming that this $x_1 = t_+ e_n$.

    Now consider the Riemannian metric $h = g(\cdot,e_n)$ in this coordinate system induced from the Finsler metric. Now  By compactness, we may choose $\kappa$ so it is uniform across all $x_0$ and $x_1$ in our coordinate system. Consider an orthonormal basis


     Using exponential coordinates at $x_0$, we may give this coordinate system another metric which makes the exponential coordinate map an isometry with the hyperbolic plane

     Take the exponential map $\exp_h$ at $x_0$ with respect to this coordinate system, 


    Use this Riemannian metric to construct a normal coordinate system around the origin.
    % g^{ij} (xi_+)_j = delta^i_n


    Rotating the coordinate system, we may assume that $x_1 = t_+ e_n$. Now for each $w \in T_{\xi^+} S_{x_0}^*$, we fix a curve $\xi(s)$ with $\xi(0) = \xi^+$, $\xi'(0) = \zeta$, and which is contained in the two dimensional plane generated by $\xi^+$ and $\zeta$. A further rotation of the coordinate system allows us to assume that

     which we may assume, in coordinates is contained in a two dimensional plane with $dx^n$ as one of these axis. By rotating the coordinate system about the origin, we may assume that $x_1 = t_+ e_n$, and that 

    and thus that $\Sigma(x_0,e_n) = \{ x \in \RR^d : x_n = 0 \}$. Let $\xi(t) = \cos(t) e_n + \sin(t) e_k$

    In BLAH it is proved that if $v = L(x,\xi)$, and if we define
    %
    \begin{equation}
        G^a(x_0,v) = \sum\nolimits_{j,k} \Gamma^a_{jk}(x_0,v) v^j v^k,
    \end{equation}
    %
    then as $u \to 0$,
    %
    \begin{equation} \label{AWODJWAIOD123912043535}
        \left| \exp( x_0, u, \xi )^a - \left( x_0^a + u^a - \frac{1}{2} \sum\nolimits_{j,k} \frac{\partial^2 G^a}{\partial v^j \partial v^k}(x_0,v) u^j u^k \right) \right| \lesssim |u|^3.
    \end{equation}
    %
    


     If we let $\xi(t) = \cos(t) $

    The bounds \eqref{AWODJWAIOD123912043535}


    Let $g$ be the Riemannian metric in this coordinate system with coefficients $g_{ij}(x, e_n)$ induced by the Finsler metric.

    Let $v_+$ be the Legendre transform of $\xi^+$. If we define a vector $G$ by setting
    %
    \[ G^a(x_0,v) = (1/2) \Gamma^a_{jk}(x_0,v) v^j v^k, \]
    %
    then
    %



    We choose to work in normal coordinates about $x_0$, and let us choose these coordinates so that $x_0$ lies at the origin, and $x_1 = t_0 e_1$, where $t_0 = \psi(\xi_0)$. Then we can identify $S_{x_0}$ with the unit sphere $S^{d-1}$ about the origin in the normal coordinate system, and $\xi_0 = e_1$. Fix a unit vector $\tilde{e} \in S^{d-1}$ with $\tilde{e}_1 = 0$ and define, for a scalar $s \in \RR$,
    %
    \begin{equation}
        \xi(s) = \cos(s) e_1 + \sin(s) \tilde{e}.
    \end{equation}
    %
    The remainder of the proof consists of showing that as $s \to 0$,
    %
    \begin{equation}
        \psi(\xi(s)) \leq t_0(1 - s^2/4) = t_0 - (t_0/2) (s^2/2),
    \end{equation}
    %
    Since $\psi(\xi(0)) = t_0$, this shows that $\partial_s^2 \{ \psi \circ \xi \}(0) \leq - t_0/2$. Since $\tilde{e}$ is arbitrary, this shows that the Hessian of $\psi$ at $e_1$ is negative definite, with all eigenvalues having magnitude exceeding $t_0/2$.



\end{comment}
    \begin{comment}



    The hyperbolic sine law says that for 

     and here we can apply the hyperbolic sine law 

    But we can now calculate this length $L$ using the trigonometry of $\mathbb{H}^2_\kappa$, which will complete the proof.


    Let $x(s)$ be the point on $\Sigma(x_0,\xi(s))$ such that the geodesic ray from $x(s)$ to $x_1$ has tangent vector $\xi(s)$ at $x(s)$, let $t(s) = \psi(\xi(s))$ denote the length of this geodesic, let $l(s)$ denote the length of the geodesic ray from $x_0$ to $x(s)$, and let $\theta(s)$ denote the angle between the geodesic ray from $x_0$ to $x_1$ and from $x_0$ to $x(s)$. Then $\theta(0) = \pi/2$, and in a technical calculation involving Jacobi fields at the end of the proof, we will show that $\theta'(0) \geq 1$. Let us suppose that $\kappa > 0$ is chosen such that, on $U$, the sectional curvature is at least $- \kappa^2$. Theorem 2.2 of \cite{Cheeger}, also known as the Toponogov comparison theorem, implies that in the hyperbolic plane with curvature $-\kappa^2$, there exists a triangle with sidelengths $t_0$ and $l(s)$, with opposing angles $\pi/2$ and $\theta(s)$, and with a final sidelength $L(s)$ for which $L(s) \geq t(s)$. But if we dilate the geodesic triangle $\Delta$ by $1/\kappa$, we obtain a geodesic triangle in the hyperbolic plane of curvature $-1$, with the same angles, but with dilated sidelengths. Applying the hyperbolic sine law, which in this situation states that
    %
    \[ \frac{\sinh(L(s) / \kappa)}{\sin(\theta(s))} = \frac{\sinh(t_0 / \kappa)}{\sin(\pi / 2)}, \]
   
   %
   See Section 1.6 of \cite{Katok} for a discussion of the hyperbolic sine law. But now, rearranging, we have
   %
   \[ \sinh(L(s) / \kappa) = \sin(\theta(s)) \sinh(t_0 / \kappa). \]
   %
   Taking $s \to 0$, and using the bounds on $\theta'(0)$, we conclude that
   %
   \[ L(0) = t_0 \quad\text{and}\quad L'(0) = 0 \quad\text{and}\quad L''(0) \geq \kappa\; \text{Tanh}( t_0 / \kappa ). \]
   %
   By Toponogov's theorem, $t''(0) \geq \kappa\; \text{Tanh}( t_0 / \kappa ) \gtrsim t_0$, which shows that $\xi_0$ is a non-degenerate critical point, with each eigenvalue of the Hessian having magnitude exceeding $t_0$.

   Now let's prove that $\theta'(0) \geq 1$, which will complete the proof. To do this, we begin by approximating $x(s)$ as $s \to 0$ by implicitly differentiate equations involving $x$. Let's start by differentiating the equation
    %
    \[ x(s) \cdot \xi(s) = 0, \]
    %
    which holds since $x(s) \in \Sigma(x_0,\xi(s))$. Since $x(0) = 0$, implicitly differentiating the first equation gives
    %
    \[ x'(s) \cdot \xi(s) + x(s) \cdot \xi'(s) = 0. \]
    %
    Setting $s = 0$, and noting that $x(0) = 0$ and $\xi(0) = e_1$, we find that
    %
    \[ x'_1(0) = 0. \]
    %
    We can also calculate the second derivative of the function $s \mapsto x(s) \cdot \xi(s)$, obtaining that
    %
    \[ x''(s) \cdot \xi(s) + 2 x'(s) \cdot \xi'(s) + x(s) \cdot \xi''(s) = 0. \]
    %
    Since $x(0) = 0$, we conclude that, at zero, since $x(0) = 0$, $\xi(0) = e_1$, and $\xi'(0) = w$, we have
    %
    \begin{equation} \label{seconderivxequation}
        x''_1(0) + 2 x'(0) \cdot w = 0.
    \end{equation}
    %
    We will come back to this equation later, after implicitly differentiating the equation
    %
    \[ \exp_{x(s)}(t(s) \xi(s)) = x_1 \]
    %
    to obtain a good estimate for $x'(0) \cdot w$, which requires more calculation. Implicitly differentiating, we conclude that
    %
    \[ a'(s) + b'(s) + c'(s) = 0, \]
    %
    where
    %
    \[ a(s) = \exp_{x(s)}(t_0 \xi_0), \quad b(s) = \exp_0( t(s) \xi_0 ), \quad\text{and}\quad c(s) = \exp_0( t_0 \xi(s) ). \]
    %
    The functions $b$ and $c$ are easy to differentiate since $\exp_0(v) = v$. Thus $c(s) = t_0 \xi(s)$, and so $c'(0) = t_0 \xi'(0) = t_0 w$, and $b(s) = t(s) \xi_0$, and so $b'(0) = t'(0) \xi_0 = 0$, where $t'(0) = 0$ follows because $t(0)$ is the maximum value of $t$. Differentiating $a$ is more technical, and we will be aided here by the theory of Jacobi fields (see \cite{Cheeger}, Chapter 1, Section 4). If we define $f(x) = \exp_x(t_0 \xi_0)$, then the chain rule implies $a'(0) = Df(0)[x'(0)]$. Since we know $x'_1(0) = 0$, we will be able to determine $a'(0)$ by calculation $Df(0)[v]$, for some unit vector $v$ with $v_1 = 0$. We consider the variation
    %
    \[ \alpha(t,s) = \exp_{sv}(t e_1). \]
    %
    Consider the vector field $T(t) = (\partial_t \alpha)(t,0)$ and $J(t) = (\partial_s \alpha)(t,0)$. Then $J$ is a Jacobi field, satisfying the differential equation
    %
    \[ J'' = R(J,T) T, \]
    %
    where $R(X,Y)Z$ is a tangent vector obtained by applying the vectors $X$, $Y$, and $Z$ to the Riemann curvature tensor on $M$. Since $\alpha(t,0) = t e_1$, $T(t) = e_1$ for all $t$. We can thus write
    %
    \[ R(J(t),T(t))T(t) = A(t) J(t), \]
    %
    We have $A(t)_{jk} = R_{jk11}(t)$, and because we are working in normal coordinates, for $k \neq 1$, $|R_{jk11}(t)| \lesssim t$. Since $v_1 = 0$, we have $\| A(t) \| \lesssim t$. Now $J(0) = v$, and $J'(0) = 0$, and so applying e.g. a Neumann series to write out the solution of the equation $J''(t) = A(t) J(t)$, we have $|J(t_0) - J(0)| \lesssim t_0^2$. But $J(t_0) = Df(0)[v]$, and so we obtain that
    %
    \[ |Df(0)[v] - v| \lesssim t_0^2. \]
    %
    Thus if $a'(0) = x'(0) + t_0^2 u$, then $|u| \lesssim 1$. But now we conclude that
    %
    \[ 0 = a'(0) + b'(0) + c'(0) = x'(0) + t_0^2 u + t_0 w. \]
    %
    Thus $x'(0) = - t_0 w - t_0^2 u$. Since we know $x'_1(0)$, we have $u_1 = 0$. But now returning to \eqref{seconderivxequation}, we can conclude that
    %
    \[ |x'(0) \cdot w + t_0| = |t_0^2 w \cdot u| \lesssim t_0^2, \]
    %
    we thus find that
    %
    \[ |x_1''(0) - 2 t_0| = |- 2 x'(0) \cdot (0,w) + 2 t_0| \lesssim t_0^2. \]
    %
    If $\varepsilon$ is chosen appropriately with respect to the implicit constant in this equation, which is uniform given the precompactness of $U$, then the bound $t_0 \leq \varepsilon$ implies that $x_1''(0) \geq t_0$, and $|x'(0)| \geq t_0 / 2$. But if we now consider the \emph{angle} $\theta(s)$ between the geodesic from $x_0$ to $x_1$ and the geodesic from $x_0$ to $x(s)$, then
    %
    \[ \cos(\theta(s)) = \frac{e_1 \cdot x(s)}{|e_1| |x(s)|} = \frac{x_1(s)}{|x(s)|}. \]
    %
    Then
    %
    \[ \cos(\theta(s)) = \frac{x_1''(0) s^2 / 2 + O(s^3)}{|x'(0)| s + O(s^2)} \geq s + O(s^2). \]
    %
    Thus $\theta(s) \leq \pi/2 - s + O(s^2)$. Since $\theta(0) = \pi/2$, we conclude that $\theta'(0) \geq 1$.
    \end{comment}
\end{proof}

\section{Analysis of Regime I via Density Methods} \label{regime1firstsection}

\subsection{\boldmath $L^2$ Estimates For Bounded Density Inputs}

We now begin obtaining bounds for the operator $T^I$ specified in Proposition \ref{TjbLemma} by using the quasi-orthogonality estimates of Proposition \ref{theMainEstimatesForWave}. Define a metric $d_M = d_M^+ + d_M^-$ on $M$. Given an input $u: M \to \CC$, we consider a maximal $1/R$ separated subset $\mathcal{X}_R$ of $M$, and then consider a decomposition $u = \sum_{x_0 \in \mathcal{X}_R} u_{x_0}$ with respect to some partition of unity, where $\text{supp}(u_{x_0}) \subset B(x_0,1/R)$. The balls $\{ B(x_0,1/R) : x_0 \in \mathcal{X}_R \}$ have finite overlap, and so
%
\begin{equation}
    \| u \|_{L^p(M)} \sim \left( \sum\nolimits_{x_0 \in \mathcal{X}_R} \| u_{x_0} \|_{L^p(M)}^p \right)^{1/p}.
\end{equation}
%
If we set $f_{x_0,t_0} = T_{t_0}^I \{ u_{x_0} \}$, then
%
\begin{equation} \label{DAPOCJAPWOCJAWOIFJOI}
    \left\| T^I u \right\|_{L^p(M)} = \left\| \sum\nolimits_{(x_0,t_0) \in \mathcal{X}_R \times \mathcal{T}_R} f_{x_0,t_0} \right\|_{L^p(M)}.
\end{equation}
%
In this subsection, we use the quasi-orthogonality estimates of the last section to obtain $L^2$ estimates on partial sums of a family of functions of the form $\{ {S\!}_{x_0,t_0} \}$, which are essentially $L^1$ normalized versions of the functions $\{ f_{x_0, t_0} \}$, under a density assumption on the set of indices we are summing over. Namely, we say a set $\mathcal{E} \subset \mathcal{X}_R \times \mathcal{T}_R$ has \emph{density type} $(A_0,A_1)$ if for any set $B \subset \mathcal{X}_R \times \mathcal{T}_R$ with $1/R \leq \text{diam}(B) \leq A_1/R$,
%
\begin{equation}
    \#(\mathcal{E} \cap B) \leq R A_0\; \text{diam}(B). \footnote[1]{This definition of density is chosen because it is `scale-invariant' as we change the parameter $R$. Indeed, if $M = \RR^d$, $\mathcal{X}_R = (\ZZ / R)^d$, and $\mathcal{T}_R = \ZZ/R$, then a set $\mathcal{E} \subset (\ZZ / R)^d \times (\ZZ/R)$ has density type $(A_0,A_1)$ if and only if $R\; \mathcal{E} \subset \ZZ^d \times \ZZ$ has density type $(A_0,A_1)$.}
\end{equation}
%
To obtain $L^p$ bounds from these $L^2$ bounds, in the next section we will perform a \emph{density decomposition} to break up $\mathcal{X}_R \times \mathcal{T}_R$ into families of indices with controlled density, and then apply Proposition \ref{L2DensityProposition} on each subfamily to control \eqref{DAPOCJAPWOCJAWOIFJOI} via an interpolation.

\begin{prop} \label{L2DensityProposition}
    Fix $A \geq 1$. Consider a set $\mathcal{E} \subset \mathcal{X}_R \times \mathcal{T}_R$. Suppose that for each $(x_0,t_0) \in \mathcal{E}$, we pick two measurable functions $b_{t_0}: I_0 \to \RR$ and $u_{x_0}: M \to \RR$, supported on $I_{t_0}$ and $B(x_0,1/R)$ respectively, such that $\| b_{t_0} \|_{L^1(I_0)} \leq 1$ and $\| u_{x_0} \|_{L^1(M)} \leq 1$. Define 
    %
    \begin{equation}
        {S\!}_{x_0,t_0} = \int b_{t_0}(t) (Q_R \circ e^{2 \pi i t P} \circ Q_R) u_{x_0}\; dt.
    \end{equation}
    %
    Write $\mathcal{E} = \bigcup_{k = 0}^\infty \mathcal{E}_k$, where
    %
    \begin{equation}
        \mathcal{E}_0 = \{ (x,t) \in \mathcal{E}: 0 \leq |t| \leq 1/R \}
    \end{equation}
    %
    and for $k > 0$, define
    %
    \begin{equation}
        \mathcal{E}_k = \{ (x,t) \in \mathcal{E}: 2^{k-1} / R < |t| \leq 2^k / R \}.
    \end{equation}
    %
    Suppose that for each $k$, the set $\mathcal{E}_k$ has density type $(A,2^{k})$.
    %, i.e. so that for any set $B \subset \mathcal{X}_R \times \mathcal{T}_R$ with $\text{diam}(B) \leq 2^{k}/R$,
    %
%    \begin{equation}
%        \#( \mathcal{E}_k \cap B ) \leq R A\; \text{diam}(B).
%    \end{equation}
    %
    Then
    %
    \begin{equation} \label{DOIAWJDOIWAJDOIwdj21312321}
        \Big\| \sum\nolimits_k \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_k} 2^{k \frac{d-1}{2}} {S\!}_{x_0,t_0} \Big\|_{L^2(M)}^2 \lesssim R^d \log(A) A^{\frac{2}{d-1}} \sum\nolimits_k 2^{k(d-1)} \# \mathcal{E}_k.
    \end{equation}
\end{prop}

\begin{remark}
    If $\| b_{t_0} \|_{L^1(I_0)} \sim 1$ and $\| u_{x_0} \|_{L^1(M)} \sim 1$, then locally constancy from the uncertainty principle and energy conservation of the wave equation tell us that morally,
    %
    \begin{equation}
        \| S_{x_0,t_0} \|_{L^2(M)}^2 \sim R^d t_0^{d-1}.
    \end{equation}
    %
    If $\| b_{t_0} \|_{L^1(I_0)} \sim 1$ and $\| u_{x_0} \|_{L^1(M)} \sim 1$ for all $(x_0,t_0) \in \mathcal{E}$, this means that Proposition \ref{L2DensityProposition} is morally equivalent to
    %
    \begin{equation}
        \Big\| \sum\nolimits_{(x_0,t_0) \in \mathcal{E}} {S\!}_{x_0,t_0} \Big\|_{L^2(M)} \lesssim \sqrt{\log(A)} A^{\frac{1}{d-1}} \left( \sum\nolimits_{(x_0,t_0) \in \mathcal{E}} \| {S\!}_{x_0,t_0} \|_{L^2(M)}^2 \right)^{1/2}.
    \end{equation}
    %
    Thus Proposition \ref{L2DensityProposition} is a kind of square root cancellation bound, albeit with an implicit constant which grows as the set $\mathcal{E}$ increases in density, a necessity given that the functions $\{ {S\!}_{x_0,t_0} \}$ are not almost-orthogonal to one another.
\end{remark}

\begin{proof}
Write $F = \sum_k F_k$, where
%
\begin{equation}
    F_k = 2^{k \frac{d-1}{2}} \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_k} {S\!}_{x_0,t_0}.
\end{equation}
%
Our goal is to bound $\| F \|_{L^2(M)}$. Applying Cauchy-Schwarz, we have
%
\begin{equation} \label{loglossbound}
    \| F \|_{L^2(M)}^2 \leq \log(A) \left( \sum\nolimits_{k \leq \log(A)} \| F_k \|_{L^2(M)}^2 + \left\| \sum\nolimits_{k \geq \log(A)} F_k \right\|_{L^2(M)}^2 \right).
\end{equation}
%
Without loss of generality, increasing the implicit constant in the final result by applying the triangle inequality, we can assume that $\{ k : \mathcal{E}_k \neq \emptyset \}$ is $10$-separated, and that all values of $t$ with $(x,t) \in \mathcal{E}$ are positive. Thus if $F_k$ and $F_{k'}$ are both nonzero functions, then $k = k'$ or $|k - k'| \geq 10$.

%We consider an expansion
%
%\begin{equation} \label{AWIOJDIOWAJF190124214}
%    \left\| \sum\nolimits_{k \geq \log(A)} F_k \right\|_{L^2(M)}^2 = \sum\nolimits_{k,k' \geq \log(A)} \langle F_k, F_{k'} \rangle.
%\end{equation}
%
Let us estimate $\langle F_k, F_{k'} \rangle$ for $k \geq k' + 10$.
%the non-diagonal terms on the right hand side of \eqref{AWIOJDIOWAJF190124214}.
We write
%
\begin{equation}
    \langle F_k, F_{k'} \rangle = \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_k} \sum\nolimits_{(x_1,t_1) \in \mathcal{E}_{k'}} 2^{k \frac{d-1}{2}} 2^{k' \frac{d-1}{2}} \langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle.
\end{equation}
%
For each $(x_0,t_0) \in \mathcal{E}_k$, and each $k' \leq k - 10$, consider the set
%
\begin{equation}
    \mathcal{G}_0(x_0,t_0,k') = \{ (x_1,t_1) \in \mathcal{E}_{k'} : |(t_0 - t_1) - d_M^-(x_0,x_1)| \leq 2^{k' + 5} / R \},
\end{equation}
%
Also consider the sets of indices
%
\begin{equation}
    \mathcal{G}_l^+(x_0,t_0,k') = \{ (x_1,t_1) \in \mathcal{E}_{k'} : 2^{l} / R < |(t_0 - t_1) + d_M^+(x_0,x_1)| \leq 2^{l + 1} / R \}.
\end{equation}
%
and
%
\begin{equation}
    \mathcal{G}_l^-(x_0,t_0,k') = \{ (x_1,t_1) \in \mathcal{E}_{k'} : 2^{l} / R < |(t_0 - t_1) - d_M^-(x_0,x_1)| \leq 2^{l + 1} / R \}.
\end{equation}
%
If we set
%
\begin{equation}
    \mathcal{G}_0(x_0,t_0,k') = (\mathcal{G}_l^+(x_0,t_0,k') \cup \mathcal{G}_l^-(x_0,t_0,k'))
\end{equation}
%
and
%
\begin{equation}
\begin{split}
    \mathcal{G}_l(x_0,t_0,k') &= \Big( \mathcal{G}_l^+(x_0,t_0,k') \cup \mathcal{G}_l^-(x_0,t_0,k') \Big)\\
    &\quad\quad\quad\quad\quad\quad - \bigcup\nolimits_{r < l} \Big(\mathcal{G}_r^+(x_0,t_0,k') \cup \mathcal{G}_r^-(x_0,t_0,k') \Big).
\end{split}
\end{equation}
%
Then $\mathcal{E}_{k'}$ is covered by $\mathcal{G}_0(x_0,t_0,k')$ and $\mathcal{G}_l(x_0,t_0,k')$ for $k' + 5 \leq l \leq 10 \log R$. Define
%
\begin{equation}
    B_0(x_0,t_0,k') = \sum\nolimits_{(x_1,t_1) \in \mathcal{G}_0(x_0,t_0,k')} 2^{k \frac{d-1}{2}} 2^{k' \frac{d-1}{2}} |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle|,
\end{equation}
%
and
%
\begin{equation}
    B_l(x_0,t_0,k') = \sum\nolimits_{(x_1,t_1) \in \mathcal{G}_l(x_0,t_0,k')} 2^{k \frac{d-1}{2}} 2^{k' \frac{d-1}{2}} |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle|.
\end{equation}
%
We thus have
%
\begin{equation}
    \langle F_k, F_{k'} \rangle \leq \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_k} B_0(x_0,t_0,k') + \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_k} \sum\nolimits_{k' + 5 \leq l \leq 10 \log R} B_l(x_0,t_0,k').
\end{equation}
%
Using the density properties of $\mathcal{E}$, we can control the size of the index sets $\mathcal{G}_{\bullet}(x_0,t_0,k')$, and thus control the quantities $B_\bullet(x_0,t_0,k')$. The rapid decay of Proposition \ref{theMainEstimatesForWave} means that only $\mathcal{G}_0(x_0,t_0,k')$ needs to be estimated rather efficiently:
%
\begin{itemize}%[leftmargin=8mm]
    \item Start by bounding the quantities $B_0(x_0,t_0,k')$. If $(x_1,t_1) \in \mathcal{G}_0(x_0,t_0,k')$, then
    %
    \begin{equation}
        |d_M^-(x_0,x_1) - (t_0 - t_1)| \leq 2^{k'+5}/R,
    \end{equation}
    %
    Thus if we consider $\text{Ann}(x_0,t_0,k') = \{ x_1: |d_M^-(x_0,x_1) - t_0| \leq 2^{k'+8}/R \}$, which is a geodesic annulus of radius $\sim 2^k / R$ and thickness $O(2^{k'}/R)$, then
    %
    \begin{equation}
        \mathcal{G}_0(x_0,t_0,k') \subset \text{Ann}(x_0,t_0,k') \times [ 2^{k'}/R, 2^{k'+1}/R ].
    \end{equation}
    %
    The latter set is covered by $O(2^{(k-k')(d-1)})$ balls of radius $2^{k'}/R$, and so the density properties of $\mathcal{E}_{k'}$ implies that
    %
    \begin{equation} \label{G0Size}
        \#( \mathcal{G}_0(x_0,t_0,k') ) \lesssim A 2^{(k-k')(d-1)} 2^{k'}. 
    \end{equation}
    %
    Since $k \geq k' + 10$, for $(x_1,t_1) \in \mathcal{G}_0(x_0,t_0,k')$ we have $d_M(x_0,x_1) \gtrsim 2^k / R$ and so
    %
    \begin{equation} \label{InnerProductG0Size}
        \langle S_{x_0,t_0}, S_{x_1,t_1} \rangle \lesssim R^d 2^{-k \left( \frac{d-1}{2} \right)}.
    \end{equation}
    %
    But putting together \eqref{G0Size} and \eqref{InnerProductG0Size} gives that
    %
    \begin{align*}
        B_0(x_0,t_0,k') &\leq (2^{k \frac{d-1}{2}} 2^{k' \frac{d-1}{2}}) ( A 2^{(k-k')(d-1)} 2^{k'} )  ( R^d 2^{-k \left( \frac{d-1}{2} \right)} )\\
        &= A R^d 2^{k(d-1)} 2^{-k' \frac{d-3}{2}}.
    \end{align*}
    %
    Thus for each $k$, since $d \geq 4$,
    % d = 2: get an extra O( log k ) factor
    \begin{equation} \label{AAAlowbounds}
        \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_k} \sum\nolimits_{k' \in [\log(A), k - 10]} B_0(x_0,t_0,k') \lesssim R^{d} 2^{k (d-1)} \# \mathcal{E}_k.
    \end{equation}

    \item Next we bound $B_l(x_0,t_0,k')$ for $k' + 5 \leq l \leq k - 5$. The set $\mathcal{G}_l^+(x_0,t_0,k')$ is empty in this case. Thus
    %
    \begin{equation}
        \mathcal{G}_l(x_0,t_0,k') \subset ( \text{Ann} \cup \text{Ann}' ) \times [t_0 - 2^{k'} / R, t_0 + 2^{k'} / R],
    \end{equation}
    %
    where
    %
    \begin{equation}
        \text{Ann} = \{ x \in M: |d_M^-(x_0,x) - (t_0 - 2^{k'}) / R| \leq 100 \cdot 2^l / R \}
    \end{equation}
    %
    and
    %
    \begin{equation}
        \text{Ann}' = \{ x \in M: |d_M^-(x_0,x) - (t_0 + 2^{k'}) / R| \leq 100 \cdot 2^l / R \},
    \end{equation}
    %
    These are geodesic annuli of thickness $O(2^l / R)$ and radius $\sim 2^k$. Thus $\mathcal{G}_l(x_0,t_0,k')$ is covered by $O( 2^{(l-k')} 2^{(k-k')(d-1)} )$ balls of radius $2^{k'} / R$, and the density of $\mathcal{E}_{k'}$ implies that
    %
    \begin{equation}
        \#(\mathcal{G}_l(x_0,t_0,k')) \lesssim R A\; 2^{(l-k')} 2^{(k-k')(d-1)} 2^{k'} / R = A 2^{l} 2^{(k-k')(d-1)}.
    \end{equation}
    %
    For $(x_1,t_1) \in \mathcal{G}_l(x_0,t_0,k')$, $d_M(x_0,x_1) \sim 2^k / R$, and thus Proposition \ref{theMainEstimatesForWave} implies
    %
    \begin{equation}
        |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle| \lesssim R^d 2^{-k \frac{d-1}{2}} 2^{-lK}.
    \end{equation}
    %
    Thus for any $K \geq 0$,
    %
    \begin{equation}
    \begin{split}
        B_l(x_0,t_0,k') &\lesssim_K \Big( A 2^{l} 2^{(k-k')(d-1)} \Big)  R^{d} 2^{k \frac{d-1}{2}} 2^{k' \frac{d-1}{2}} \Big( 2^{-k \frac{d-1}{2}} 2^{-lK} \Big)\\
        &\lesssim A R^d 2^l 2^{k(d-1)} 2^{-k' \frac{d-1}{2}} 2^{-lK}.
    \end{split}
    \end{equation}
    %
    Picking $K > 1$, we conclude that
    % -d/2 + 1/2
    \begin{equation} \label{AAAlBoundSmall}
        \sum_{(x_0,t_0) \in \mathcal{E}_k} \sum_{k' \in [\log(A), k - 10]} \sum_{l \in [k' + 10, k - 5]} B_l(x_0,t_0,k') \lesssim R^{d} 2^{k (d-1)} \# \mathcal{E}_k.
    \end{equation}

    \item Finally, let's bound $B_l(x_0,t_0,k')$ for $k - 5 \leq l \leq 10 \log R$. If either $(x_1,t_1) \in \mathcal{G}_l^-(x_0,t_0,k')$ or $(x_1,t_1) \in \mathcal{G}_l^+(x_0,t_0,k')$, then $d_M(x_0,x_1) \lesssim 2^l / R$. So $\mathcal{G}_l(x_0,t_0,k')$ is covered by $O( 2^{(l-k')d} )$ balls of radius $2^{k'} / R$, and thus
    %
    \begin{equation}
        \#(\mathcal{G}_l(x_0,t_0,k')) \lesssim R A\; 2^{(l-k')d} (2^{k'} / R) = A 2^{(l-k')d} 2^{k'}.
    \end{equation}
    %
    For $(x_1,t_1) \in \mathcal{G}_l(x_0,t_0,k')$, we have no good control over $d_M(x_0,t_1)$ aside from the trivial estimate $d_M(x_0,x_1) \lesssim 1$. Thus Proposition \ref{theMainEstimatesForWave} yields a bound of the form
    %
    \begin{equation}
        |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle| \lesssim R^d 2^{-lK}.
    \end{equation}
    %
    Thus we conclude that
    %
    \begin{equation}
    \begin{split}
        B_l(x_0,t_0,k') &\lesssim_N R^{d} 2^{k \frac{d-1}{2}} 2^{k' \frac{d-1}{2}} \Big( A 2^{(l-k')d} 2^{k'} \Big) \Big( 2^{-lN} \Big)\\
        &= A R^d 2^{k \frac{d-1}{2}} 2^{-k' \frac{d-1}{2}} 2^{-lN}
    \end{split}
    \end{equation}
    %
    Picking $K > d$, we conclude that
    %
    \begin{equation} \label{AAAlBoundBig}
        \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_k} \sum\nolimits_{k' \in [\log(A), k - 10]} \sum\nolimits_{l \in [k+10,\log R]} B_l(x_0,t_0,k')  \lesssim R^d.
    \end{equation}
\end{itemize}
%
The three bounds \eqref{AAAlowbounds}, \eqref{AAAlBoundSmall} and \eqref{AAAlBoundBig} imply that
%
\begin{equation} \label{DADAOWIDJAWOIDJAWDaweq13412}
    \sum\nolimits_k \sum\nolimits_{k' \in [\log(A), k]} |\langle F_k, F_{k'} \rangle| \lesssim R^d \sum\nolimits_k 2^{k (d-1)} \# \mathcal{E}_k.
\end{equation}
%
In particular, combining \eqref{DADAOWIDJAWOIDJAWDaweq13412} with \eqref{loglossbound}, we have
%
\begin{equation} \label{DPOWADPAWKDPOWAKDOPWAK}
    \| F \|_{L^2(M)}^2 \lesssim \log(A) \left( \sum\nolimits_k \| F_k \|_{L^2(M)}^2 + R^d \sum\nolimits_k 2^{k (d-1)} \# \mathcal{E}_k \right).
\end{equation}
%
Next, consider some parameter $a$ to be determined later, and decompose the interval $[2^{k} / R, 2^{k+1} / R]$ into the disjoint union of length $A^a / R$ intervals of the form
%
\begin{equation}
    I_{k,\mu} = [ 2^{k} / R + (\mu - 1) A^a / R, 2^{k} / R + \mu A^a / R] \quad\text{for $1 \leq \mu \leq 2^k/A^a$}.
\end{equation}
%
We thus consider a further decomposition $\mathcal{E}_k = \bigcup \mathcal{E}_{k,\mu}$, where $F_k = \sum F_{k,\mu}$. As before, increasing the implicit constant in the Proposition, we may assume without loss of generality that the set $\{ \mu: \mathcal{E}_{k,\mu} \neq \emptyset \}$ is $10$-separated. We now estimate
%
\begin{equation}
    \sum\nolimits_{\mu \geq \mu' + 10} |\langle F_{k,\mu}, F_{k,\mu'} \rangle|.
\end{equation}
%
For $(x_0,t_0) \in \mathcal{E}_{k,\mu}$ and $l \geq 1$, define
%
\begin{equation}
    \mathcal{H}_l(x_0,t_0,\mu') = \Big\{ (x_1,t_1) \in \mathcal{E}_{k,\mu'} : \frac{2^l A^a}{2R} \leq \max(d_M(x_0,x_1), t_0 - t_1) \leq \frac{2^l A^a}{R} \Big\}.
\end{equation}
%
Then $\bigcup_{l \geq 1} \mathcal{H}_l(x_0,t_0,\mu')$ covers $\bigcup_{\mu \geq \mu' + 10} \mathcal{E}_{k,\mu'}$. Set
%
\begin{equation}
    B'_l(x_0,t_0,\mu') = \sum\nolimits_{(x_1,t_1) \in \mathcal{H}_l(x_0,t_0,\mu')} 2^{k(d-1)} |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle|.
\end{equation}
%
Then
%
\begin{equation}
    \langle F_{k,\mu}, F_{k,\mu'} \rangle \leq \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_{k,\mu}} \sum\nolimits_l B'_l(x_0,t_0,\mu').
\end{equation}
%
We now bound the constants $B'_l$. Pick a constant $r$ such that $d_M \leq 2^r d_M^+$ and $d_M \leq 2^r d_M^-$. As in the estimates of the quantities $B_l$, the quantities where $l$ is large have negligible magnitude:
%
\begin{itemize}[leftmargin=8mm]
    \item For $l \leq k - a \log_2 A + 10 r$, we have $2^l A^a / R \lesssim 2^k / R$. The set $\mathcal{H}_l(x_0,t_0,\mu')$ is covered by $O(1)$ balls of radius $2^l A^a / R$, and density properties imply
    %
    \begin{equation}
        \# \mathcal{H}_l(x_0,t_0,\mu') \lesssim (R A) (2^l A^a / R) = A^{a+1} 2^l
    \end{equation}
    %
    For $(x_1,t_1) \in \mathcal{H}_l(x_0,t_0,\mu')$, we claim that
    %
    \begin{equation}
        2^{k(d-1)} |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle| \lesssim R^{d} 2^{k(d-1)} (2^l A^a)^{- \frac{d-1}{2}}.
    \end{equation}
    %
    Indeed, for such tuples we have
    %
    \begin{equation}
        d_M(x_0,x_1) \gtrsim 2^l A^a / R \quad\text{or}\quad \min\nolimits_{\pm} |d_M^{\pm}(x_0,x_1) - (t_0 - t_1)| \gtrsim 2^l A^a / R,
    \end{equation}
    %
    and the estimate follows from Proposition \ref{theMainEstimatesForWave} in either case. Since $d \geq 4$, we conclude that
    %
    \begin{align} \label{BBBEquation}
    \begin{split}
        &\sum\nolimits_{l \in [1, k - a \log_2 A + 10]} B'_l(x_0,t_0,,\mu')\\
        &\quad\quad\quad\quad \lesssim \sum\nolimits_{l \in [1, k - a \log_2 A + 10]} R^{d} (2^{k(d-1)}) (2^l A^a)^{- \frac{d-1}{2}} (A^{a+1} 2^l)\\
        &\quad\quad\quad\quad \lesssim \sum\nolimits_{l \in [1, k - a \log_2 A + 10]} R^{d}  2^{k(d-1)} 2^{-l \frac{d-3}{2}} A^{1 - a \left( \frac{d-3}{2} \right)}\\
        &\quad\quad\quad\quad \lesssim R^{d} 2^{k(d-1)} A^{1 - a \left( \frac{d-3}{2} \right)}.
    \end{split}
    \end{align}

    \item For $l > k - a \log_2 A + 10 r$, a tuple $(x_1,t_1) \in \mathcal{E}_k$ lies in $\mathcal{H}_l(x_0,t_0,\mu')$ if and only if $2^l A^a / 2 R \leq d_M(x_0,x_1) \leq 2^l A^a / R$, since we always have
    %
    \begin{equation}
         t_0 - t_1 \leq 2^{k+1}/R < 2^{l+r} A^a / 8R.
    \end{equation}
    %
    and so $d_M(x_0,x_1) \geq 2^l A^a / 2R$. And so 
    %
    \[ |(t_0 - t_1) - d_M^-(x_0,x_1)| \geq 2^{-r} 2^l A^a / 2R - 2^{k+1} / R \geq 2^{-r} 2^l A^a / 4R. \]
    %
    Also $|(t_0 - t_1) + d_M^+(x_0,x_1)| \geq |d_M^+(x_0,x_1) \geq 2^{-r} 2^l A^a / 4R$. Thus we conclude from Proposition \ref{theMainEstimatesForWave} that
    %
    \begin{equation}
        2^{k(d-1)} |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle| \lesssim_K R^{d} 2^{k(d-1)} (2^l A^a)^{- K}.
    \end{equation}
    %
    Now $\mathcal{H}_l(x_0,t_0,\mu')$ is covered by $O( (2^{l-k} A^a)^d )$ balls of radius $2^k / R$, and the density properties of $\mathcal{E}_k$ thus imply that
    %
    \begin{equation}
        \#(\mathcal{H}_l(x_0,t_0,\mu')) \lesssim (RA) (2^{l-k} A^a)^d ( 2^k / R ) \lesssim A^{1 + ad} 2^{ld} 2^{-k(d-1)}.
    \end{equation}
    %
    Thus, picking $K > \max(d,1+ad)$, we conclude that
    %
    \begin{align} \label{BBB2}
    \begin{split}
        &\sum\nolimits_{l \geq k - a \log_2 A + 10} B'_l(x_0,t_0,\mu')\\
        &\quad \lesssim R^{d} \sum\nolimits_{l \geq k - a \log_2 A + 10} (2^{k(d-1)}) (2^l A^a)^{-M} A^{1 + ad} 2^{ld} 2^{-k(d-1)} \lesssim R^{d}.
    \end{split}
    \end{align}
    \end{itemize}
    %
    Combining \eqref{BBBEquation} and \eqref{BBB2}, and then summing over the tuples $(x_0,t_0) \in \mathcal{E}_{k,\mu}$, we conclude that
    %
    \begin{equation} \label{DOUIAWJDOIAWJVIO}
        \sum\nolimits_{\mu \geq \mu' + 10} |\langle F_{k,\mu}, F_{k,\mu'} \rangle| \lesssim R^{d} \left( 1 + 2^{k(d-1)} A^{1 - a \left( \frac{d-3}{2} \right)} \right) \# \mathcal{E}_{k,\mu}.
    \end{equation}
    %
    Now summing in $\mu$, \eqref{DOUIAWJDOIAWJVIO} implies that
    %
    \begin{equation} \label{DAOWDHAODWWID}
        \| F_k \|_{L^2(M)}^2 \lesssim \sum\nolimits_\mu \| F_{k,\mu} \|_{L^2(M)}^2 + R^{d} \left( 1 + 2^{k(d-1)} A^{1 - a \left( \frac{d-3}{2} \right)} \right) \# \mathcal{E}_k.
    \end{equation}
\begin{comment}

But this means that
%
\[ \sum_{(x_1,t_1) \in \mathcal{H}_{x_0,t_0,l}} |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle| \lesssim (2^l u^a)^{- \frac{d-1}{2}} (u^{a+1} 2^l) = u^{1 - a \left( \frac{d-3}{2} \right)} 2^{- l \left( \frac{d-3}{2} \right)}. \]
%
Since $d \geq 4$, we can sum to conclude that
%
\[ \sum_{1 \leq l \leq k - a \log_2 u} \]
Summing over $1 \leq l \leq k - a \log_2 u$, 

\[ \lesssim \frac{1}{(R d_M(x_0,x_1))^{\frac{d-1}{2}}} \langle R | d_M(x_0,x_1) - (t_0 - t_1) | \rangle^{-M} \]


 and $0 \leq l \lesssim R u^{-a}$, define
%
\[ \mathcal{H}_{x_0,t_0,l} = \left\{ (x_1,t_1) \in \mathcal{E}_{k,\mu'} : l(u^a / R) \leq d_M(x_0,x_1) \leq (l+1)(u^a/R) \right\}. \]
%
Note that for $(x_0,t_0) \in \mathcal{E}_{k,\mu}$ and $(x_1,t_1) \in \mathcal{E}_{k,\mu'}$, $t_0 - t_1$ lies in a radius $O(u^a / R)$ interval centered at $(\mu - \mu') (u^a / R)$:
%
\begin{itemize}
    \item For $0 \leq l \leq (\mu - \mu') / 2$, if $(x_1,t_1) \in \mathcal{H}_{x_0,t_0,l}$, then $x_1$ lies in a thickness $O(u^a / R)$, radius $l (u^a / R)$ geodesic annulus centered at $x_0$. Thus $\mathcal{H}_{x_0,t_0,l}$ is covered by $O( \langle l \rangle ^{d-1})$ balls of radius $u^a / R$. Note that we incurred the logarithmic error in $u$ so that we can assume $u^a \leq 2^k$, so that the density properties of $\mathcal{E}_k$ imply that
    %
    \[ \# \mathcal{H}_{x_0,t_0,l} \lesssim (Ru) \langle l^{d-1} \rangle (u^a / R) = u^{a+1} \langle l \rangle^{d-1}. \]
    % We use the logaorithmic error to get u^a <= 2^k
    %
    of\ref{mainOrthogonalityLemma}, we conclude that
    %
    \[ \sum_{(x_1,t_1) \in \mathcal{H}_{x_0,t_0,l}} |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle| \lesssim_M ( u^{a+1} \langle l \rangle^{d-1} ) \left( \langle l u^a \rangle^{- \frac{d-1}{2}} \left( (\mu - \mu') u^a \right)^{-M} \right). \]
    %
    Picking $M \gtrsim d$ and summing over $0 \leq l \leq (\mu - \mu') / 2$, $\mu' \leq \mu - 10$, and $(x_0,t_0) \in \mathcal{E}_{k,\mu}$ gives that
    %
    \[ \sum_{(x_0,t_0) \in \mathcal{E}_{k,\mu}} \sum_{\mu' \leq \mu - 10} \sum_{0 \leq l \leq (\mu - \mu') / 2} \sum_{(x_1,t_1) \in \mathcal{H}_{x_0,t_0,l}'} |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle| \lesssim \# \mathcal{E}_{k,\mu}. \]

    \item For $(\mu - \mu') / 2 \leq l \leq 2 (\mu - \mu')$, if $(x_1,t_1) \in \mathcal{H}_{x_0,t_0,l}$, then $x_1$ lies in a thickness $O(u^a / R)$, radius $O((\mu - \mu') (u^a / R))$ geodesic annulus centered at $x_0$. Thus $\mathcal{H}_{x_0,t_0,l}$ is covered by $O( (\mu - \mu')^{d-1} )$ balls of radius $u^a / R$. The density properties of $\mathcal{E}_k$ imply that
    %
    \[ \# \mathcal{H}_{x_0,t_,l} \lesssim Ru (\mu - \mu')^{d-1} (u^a/R) = u^{a+1} (\mu - \mu')^{d-1}. \]
    %
    Together with Lemma \ref{mainOrthogonalityLemma}, we conclude that
    %
    \begin{align*}
        &\sum_{(x_1,t_1) \in \mathcal{H}_{x_0,t_0,l}} |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle|\\
        &\quad\quad \lesssim_M (u^{a+1} (\mu - \mu')^{d-1}) \left( ((\mu - \mu') u^a )^{- \frac{d-1}{2}} \langle (l - (\mu - \mu')) u^a \rangle^{-M} \right)\\
        &\quad\quad = u^{1 - a \left( \frac{d-3}{2} \right)} (\mu - \mu')^{\frac{d-1}{2}} \langle (l - (\mu - \mu')) u^a \rangle^{-M}.
    \end{align*}
    %
    Picking $M \gtrsim d$ and summing over $(\mu - \mu') / 2 \leq l \leq 2 (\mu - \mu')$, $\mu' \leq \mu - 10$, and $(x_0,t_0) \in \mathcal{E}_{k,\mu}$ gives that
    %
    \[ \sum_{(x_0,t_0) \in \mathcal{E}_{k,\mu}} \sum_{\mu' \leq \mu - 10} \sum_{(\mu - \mu')/2 \leq l \leq 2(\mu - \mu')} \sum_{(x_1,t_1) \in \mathcal{H}_{x_0,t_0,l}'} |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle| \lesssim u^{1 - a \left( \frac{d-3}{2} \right)} \mu^{\frac{d+1}{2}} \# \mathcal{E}_{k,\mu}. \]

    \item For $l > 2(\mu - \mu')$, if $(x_1,t_1) \in \mathcal{H}_{x_0,t_0,l}$, then $x_1$ lies in a geodesic annulus with thickness $O(u^a/R)$ and radius $\gtrsim l u^a / R$, centered at $x_0$. Thus $\mathcal{H}_{x_0,t_0,l}$ is covered by $O(l^{d-1})$ balls of radius $u^a / R$. The density properties of $\mathcal{E}_k$ imply that
    %
    \[ \# \mathcal{H}_{x_0,t_0,l} \lesssim (Ru) l^{d-1} (u^a/R) = u^{a+1} l^{d-1}. \]
    %
    Together with Lemma \ref{mainOrthogonalityLemma}, we conclude that
    %
    \[ \sum_{(x_1,t_1) \in \mathcal{H}_{x_0,t_0,l}} |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle| \lesssim_M (u^{a+1} l^{d-1}) (l u^a)^{-M}. \]
    %
    Picking $M \gtrsim d$ and summing over $l > 2 (\mu - \mu')$, $\mu' \leq \mu - 10$, and $(x_0,t_0) \in \mathcal{E}_{k,\mu}$ gives that
    %
    \[ \sum_{(x_0,t_0) \in \mathcal{E}_{k,\mu}} \sum_{\mu' \leq \mu - 10} \sum_{(\mu - \mu')/2 \leq l} \sum_{(x_1,t_1) \in \mathcal{H}_{x_0,t_0,l}} |\langle {S\!}_{x_0,t_0}, {S\!}_{x_1,t_1} \rangle| \lesssim \# \mathcal{E}_{k,\mu}. \]
\end{itemize}
%
Thus we have, using that $\mu \lesssim 2^k / u^a$, we conclude that
%
\begin{align*}
    \| F_k \|_{L^2(M)}^2 &\lesssim \sum_\mu \| F_{k,\mu} \|_{L^2(M)}^2 + \sum_\mu \left( 1 + u^{1 - a \left( \frac{d-3}{2} \right)} \mu^{\frac{d+1}{2}} \right) \# \mathcal{E}_{k,\mu}\\
    &= \sum_\mu \| F_{k,\mu} \|_{L^2(M)}^2 + \left( 1 + 2^{k \frac{d+1}{2}} u^{1 - a (d-1)} \right) \# \mathcal{E}_k.
\end{align*}
%
\end{comment}
The functions in the sum defining $F_{k,\mu}$ are highly coupled, and it is difficult to use anything except Cauchy-Schwarz to break them apart. Since $\# ( \mathcal{T}_R \cap I_{k,\mu}) \sim A^a$, if we set $F_{k,\mu} = \sum_{t \in \mathcal{T}_R \cap I_{k,\mu}} F_{k,\mu,t}$, where
%
\begin{equation}
    F_{k,\mu,t} = \sum\nolimits_{(x_0,t) \in \mathcal{E}_{k,\mu}} 2^{k \frac{d-1}{2}} {S\!}_{x_0,t}.
\end{equation}
%
Then Cauchy-Schwarz implies that
%
\begin{equation} \label{IOJDAOIWDJAWOIJF}
    \| F_{k,\mu} \|_{L^2(M)}^2 \lesssim A^a \sum\nolimits_{t \in \mathcal{T}_R \cap I_{k,\mu}} \| F_{k,\mu,t} \|_{L^2(M)}^2.
\end{equation}
%
Since the elements of $\mathcal{X}_R$ are $1/R$ separated, the functions in the sum defining $F_{k,\mu,t}$ are quite orthogonal to one another; Proposition \ref{theMainEstimatesForWave} implies that for $x_0 \neq x_1$,
%
\begin{equation}
    |\langle {S\!}_{x_0,t}, {S\!}_{x_1,t} \rangle| \lesssim R^d ( R d_M(x_0,x_1) )^{-K} \quad\text{for all $K \geq 0$}.
\end{equation}
%
Thus
%
\begin{equation}
    \| F_{k,\mu,t} \|_{L^2(M)}^2 \lesssim R^{d} 2^{k(d-1)} \# (\mathcal{E}_k \cap (M \times \{ t \})).
\end{equation}
%
But this means that
%
\begin{equation} \label{eoqiejoiwjdoiaevjoa}
    A^a \sum\nolimits_{t \in \mathcal{T}_R \cap I_{k,\mu}} \| F_{k,\mu,t} \|_{L^2(M)}^2 \lesssim R^{d} 2^{k(d-1)} A^a \# \mathcal{E}_{k,\mu}.
\end{equation}
%
Thus \eqref{DAOWDHAODWWID}, \eqref{IOJDAOIWDJAWOIJF}, and \eqref{eoqiejoiwjdoiaevjoa} imply that
%
\begin{equation}
\begin{split}
    \| F_k \|_{L^2(M)}^2 &\lesssim \sum\nolimits_\mu \| F_{k,\mu} \|_{L^2(M)}^2 + R^{d} \left( 1 + 2^{k(d-1)} A^{1 - a \left( \frac{d-3}{2} \right)} \right) \# \mathcal{E}_k\\
    &\lesssim R^{d} \left( 2^{k(d-1)} A^a + (1 + 2^{k(d-1)} A^{1 - a \left( \frac{d-3}{2} \right)} \right) \# \mathcal{E}_k.
\end{split}
\end{equation}
% u^a = u^{1 - a(d-3)/2}
% a ( (d-1)/2 ) = 1
%
Optimizing by picking $a = 2 / (d-1)$ gives that
%
\begin{equation} \label{OICJOAIEVJAIOJFAOIJRIO}
    \| F_k \|_{L^2(M)}^2 \lesssim R^{d} 2^{k(d-1)} A^{\frac{2}{d-1}} \# \mathcal{E}_k.
\end{equation}
%
The proof is completed by combining \eqref{DPOWADPAWKDPOWAKDOPWAK} with \eqref{OICJOAIEVJAIOJFAOIJRIO}.
\end{proof}






\subsection{\boldmath $L^p$ Estimates Via Density Decompositions} \label{regime1densitydecomposition}

Combining the $L^2$ analysis of Section \ref{regime1firstsection} with a density decomposition argument, we can now prove the following Lemma, which completes the analysis of the operator $T^I$ in Proposition \ref{TjbLemma}.

\begin{lemma} \label{regime1Lemma}
    Using the notation of Proposition \ref{TjbLemma}, let $T^I = \sum\nolimits_{t_0 \in \mathcal{T}_R} T^I_{t_0}$, where
    %
    \begin{equation}
        T^I_{t_0} = b_{t_0}^I(t) (Q_R \circ e^{2 \pi i t P} \circ Q_R)\; dt.
    \end{equation}
    %
    Then for $1 \leq p < 2 (d-1) / (d+1)$,
    %
    \begin{equation}
        \| T^I u \|_{L^p(M)} \lesssim R^{-1/p'} \left( \sum\nolimits_{t_0 \in \mathcal{T}_R} \Big[ \| b^I_{t_0} \|_{L^p(I_0)} \langle R t_0 \rangle^{\alpha(p)} \Big]^p \right)^{1/p} \| u \|_{L^p(M)}.
    \end{equation}
\end{lemma}

We prove Lemma \ref{regime1Lemma} via a \emph{density decomposition} argument, adapted from the methods of \cite{HeoandNazarovandSeeger}. Given a function $u: M \to \CC$, we use a partition of unity to write
%
\begin{equation}
    u = \sum\nolimits_{x_0 \in \mathcal{X}_R} u_{x_0},
\end{equation}
%
where $u_{x_0}$ is supported on $B(x_0,1/R)$, and
%
\begin{equation}
\begin{split}
    \left( \sum\nolimits_{x_0 \in \mathcal{X}_R} \| u_{x_0} \|_{L^1(M)}^p \right)^{1/p} &\lesssim R^{-d/p'} \left( \sum\nolimits_{x_0 \in \mathcal{X}_R} \| u_{x_0} \|_{L^p(M)}^p \right)^{1/p} \lesssim R^{-d/p'} \| u \|_{L^p(M)}.
\end{split}
\end{equation}
% L^p is H^p R^{-1}
% L^1 is H^p R^{-p}
Define
%
\begin{equation}
    \mathcal{X}_{a} = \{ x_0 \in \mathcal{X}_R: 2^{a-1} < \| u_{x_0} \|_{L^1(M)} \leq 2^a \}
\end{equation}
%
and let
%
\begin{equation}
    \mathcal{T}_{b} = \{ t_0 \in \mathcal{T}_R: 2^{b-1} < \| b_{t_0}^I \|_{L^1(M)} \leq 2^b \}.
\end{equation}
%
Define functions $f_{x_0,t_0} = T_{t_0}^I u_{x_0}$. Lemma \ref{regime1Lemma} follows from the following result.

\begin{lemma} \label{LpBoundLemma}
    Fix $u \in L^p(M)$, and consider $\mathcal{X}_{a}$, $\mathcal{T}_{b}$, and $\{ f_{x_0,t_0} \}$ as above. For any function $c: \mathcal{X}_R \times \mathcal{T}_R \to \CC$, and $1 < p < 2 (d-1) / (d+1)$,
    %
    \begin{equation}
    \begin{split}
    &\Bigg\| \sum\nolimits_{a,b} \sum\nolimits_{(x_0,t_0) \in \mathcal{X}_{a} \times \mathcal{T}_{b}} 2^{-(a+b)} \langle R t_0 \rangle^{\frac{d-1}{2}} c(x_0,t_0) f_{x_0,t_0} \Big\|_{L^p(M)}\\
    &\quad\quad\quad\quad \lesssim R^{ d / p'} \left( \sum\nolimits_{a,b} \sum\nolimits_{(x_0,t_0) \in \mathcal{X}_{a} \times \mathcal{T}_{b}} |c(x_0,t_0)|^p \langle R t_0 \rangle^{d-1} \right)^{1/p}.
    \end{split}
    \end{equation}
\end{lemma}

To see how Lemma \ref{LpBoundLemma} implies Lemma \ref{regime1Lemma}, set $c(x_0,t_0) = 2^{a+b} \langle R t_0 \rangle^{- \frac{d-1}{2}}$ for $x_0 \in \mathcal{X}_{a}$ and $t_0 \in \mathcal{T}_{b}$. Then Lemma \ref{LpBoundLemma} implies that
%
\begin{equation}
\begin{split}
    &\| T^I u \|_{L^p(M)}\\
    &\quad = \left\| \sum f_{x_0,t_0} \right\|_{L^p(M)}\\
    &\quad \lesssim R^{d/p'} \left( \sum\nolimits_{(x_0,t_0)} \left[ \| b_{t_0}^I \|_{L^1(\RR)} \| u_{x_0} \|_{L^1(M)} \langle R t_0 \rangle^{\alpha(p)} \right]^p \right)^{1/p}\\
    &\quad \lesssim R^{-1/p'} \left( \sum\nolimits_{t_0} \Big[ \| b_{t_0}^I \|_{L^p(I_0)} \langle R t_0 \rangle^{\alpha(p)} \Big]^p \right)^{1/p}\left( \sum\nolimits_{x_0} \left[ \| u_{x_0} \|_{L^1(M)}  R^{d/p'} \right]^p \right)^{1/p}\\
    &\quad \lesssim R^{-1/p'} \left( \sum\nolimits_{t_0} \Big[ \| b_{t_0}^I \|_{L^p(I_0)} \langle R t_0 \rangle^{\alpha(p)} \Big]^p \right)^{1/p} \| u \|_{L^p(M)}.
\end{split}
\end{equation}
% C = alpha(p) + d/p'
%   = (d+1)/2 - 1/p
%
Thus we have proved Lemma \ref{regime1Lemma}. We take the remainder of this section to prove Lemma \ref{LpBoundLemma} using a density decomposition argument.

\begin{proof}[Proof of Lemma \ref{LpBoundLemma}]

For $p = 1$, this inequality follows simply by applying the triangle inequality, and applying the pointwise estimates of Proposition \ref{theMainEstimatesForWave}. By methods of interpolation, to prove the result for $p > 1$, we thus only need only prove a restricted strong type version of this inequality. In other words, we can restrict $c$ to be the indicator function of a set $\mathcal{E} \subset \mathcal{X}_R \times \mathcal{T}_R$. Write $\mathcal{E} = \bigcup_{k \geq 0} \mathcal{E}_{k,a,b}$, where
%
\begin{equation}
    \mathcal{E}_{0,a,b} = \{ (x,t) \in \mathcal{E} \cap (\mathcal{X}_{a} \times \mathcal{T}_{b}) : |t| \leq 1/R \}
\end{equation}
%
and for $k > 0$, let
%
\begin{equation}
    \mathcal{E}_{k,a,b} = \{ (x,t) \in \mathcal{E} \cap (\mathcal{X}_{a} \times \mathcal{T}_{b}) : 2^{k-1} / R < |t| \leq 2^{k} / R \}.
\end{equation}
%
Write
%
\begin{equation}
    F_k = \sum\nolimits_{a,b} \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_{k,a,b}} 2^{k \left( \frac{d-1}{2} \right)} 2^{-(a+b)} f_{x_0,t_0}.
\end{equation}
%
Our proof will be completed if we can show that
%
\begin{equation} \label{oDOIAWJCVOIEJOIJER1312s}
    \Big\| \sum\nolimits_k F_k \Big\|_{L^p(M)} \lesssim R^{d ( 1 - 1/p )} \Big( \sum\nolimits_k 2^{k(d-1)} \# \mathcal{E}_k \Big)^{1/p}.
\end{equation}
%
To prove \eqref{oDOIAWJCVOIEJOIJER1312s}, we perform a density decomposition on the sets $\{ \mathcal{E}_k \}$. For $u \geq 0$, let $\widehat{\mathcal{E}}_k(u)$ be the set of all points $(x_0,t_0) \in \mathcal{E}_k$ that are contained in a ball $B$ with $\text{rad}(B) \leq 2^{k} / 100 R$, such that $\#( \mathcal{E}_k \cap B ) \geq R 2^{u} \text{rad}(B)$. Then define
%
\begin{equation}
    \mathcal{E}_k(u) = \widehat{\mathcal{E}}_k(u) - \bigcup\nolimits_{u' > u} \widehat{\mathcal{E}}_k(u').
\end{equation}
%
Because the set $\mathcal{E}_k$ is $1/R$ discretized, we have
%
\begin{equation}
    \mathcal{E}_k = \bigcup\nolimits_{u \geq 0} \mathcal{E}_k(u).
\end{equation}
%
Moreover $\mathcal{E}_k(u)$ has density type $(R 2^{u}, 2^{k} / 100 R)$, and thus by a covering argument, also has density type $(C_d R 2^{u}, 2^{k} / R)$ for $C_d = 1000^d$. Furthermore, there are disjoint balls $B_{k,u,1},\dots,B_{k,u,N_{k,u}}$ of radius at most $2^{k} / 100 R$ such that
%
\begin{equation}
    \sum\nolimits_n \text{rad}(B_{k,u,n}) \leq 2^{-u} / R \# \mathcal{E}_k.
\end{equation}
%
and such that $\mathcal{E}_k(u)$ is covered by the balls $\{ B_{k,u,n}^* \}$, where, for a ball $B$, $B^*$ denotes the ball with the same center as $B$, but 5 times the radius. Now write
%
\begin{equation}
    F_{k,u} = \sum\nolimits_{a,b} \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_{k,a,b}(u)} 2^{k \left( \frac{d-1}{2} \right)} 2^{-(a+b)} f_{x_0,t_0}.
\end{equation}
%
Using the density assumption on $\mathcal{E}_k(u)$, we can apply Lemma \ref{L2DensityProposition} of the last section, which implies that, with ${S\!}_{x_0,t_0} = 2^{-(a+b)} f_{x_0,t_0}$ for $(x_0,t_0) \in \mathcal{E}_{k,a,b}(u)$,
%
\begin{equation} \label{DOIWAJOIAJVOIWAJFOIWF}
\begin{split}
    \Big\| \sum\nolimits_k F_{k,u} \Big\|_{L^2(M)} \lesssim R^{d/2} \left( u^{1/2} 2^{u \left( \frac{1}{d-1} \right)} \right) \left( \sum\nolimits_k 2^{k(d-1)} \# \mathcal{E}_k \right)^{1/2}.
\end{split}
\end{equation}
%
Let $(y_{k,u,n}, t_{k,u,n})$ denote the center of $B_{k,u,n}$. Then
%
\begin{equation}
    \sum\nolimits_{(x_0,t_0) \in [B_{k,u,n} \cap \mathcal{E}_k(u)]} 2^{-(a+b)} f_{x_0,t_0}
\end{equation}
%
has mass concentrated on the geodesic annulus $\text{Ann}_{k,u,n} \subset M$ with center $y_{k,u,n}$, with radius $t_{k,u,n} \sim 2^{k} / R$, and with thickness $5\; \text{rad}(B_{k,u,n})$. Thus
%
\begin{equation}
    \sum\nolimits_n |\text{Ann}_{k,u,n}| \lesssim \sum\nolimits_{n} (2^{k} / R)^{d-1} \text{rad}(B_{k,u,n}) \leq (2^{k}/R)^{d-1} R^{-1} 2^{-u} \# \mathcal{E}_k.
\end{equation}
%
If we set $\Lambda_u = \bigcup_k \bigcup_n \text{Ann}_{k,u,n}$, then
%
\begin{equation}
    |\Lambda_u| \lesssim R^{-d} 2^{-u} \sum\nolimits_k 2^{k(d-1)} \# \mathcal{E}_k
\end{equation}
%
Since $1/p - 1/2 > 1/(d-1)$, so that $\alpha(p) > 1$, H\"{o}lder's inequality implies that
%
\begin{equation}
\begin{split}
    \Big\| \sum\nolimits_k F_{k,u} \Big\|_{L^p(\Lambda_u)} &\lesssim |\Lambda_u|^{1/p - 1/2} \Big\| \sum\nolimits_k F_{k,u} \Big\|_{L^2(\Lambda_{k,u})}\\
    &\lesssim \left( R^{-d} 2^{-u} \sum\nolimits_k 2^{k(d-1)} \# \mathcal{E}_k \right)^{1/p - 1/2}\\
    &\quad\quad\quad\quad \left( R^{d} \left( u 2^{u \left( \frac{2}{d-1} \right)} \right) \sum\nolimits_k 2^{k(d-1)} \# \mathcal{E}_k \right)^{1/2}\\
    &= R^{d(1-1/p)} \left( u^{1/2} 2^{-u \left( \frac{\alpha(p) - 1}{d - 1} \right)}  \right) \left( \sum\nolimits_k 2^{k(d-1)} \# \mathcal{E}_k \right)^{1/p}\\
    &\lesssim R^{d(1 - 1/p)} 2^{-u \varepsilon} \left( \sum\nolimits_k 2^{k(d-1)} \# \mathcal{E}_k \right)^{1/p}
\end{split}
\end{equation}
%
for some suitable small $\varepsilon > 0$. For each $(x_0,t_0) \in \mathcal{E}_{k,a,b}(u) \cap B_{k,u,n}$, we calculate using the pointwise bounds for the functions $\{ f_{x_0,t_0} \}$ that
%
%\begin{align*}
%        \| 2^{-(l+r)} f_{x_0,t_0} \|_{L^q(\text{Ann}_n^c)}^q &= 2^{jdq} \int_{\text{Ann}_j^c} \langle 2^j d_M(x,x_0) \rangle^{- q \left( \frac{d-1}{2} \right)} \langle 2^j |t_0 - d_M(x,x_0)| \rangle^{-qM}\; dx \\
%        &\lesssim 2^{jq \left( \frac{d+1}{2} - M \right)} \int_{5\; \text{rad}(B_n)}^{O(1)} (2^{k-j} + s)^{(d-1) - q \left( \frac{d-1}{2} \right)} s^{-qM}\; ds\\
%    &\lesssim 2^{k(d-1)(1-q/2)} 2^{j[ q \left( d - M \right) - (d-1) ]} \text{rad}(B_n)^{1 - qM}.
%\end{align*}
\begin{equation}
\begin{split}
        \| 2^{-(a+b)} f_{x_0,t_0} \|_{L^1(\Lambda(u)^c)} &= R^{d} \int_{\text{Ann}_R^c} \langle R d_M(x,x_0) \rangle^{- \left( \frac{d-1}{2} \right)} \langle R |t_0 - d_M(x,x_0)| \rangle^{-M}\; dx\\
        &\lesssim R^{ \left( \frac{d+1}{2} - M \right)} \int_{5\; \text{rad}(B_{k,u,n})}^{O(1)} ( t_{k,u,n} + s)^{\left( \frac{d-1}{2} \right)} s^{-M}\; ds\\
        &\lesssim R^{ \left( \frac{d+1}{2} - M \right)} \text{rad}(B_{k,u,n})^{1-M} t_{k,u,n}^{\left( \frac{d-1}{2} \right)}\\
        &\lesssim 2^{k \left( \frac{d-1}{2} \right)} (R \text{rad}(B_{k,u,n}))^{1 - M}.
\end{split}
\end{equation}
%
Thus
%
%\[ \| 2^{k \left( \frac{d-1}{2} \right)} 2^{-(l+r)} f_{x_0,t_0} \|_{L^q(\text{Ann}_n^c)} \lesssim 2^{k \left( \frac{d-1}{q} \right) } 2^{jd/q'} (2^j \text{rad}(B_n))^{1/q - M} \]\
\begin{equation}
    \| 2^{k \left( \frac{d-1}{2} \right)} 2^{-(a+b)} f_{x_0,t_0} \|_{L^1(\text{Ann}_n^c)} \lesssim 2^{k(d-1)} (R \text{rad}(B_{k,u,n}))^{1 - M}
\end{equation}
%
% 2^{(k-j)[-p(d-1)/2]} rad(B_n)^{d-pM}
%      >> 2^{(k-j)( -p(d-1)/2 + d - pM )}
%
% 2^{(k-j)( d - p(d-1)/2 - pM )}
% 
%
Because the set of points in $\mathcal{E}_k$ is $1/R$ separated, there are at most $O( (R \text{rad}(B_{k,u,n}))^{d+1} )$ points in $\mathcal{E}_k(u) \cap B_{k,u,n}$, and so the triangle inequality implies that
% \mathcal{E}_{k,l,r} - \widehat{\mathcal{E}}_{k,l,r} = \bigcup_{l,r} 
%\begin{align*}
%    &\left\| \sum\nolimits_{l,r} \sum\nolimits_{(x_0,t_0) \in (\mathcal{E}_{k,l,r} - \widehat{\mathcal{E}}_{k,l,r}) \cap B_n} 2^{k \left( \frac{d-1}{2} \right)} 2^{-(l+r)} f_{x_0,t_0} \right\|_{L^q(\text{Ann}_n^c)}\\
%    &\quad\quad \lesssim 2^{k \left( \frac{d-1}{q} \right)} 2^{jd/q'} (2^j \text{rad}(B_n))^{d + 1 + 1/q - M}
%\end{align*}
\begin{equation}
\begin{split}
    &\Big\| \sum\nolimits_{a,b} \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_{k,a,b}(u) \cap B_{k,u,n}} 2^{k \left( \frac{d-1}{2} \right)} 2^{-(a+b)} f_{x_0,t_0} \Big\|_{L^1(\Lambda(u)^c)}\\
    &\quad\quad\quad \lesssim 2^{k (d - 1)} (R \text{rad}(B_{k,u,n}))^{d + 2 - M}.
\end{split}
\end{equation}
%
Since $\# \mathcal{E}_k \cap B_{k,u,n} \geq R 2^{u}\ \text{rad}(B_{k,u,n})$, and $\mathcal{E}_k$ is $1/R$ discretized, we must have
%
\begin{equation}
    \text{rad}(B_{k,u,n}) \geq (2^u / 2^d)^{\frac{1}{d-1}}\; 1/R.
\end{equation}
%
Thus
%
% 2^{jd(p-1)}
%
\begin{equation}
\begin{split}
    \Big\| \sum\nolimits_k F_{k,u} \Big\|_{L^1(\Lambda(u)^c)} &\lesssim_M \sum\nolimits_k \sum\nolimits_n 2^{k (d-1)} (R \text{rad}(B_{k,u,n}))^{d + 2 - M}\\
    &\lesssim \sum\nolimits_k 2^{k(d-1)} \Big( R \min\nolimits_n \text{rad}(B_{k,u,n}) \Big)^{d + 1 - M}\\
    &\quad\quad\quad\quad \left( \sum\nolimits_n R \text{rad}(B_{k,u,n}) \right) \\
    &\lesssim \sum\nolimits_k 2^{k (d-1)} 2^{u \left( \frac{d+1-M}{d-1} \right)} \left( 2^{-u} \# \mathcal{E}_k \right)\\
    &\lesssim 2^{u \left( \frac{2-M}{d-1} \right)} \sum\nolimits_k 2^{k (d-1)} \# \mathcal{E}_k
\end{split}
\end{equation}
%
Picking $M > 2 + (1 - 1/p)(1/p - 1/2)^{-1}$, and interpolating with the bounds on $\| \sum_k F_{k,u} \|_{L^2(M)}$ yields that
%
\begin{equation}
\begin{split}
    \Big\| \sum\nolimits_k F_{k,u} \Big\|_{L^p(\Lambda(u)^c)} &\lesssim \left( 2^{u \left( \frac{2-M}{d-1} \right)} \right)^{2/p - 1} \left( R^{d/2} \left( u^{1/2} 2^{u \left( \frac{1}{d-1} \right)} \right) \right)^{2(1 - 1/p)}\\
    &\quad\quad\quad\quad \left( \sum 2^{k(d-1)} \# \mathcal{E}_k \right)^{1/p} \\
    &\lesssim 2^{-u \varepsilon} R^{d(1 - 1/p)} \sum\nolimits_k \left( \sum 2^{k(d-1)} \# \mathcal{E}_k \right)^{1/p}.
\end{split}
\end{equation}
%
So now we know
%
\begin{equation}
    \Big\| \sum\nolimits_k F_{k,u} \Big\|_{L^p(M)} \lesssim R^{d(1-1/p)} 2^{- u \varepsilon} \left( \sum\nolimits_k 2^{k(d-1)} \# \mathcal{E}_k \right)^{1/p}.
\end{equation}
%
The exponential decay in $u$ allows us to sum in $u$ to obtain that
%
\begin{equation}
    \Big\| \sum\nolimits_u \sum\nolimits_k F_{k,u} \Big\|_{L^p(M)} \lesssim R^{d(1 - 1/p)} \left( \sum\nolimits_k 2^{k(d-1)} \# \mathcal{E}_k \right)^{1/p}.
\end{equation}
%
This is precisely the bound we were required to prove.
\end{proof}



\begin{comment}



which, within the range $p < 2(d-1)/(d+1)$ we are considering, and since we are assuming $\lambda \gtrsim_d 2^{jd}$, satisfies
%
\begin{align*}
        \sum_n |\text{Ann}_n| &\lesssim \log(\lambda/2^{jd})^{(2-p) \left( \frac{d-1}{2} \right)} (2^{k-j})^{d-1} 2^{-j} \left( 2^{jd} / \lambda \right)^{(2-p) \left( \frac{d-1}{2} \right)} \# \mathcal{E}_k\\
        &= \log(\lambda/2^{jd})^{(2-p) \left( \frac{d-1}{2} \right)} (\lambda/2^{jd})^{p \left( \frac{d+1}{2} \right) - (d-1)} 2^{jd(p-1)} \Big( 2^{k(d-1)} \# \mathcal{E}_k \Big) \lambda^{-p}\\
        &\lesssim 2^{jd(p-1)} \Big( 2^{k(d-1)} \# \mathcal{E}_k \Big) \lambda^{-p}.
\end{align*}
%
The last inequality uses the fact that for any $\varepsilon, \delta > 0$, and $x \geq 10^{d+1}$,
%
\[ \log(x)^\delta x^{-\varepsilon} \lesssim_{\delta,\varepsilon} 1. \]


% 2^{k-j} in diameter
% So contained in a ball with radius 2^{k-j}
%   Place balls of radius 1/20 in this ball. Then the union of these balls is
%   contained in a ball of radius 21/20. Therefore there are at most 21^d
%   such balls
% On each of these balls, << 2^{j+u} rad(B) / 20


% p / p' = p (1 - 1/p) = p - 1
% When p = 1 the inequality says
% Sum_k 2^{k (d-1)/2} |S_{x_0,t_0}|_{L^1} << R^{-1} sum_k 2^{k(d-1)} #(E_k)
%
% d_g(x,x_0) ~ 2^k / R
%
% Has height R^{d-1} 2^{-k (d-1)/2}
%
% on an annulus of thickness 1/R, and radius 2^k/R
%
% So has L^1 norm R^{-1} 2^{k(d-1)/2}
%
% SO WE DO GET THE p = 1 INEQUALITY!
%
% So for weak type estimate, by interpolation,
% we only have to consider large lambda superlevel sets
%
% LOW DENSITY PART: DEALT WITH USING L2 ESTIMATE
% HIGH DENSITY PART: Since lambda is large,
%           can use essential support of function.
%



This is equivalent to showing that for any $\lambda > 0$,
%
\begin{align} \label{FSumLPInfBound}
\begin{split}
    &\bigg| \Big\{ x: \Big| \sum\nolimits_{a,b}  \sum\nolimits_k \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_{k,a,b}} 2^{k \left( \frac{d-1}{2} \right)} 2^{-(a+b)} f_{x_0,t_0}(x) \Big| \geq \lambda \Big\} \bigg|\\
    &\quad\quad\quad \lesssim  2^{j d (p - 1)} \left( \sum_k 2^{k(d-1)} \# \mathcal{E}_k \right) \lambda^{-p}.
\end{split}
\end{align}
%
Applying Markov's inequality, as well as the $p = 1$ inequality, we find that
%
\[ \bigg| \Big\{ x: \Big|\sum\nolimits_{a,b} \sum\nolimits_{k} \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_{k,a,b}} 2^{k \left( \frac{d-1}{2} \right)} 2^{-(a+b)} f_{x_0,t_0}(x) \Big| \geq \lambda \Big\} \bigg| \lesssim \left( \sum_k 2^{k(d-1)} \# \mathcal{E}_k \right) \lambda^{-1} \]
%
We have
%
\[ \left( \sum_k 2^{k(d-1)} \# \mathcal{E}_k \right) \lambda^{-1} \lesssim 2^{jd(p-1)} \left( \sum_k 2^{k(d-1)} \# \mathcal{E}_k \right) \lambda^{-p} \]
% lambda^{p-1} << 2^{jd(p-1)}
for $\lambda \leq 10^{d+2} 2^{jd}$, so we may assume $\lambda \geq 10^{d+2} 2^{jd}$ in what follows. We then employ the method of density decompositions introduced in \cite{HeoandNazarovandSeeger}. Fix a quantity $A \geq 10^{d+1}$, to be chosen later. For each $k$, we consider the collection $\mathcal{B}_k$ of all balls $B$ with radius at most $2^{k-j} / 10$ such that $\# \mathcal{E}_k \cap B \geq (2^j A) \text{rad}(B)$. Applying the Vitali covering lemma, we can find a disjoint family of balls $\{ B_1, \dots, B_N \}$ in $\mathcal{B}_k$ such that the balls $\{ B_1^*, \dots, B_N^* \}$ obtained by dilating the balls by 5 cover $\bigcup \mathcal{B}_k(\lambda)$. Then
%
\[ \sum_n \text{rad}(B_n) \leq 2^{-j} A^{-1} \# \mathcal{E}_k. \]
%
Then the set $\widehat{\mathcal{E}}_k = \mathcal{E}_k - \bigcup \mathcal{B}_k(\lambda)$ has density type $(10^d 2^j A, 2^{k-j})$. Thus we can apply Lemma \ref{L2DensityProposition} of the last section, which implies that, with ${S\!}_{x_0,t_0} = 2^{-(a+b)} f_{x_0,t_0}$ for $(x_0,t_0) \in \mathcal{E}_{k,a,b}$,
%
\[ \Big\| \sum\nolimits_{a,b} \sum\nolimits_k \sum\nolimits_{(x_0,t_0) \in \widehat{\mathcal{E}}_{k,a,b}} 2^{k \left( \frac{d-1}{2} \right)} 2^{-(a+b)} f_{x_0,t_0} \Big\|_{L^2(M)}^2 \lesssim 2^{jd} \log(A) A^{\frac{2}{d-1}} \sum_k 2^{k(d-1)} \# \mathcal{E}_k. \]
%2^{j(d-2)} \log(A) A^{\frac{2}{d-1}} \sum_k 2^{k(d-1)} \# \mathcal{E}_k. \]
%
Applying Chebyshev's inequality, we conclude that
%
\begin{align*}
    &\bigg| \Big\{ x: \Big|\sum\nolimits_{a,b} \sum\nolimits_k \sum\nolimits_{(x_0,t_0) \in \widehat{\mathcal{E}}_k} 2^{k \frac{d-1}{2}} 2^{-(a+b)} f_{x_0,t_0}(x)\Big| \geq \lambda / 2 \Big\} \bigg|\\
    &\quad\quad\quad \lesssim 2^{jd} \log(A) A^{\frac{2}{d-1}} \Big( \sum_k 2^{k(d-1)} \# \mathcal{E}_k \Big) \lambda^{-2}.
\end{align*}
% 
% TODO: Bound A^s log(A) by some power of A for large A.
%
% 2^{jd} A^{2/(d-1)} / lambda^2 << 2^{jd(p-1)} / lambda^p
% A << (lambda / 2^{jd})^{(2-p)(d-1)/2}
%
% A^{2/(d-1) + log log A} << (L/2^{jd})^{2-p}
% If A <= (L/2^{jd})^{(2-p)(d-1)/2}
% then log log A <= C_{p,d} + log log (L / 2^{jd})
% A^{2/(d-1) + C_{p,d} + log log (L / 2^{jd})} <= (L / 2^{jd})^{2-p}
% (2/(d-1) + C_{p,d} + log log (L / 2^{jd})) log A <= (2-p) log (L / 2^{jd})
% log A <= (2-p) log (L / 2^{jd}) / (C + log log(L / 2^{jd}))
% A = (L / 2^{jd})^{(2-p) / (C + log log (L / 2^{jd})) }
% So if L = 2^{2^N} 2^{jd},
% A = 2^{(2^N - jd)(2-p)/(C + N)}
%
Choose
%
\[ A = \log(\lambda / 2^{jd})^{\left( \frac{d-1}{2} \right)} \left( \lambda / 2^{jd} \right)^{(2-p) \left( \frac{d-1}{2} \right)}, \]
%
so that
%
\[ (\log A)^{\left( \frac{d-1}{2} \right)} A \lesssim (\lambda / 2^{jd})^{(2-p) \left( \frac{d-1}{2} \right)}. \]
%
Then
%
\begin{align} \label{ChebyshevFirstBound}
\begin{split}
    &\bigg| \bigg\{ x: \bigg|\sum_{a,b} \sum_k \sum_{(x_0,t_0) \in \widehat{\mathcal{E}}_k} 2^{k \frac{d-1}{2}} 2^{-(a+b)} f_{x_0,t_0}(x)\bigg| \geq \lambda / 2 \bigg\} \bigg|\\
    &\quad\quad\quad \lesssim 2^{jd(p-1)} \left( \sum_k 2^{k(d-1)} \# \mathcal{E}_k \right) \lambda^{-p}.
\end{split}
\end{align}
%
\eqref{ChebyshevFirstBound} gives a good enough bound for $\widehat{\mathcal{E}}_k$. Conversely, we exploit the clustering of the sets $\mathcal{E}_k - \widehat{\mathcal{E}}_k$ to bound
%
\[ \bigg| \Big\{ x: \Big| \sum\nolimits_{a,b} \sum\nolimits_k \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_k - \widehat{\mathcal{E}}_k} 2^{k \frac{d-1}{2}} 2^{-(a+b)} f_{x_0,t_0}(x) \Big| \geq \lambda / 2 \Big\} \bigg| \]
%
We have found balls $B_1^*, \dots, B_N^*$, each with radius at most $2^{k-j} / 10$, such that
%
\[ \sum \text{rad}(B_n) \leq 2^{-j} A^{-1} \# \mathcal{E}_k. \]
%
Let $(y_n,r_n)$ denote the center of the ball $B_n$. Then the function
%
\[ \sum\nolimits_{(x_0,t_0) \in B_n \cap (\mathcal{E}_k - \widehat{\mathcal{E}}_k)} 2^{-(a+b)} f_{x_0,t_0} \]
%
has mass concentrated on the geodesic annulus $\text{Ann}_n \subset M$ centred at $y_n$, with radius $r_n \sim 2^{k-j}$ and thickness $5\; \text{rad}(B_n)$. But we then calculate that
%
\[ \sum_n |\text{Ann}_n| \lesssim \sum_n (2^{k-j})^{d-1} \text{rad}(B_n) \leq (2^{k-j})^{d-1} 2^{-j} A^{-1} \# \mathcal{E}_k \]
%
which, within the range $p < 2(d-1)/(d+1)$ we are considering, and since we are assuming $\lambda \gtrsim_d 2^{jd}$, satisfies
%
\begin{align*}
        \sum_n |\text{Ann}_n| &\lesssim \log(\lambda/2^{jd})^{(2-p) \left( \frac{d-1}{2} \right)} (2^{k-j})^{d-1} 2^{-j} \left( 2^{jd} / \lambda \right)^{(2-p) \left( \frac{d-1}{2} \right)} \# \mathcal{E}_k\\
        &= \log(\lambda/2^{jd})^{(2-p) \left( \frac{d-1}{2} \right)} (\lambda/2^{jd})^{p \left( \frac{d+1}{2} \right) - (d-1)} 2^{jd(p-1)} \Big( 2^{k(d-1)} \# \mathcal{E}_k \Big) \lambda^{-p}\\
        &\lesssim 2^{jd(p-1)} \Big( 2^{k(d-1)} \# \mathcal{E}_k \Big) \lambda^{-p}.
\end{align*}
%
The last inequality uses the fact that for any $\varepsilon, \delta > 0$, and $x \geq 10^{d+1}$,
%
\[ \log(x)^\delta x^{-\varepsilon} \lesssim_{\delta,\varepsilon} 1. \]
% p < 2(d-1)/(d+1)
% p - (2 - p)(d-1)/2 = 2(d-1)/(d+1) - (2(d-1)/(d+1))
%
%\begin{align*}
%     2^{-jd} & \left( \frac{\lambda^{2-p}}{2^{j(d+1)(1 - p/2)}} \right)^{- \frac{d-1}{2}} ( 2^{k(d-1)} \# \mathcal{E}_k )\\
%     &\quad\quad = 2^{j \left[ \left( \frac{d-1}{2} \right) (d+1)(1 - p/2) - d \right]} \lambda^{p - (2-p) \left( \frac{d-1}{2} \right)} ( 2^{k(d-1)} \# \mathcal{E}_k ) \lambda^{-p}\\
%     &\quad\quad = 2^{j \left[ \left( \frac{d-1}{2} \right) (d+1)(1 - p/2) - d \right]} \lambda^{p \left( \frac{d+1}{2} \right) - (d-1)} ( 2^{k(d-1)} \# \mathcal{E}_k ) \lambda^{-p}\\
%     &\quad\quad \leq 2^{j \left[ \left( \frac{d-1}{2} \right) (d+1)(1 - p/2) - d + \left\{ p \left( \frac{d+1}{2} \right) - (d-1) \right\} \left( \frac{d+1}{2} \right) \right]}\\
%     &\quad\quad = 2^{j \left[ - d + (p/4) [ d^2 + d + 2 ] \right]} ( 2^{k(d-1)} \# \mathcal{E}_k ) \lambda^{-p}\\
%     &\quad\quad = 2^{j \left[ p \left( \frac{d+1}{2} \right) - 1 \right]} 2^{j(d-1)[ pd/4 - 1]}
%\end{align*}
% - d
%
%\begin{align*}
%    (2^{k-j})^{d-1} 2^{-j} &  \left( \frac{\lambda^{2-p}}{2^{j \left[ (d-1) - p \left( \frac{d+1}{2} \right) \right]}} \right)^{- \frac{d-1}{2}} \# \mathcal{E}_k\\
%    &= 2^{j \left[ \frac{d^2 - 4d + 1}{2} - p  \left( \frac{d^2 - 1}{4} \right) \right]}\; \lambda^{p \left( \frac{d+1}{2} \right) - (d-1)}  \; 2^{k(d-1)} \# \mathcal{E}_k\; \lambda^{-p}\\
%    &\lesssim 2^{j \left[ \frac{d^2 - 4d + 1}{2} - p  \left( \frac{d^2 - 1}{4} \right) \right]} 2^{j \left[ p \left( \frac{d^2+2d + 1}{4} \right) - \left( \frac{d^2 - 1}{2} \right) \right]}  \; \left( 2^{k(d-1)} \# \mathcal{E}_k \right)\; \lambda^{-p}\\
%    &= 2^{j \left[ p \left( \frac{d + 1}{2} \right) - (2d - 1) \right]}   \; \left( 2^{k(d-1)} \# \mathcal{E}_k \right)\; \lambda^{-p}\\
%    &\lesssim 2^{j \left[ p \left( \frac{d+1}{2} \right) - 1 \right]}   \; \left( 2^{k(d-1)} \# \mathcal{E}_k \right)\; \lambda^{-p}
%\end{align*}
%
%\textcolor{red}{TODO: LAST INEQUALITY SEEMS SUSPICIOUSLY LOOSE.}
%
Defining $\text{Bad}_k = \bigcup_n \text{Ann}_n$, and $\text{Bad} = \bigcup_k \text{Bad}_k$, we have
%
\begin{align} \label{BadSet}
    |\text{Bad}| \lesssim 2^{jd (p-1)} \sum\nolimits_k 2^{k(d-1)} \# \mathcal{E}_k\; \lambda^{-p}.
\end{align}
%
This is an exceptional set with size appropriate for the proof of the $L^{p,\infty}$ bound. All that remains is to account for the negligible error terms associated with the functions $f_{x_0,t_0}$, i.e. their behaviour on $\text{Bad}^c$. For each $(x_0,t_0) \in (\mathcal{E}_{k,a,b} - \widehat{\mathcal{E}}_{k,a,b}) \cap B_n$, we calculate using the pointwise bounds for the functions $\{ f_{x_0,t_0} \}$ that
%
%\begin{align*}
%        \| 2^{-(l+r)} f_{x_0,t_0} \|_{L^q(\text{Ann}_n^c)}^q &= 2^{jdq} \int_{\text{Ann}_j^c} \langle 2^j d_g(x,x_0) \rangle^{- q \left( \frac{d-1}{2} \right)} \langle 2^j |t_0 - d_g(x,x_0)| \rangle^{-qM}\; dx \\
%        &\lesssim 2^{jq \left( \frac{d+1}{2} - M \right)} \int_{5\; \text{rad}(B_n)}^{O(1)} (2^{k-j} + s)^{(d-1) - q \left( \frac{d-1}{2} \right)} s^{-qM}\; ds\\
%    &\lesssim 2^{k(d-1)(1-q/2)} 2^{j[ q \left( d - M \right) - (d-1) ]} \text{rad}(B_n)^{1 - qM}.
%\end{align*}
\begin{align*}
        \| 2^{-(a+b)} f_{x_0,t_0} \|_{L^1(\text{Ann}_n^c)} &= 2^{jd} \int_{\text{Ann}_j^c} \langle 2^j d_g(x,x_0) \rangle^{- \left( \frac{d-1}{2} \right)} \langle 2^j |t_0 - d_g(x,x_0)| \rangle^{-M}\; dx \\
        &\lesssim 2^{j \left( \frac{d+1}{2} - M \right)} \int_{5\; \text{rad}(B_n)}^{O(1)} (2^{k-j} + s)^{\left( \frac{d-1}{2} \right)} s^{-M}\; ds\\
    &\lesssim 2^{k \left( \frac{d-1}{2} \right)} 2^{j(1 - M)} \text{rad}(B_n)^{1 - M}.
\end{align*}
%
Thus
%
%\[ \| 2^{k \left( \frac{d-1}{2} \right)} 2^{-(l+r)} f_{x_0,t_0} \|_{L^q(\text{Ann}_n^c)} \lesssim 2^{k \left( \frac{d-1}{q} \right) } 2^{jd/q'} (2^j \text{rad}(B_n))^{1/q - M} \]
\[ \| 2^{k \left( \frac{d-1}{2} \right)} 2^{-(a+b)} f_{x_0,t_0} \|_{L^1(\text{Ann}_n^c)} \lesssim 2^{k \left( d - 1 \right) } (2^j \text{rad}(B_n))^{1 - M} \]
%
% 2^{(k-j)[-p(d-1)/2]} rad(B_n)^{d-pM}
%      >> 2^{(k-j)( -p(d-1)/2 + d - pM )}
%
% 2^{(k-j)( d - p(d-1)/2 - pM )}
% 
%
Because the set of points in $\mathcal{E}_k$ is $2^{-j}$ separated, there are at most $O( (2^j \text{rad}(B_n))^{d+1} )$ points in $(\mathcal{E}_{k,a,b} - \widehat{\mathcal{E}}_{k,a,b}) \cap B_n$, and so the triangle inequality implies that
% \mathcal{E}_{k,l,r} - \widehat{\mathcal{E}}_{k,l,r} = \bigcup_{l,r} 
%\begin{align*}
%    &\left\| \sum\nolimits_{l,r} \sum\nolimits_{(x_0,t_0) \in (\mathcal{E}_{k,l,r} - \widehat{\mathcal{E}}_{k,l,r}) \cap B_n} 2^{k \left( \frac{d-1}{2} \right)} 2^{-(l+r)} f_{x_0,t_0} \right\|_{L^q(\text{Ann}_n^c)}\\
%    &\quad\quad \lesssim 2^{k \left( \frac{d-1}{q} \right)} 2^{jd/q'} (2^j \text{rad}(B_n))^{d + 1 + 1/q - M}
%\end{align*}
\begin{align*}
    &\Big\| \sum\nolimits_{a,b} \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_{k,a,b} - \widehat{\mathcal{E}}_{k,a,b}} 2^{k \left( \frac{d-1}{2} \right)} 2^{-(a+b)} f_{x_0,t_0} \Big\|_{L^1(\text{Ann}_n^c)} \lesssim 2^{k \left( d - 1 \right)} (2^j \text{rad}(B_n))^{d + 2 - M}.
\end{align*}
%
Since $\# \mathcal{E}_k \cap B_n \geq 2^j A\ \text{rad}(B_n)$, and because $\mathcal{E}_k$ is $2^{-j}$ discretized, we must have
%
\[ \text{rad}(B_n) \geq (A / 2^d)^{\frac{1}{d-1}}\; 2^{-j}. \]
%
%
%
% CALCULATIONS q = 1:
%
%   L^1 norm for fixed ball is 2^{k(d-1)} ( 2^j \text{rad}(B_n) )^{d + 2 - M}
%       Summing over, using the fact that sum 2^j rad(B_n) <= A^{-1} #(E_k)
%       and that 2^j rad(B_n) >> A^{1/(d-1)}
%   
%       2^{k(d-1)} A^{-1} (2^j rad(B_n))^{d + 1 - M} #(E_k)
%       2^{k(d-1)} A^{-(M - 2)/(d-1)} #(E_k)
%       A^{-(M-2)/(d-1)} 2^{k(d-1)} #(E_k)
%       (L/2^{jd)}^{-(2-p)(M-2)/2}) 2^{k(d-1)} #(E_k)
%           Choosing M Appropriately
%       L^{1-p} 2^{jd(p-1)} 2^{k(d-1)} #(E_k)
%
%       But this means the points above L have measure
%
%       2^{jd(p-1)} 2^{k(d-1)} #(E_k) L^{-p}
%
%       Consider x_n
%
%       With 2^{u eps} <= x_n <= 2^k
%
%       And sum x_n <= 2^{-u} A

Suppose we pick $M$ to be any integer larger than $3 + 2(p-1)/(2-p)$. Then we conclude that for $\lambda \geq 10^{d+1} 2^{jd}$, using the bound on $\sum \text{rad}(B_n)$ and the individual upper bounds on $\text{rad}(B_n)$, imply that
%
% 2^{jd(p-1)}
%
\begin{align*}
    &\Big\| \sum\nolimits_{a,b} \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_{k,a,b} - \widehat{\mathcal{E}}_{k,a,b}} 2^{k \left( \frac{d-1}{2} \right)} 2^{-(a+b)} f_{x_0,t_0} \Big\|_{L^1(\text{Bad}_k^c)}\\
    &\quad\quad\quad \lesssim_M \sum_n 2^{k (d-1)} (2^j \text{rad}(B_n))^{d + 2 - M}\\
    &\quad\quad\quad \lesssim 2^{k(d-1)} A^{- \frac{M - d - 2}{d - 1}} A^{-1} \# \mathcal{E}_k \\
    &\quad\quad\quad \lesssim 2^{k (d-1)} A^{- \frac{M-3}{d-1}} \# \mathcal{E}_k\\
    &\quad\quad\quad \lesssim 2^{k(d-1)} \left( \log(\lambda / 2^{jd})^{\left( \frac{d-1}{2} \right)} (\lambda / 2^{jd})^{(2-p) \left( \frac{d - 1}{2} \right)} \right)^{- \frac{M-3}{d-1}} \# \mathcal{E}_k\\
    &\quad\quad\quad \lesssim 2^{jd(p-1)} \Big( 2^{k(d-1)} \# \mathcal{E}_k \Big) \lambda^{1-p}.
%    &\quad\quad\quad \lesssim 2^{j \left[\alpha(p) + d/p' \right]} ( \lambda / 2^{jd} )^{- (2 - p) \left( \frac{d-1}{2} \right) \left( \frac{M-2}{d-1} \right)} \left( 2^{k \left( \frac{d - 1}{2} \right)} \# \mathcal{E}_k \right)\\
%    &\quad\quad\quad \lesssim 2^{jd/p'} \left( 2^{k \left( \frac{d - 1}{2} \right)} \# \mathcal{E}_k \right)
\end{align*}
%
The last inequality again used the fact that $\log(x)^\delta x^{-\varepsilon} \lesssim 1$ for any $\delta,\varepsilon > 0$ and $x \geq 10^{d+1}$.
% j(d-1)
%
%     \left( A^{-1} \# \mathcal{E}_k \right) (A / 2^d)^{\frac{d+1-M}{d-1}}\\
%    &\quad\quad\quad \lesssim_n 2^{k \left( \frac{3d - 3}{2} \right)} A^{\frac{2 - M}{d - 1}} \# \mathcal{E}_k\\
%    &\quad\quad\quad \lesssim_n 2^{k \left( \frac{3d - 3}{2} \right)} \left( \lambda / 2^{jd} \right)^{\frac{(2-p)(2 - M)}{2}} \# \mathcal{E}_k\\
%    &\quad\quad\quad \lesssim 2^{jd(p-1)} 2^{k(d-1)} \Big( 2^{k \left( \frac{d-1}{2} \right)} \# \mathcal{E}_k \Big) \lambda^{1-p}
%
But this means that
%
\begin{align*}
    &\Big\| \sum\nolimits_k \sum\nolimits_{a,b} \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_{k,a,b} - \widehat{\mathcal{E}}_{k,a,b}} 2^{k \left( \frac{d-1}{2} \right)} 2^{-(a+b)} f_{x_0,t_0} \Big\|_{L^1(\text{Bad}^c)}\\
    &\quad\quad\quad\quad \lesssim \Big( 2^{jd(p-1)} \sum_k 2^{k(d-1)} \# \mathcal{E}_k \Big) \lambda^{1-p}.
\end{align*}
%
Applying Markov's inequality, this is enough to justify that
%
\begin{align} \label{LastMarkovRemainderBound}
\begin{split}
    &\bigg| \Big\{ x \in \text{Bad}^c: \Big|\sum\nolimits_{a,b} \sum\nolimits_k \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_{k,a,b} - \widehat{\mathcal{E}}_{k,a,b}} 2^{-(a+b)} f_{x_0,t_0}(x)\Big| \geq \lambda / 2 \Big\} \bigg|\\
    &\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \lesssim 2^{jd(p-1)} \left( \sum_k 2^{k(d-1)} \# \mathcal{E}_k \right) \lambda^{-p}.
\end{split}
\end{align}
%
Putting \eqref{ChebyshevFirstBound}, \eqref{BadSet}, and \eqref{LastMarkovRemainderBound} together implies that for $\lambda \geq 2^{jd}$,
%
\begin{align*}
    &\bigg| \Big\{ x: \Big|\sum\nolimits_{a,b} \sum\nolimits_k \sum\nolimits_{(x_0,t_0) \in \mathcal{E}_{k,a,b}} 2^{-(a+b)} f_{x_0,t_0}(x)\Big| \geq \lambda \Big\} \bigg| \\
    &\quad\quad\quad \lesssim 2^{jd(p-1)} \left( \sum_k 2^{k(d-1)} \# \mathcal{E}_k \right) \lambda^{-p}
\end{align*}
%
This completes the proof of \eqref{FSumLPInfBound}, and thus the theorem.
\end{proof}

% The sum of S_{x_0,t_0}, where (x_0,t_0) range over some ball B_j
% has measure concentrated on a radius ~ 2^k / R annulus with thickness rad(B_j), a set with measure (2^k / R)^{d-1} rad(B_j).

%
% Picking A = (L / R^{d-1})^{ps_n}
% gives slightly too large a bound,
% by a factor log(L / R^{d-1})
%
% If we can replace ps_p by ps_p - epsilon
% we're probably good.
%
% Or even (L / R^{d-1})^{ps_p} log ( L / R^{d-1} )^{-O(1)}
%
% Then
%
% We're left with dealing with the concentrated case
% points are covered by balls B_1 ... B_N with radius at most 2^k / R
% and with sum rad(B_i) <= R^{-1} A^{-1} #E_k
%
% <= R^{-1} (L / R^{d-1})^{-(d-1)(1 - p/2)} log(L / R^{d-1})^{O(1)}
%
% The sum of S_{x_0,t_0}, where (x_0,t_0) range over some ball B_j
% has measure concentrated on a radius ~ 2^k / R annulus with thickness rad(B_j), a set with measure (2^k / R)^{d-1} rad(B_j).
%
% Summing over j, the total measure of these annulus are
%
% << (2^k / R)^{d-1} R^{-1} (L / R^{d-1} )^{-(d-1)(1 - p/2)} log(L / R^{d-1})^{O(1)}
% 2^{k(d-1)} R^{-d + (d-1)p} L^{-p} #(E_k)
%

% This is good provided that
% R^{d-2} L^{2p/(d-1) - 2} <= L^{-p} R^{(d-1)p - d}
% R^{(2-p)(d-1)} <= L^{2-p - 2p/(d-1)}
% R^{(2-p)(d-1)} <= L^{2 - p( (d+1)/(d-1) )}
% L >= R^{(d-1) [ (2-p)/(2 - p(d+1)/(d-1))]} = R^{(d-1) + eps_p}
% The right exponent exceeds d-1 in the range we are considering
% Shouldn't this be bad?

% s(lambda))^{2p/(d-1)} <= lambda^{2-p} R^{-(2-p)(d-1)}
% s(lambda) <= lambda^{ s_p } R^{ - (d-1)s_p } = ( lambda / R^{d-1} )^{s_p}
% p = 2 (d-1)/ (d+1)

% (2 - p)(d-1) / 2p > 1 in the range we are considering

% log_2(s(lambda)) s(lambda) <= lambda^{(2-p)(d-1)/2p} R^{[(d-1)^2(p-1) - d(d-1)]/2p}


% \log_2(\lambda) lambda^{(d+1)/(d-1)[p - 2(d-1)/(d+1)]} <= R^{2 + (d-1)p - 2d}
% if
% R^{(2-p)(d-1)} <= lambda^{2 - p(d+1)/(d-1)}
% which holds iff
% lambda >= R^{(2-p)(d-1)/[2 - p(d+1)/(d-1)]}
% L^{(2p)/(d-1) - 2 + p} <= R^{(d-1)p - d + 2 - d} 

% d > (p+2)/(2-p)
% 


\end{comment}








\section{Analysis of Regime II via Local Smoothing} \label{regime2finalsection}

In this section, we bound the operators $\{ T^{II} \}$, by a reduction to an endpoint local smoothing inequality, namely, the inequality that
%
\begin{equation} \label{thelocalsmoothinginequality}
    \| e^{2\pi i t P} f \|_{L^{p'}(M) L^{p'}_t(I_0)} \lesssim \| f \|_{L^{p'}_{\alpha(p) - 1/p'}}.
\end{equation}
%
This inequality is proved in Corollary 1.2 of \cite{LeeSeeger} for $1 < p < 2(d-1)/(d+1)$ for classical elliptic pseudodifferential operators $P$ satisfying the cosphere assumption of Theorem \ref{CpVersionOfTheorem}. The range of $p$ here is also precisely the range of $p$ in Theorem \ref{CpVersionOfTheorem}. Alternatively, Lemma \ref{LpBoundLemma} can be used to prove \eqref{thelocalsmoothinginequality} independently of \cite{LeeSeeger} in the same range by a generalization of the method of Section 10 of \cite{HeoandNazarovandSeeger}.

\begin{lemma} \label{LocalSmoothingLargeTimesTheorem}
    Using the notation of Proposition \ref{TjbLemma}, let
    %
    \begin{equation}
        T^{II} = \int b^{II}(t) (Q_R \circ e^{2 \pi i t P} \circ Q_R)\; dt.
    \end{equation}
    %
    For $1 < p < 2 (d-1)/(d+1)$, we then have
    %
    \begin{equation}
        \| T^{II} u \|_{L^p(M)} \lesssim R^{\alpha(p) - 1/p'} \| b^{II} \|_{L^p(I_0)} \| u \|_{L^p(M)}.
    \end{equation}
\end{lemma}
\begin{proof}
    For each $R$, the \emph{class} of operators of the form $\{ T^{II} \}$ formed from a given function $b^{II}$ is closed under taking adjoints. Indeed, if $T^{II}$ is obtained from $b^{II}$, then $(T^{II})^*$ is obtained from the multiplier $\overline{b^{II}}$. Because of this self-adjointness, if we can prove that
    %
    \begin{equation}
        \| T^{II} u \|_{L^{p'}(M)} \lesssim R^{\alpha(p) - 1/p'} \| b^{II} \|_{L^p(I_0)} \| u \|_{L^{p'}(M)},
    \end{equation}
    %
    then we obtain the required result by duality. We apply this duality because it is easier to exploit local smoothing inequalities in $L^{p'}(M)$ since now $p' > 2$.

    We begin by noting that the operators $\{ Q_R \}$, being a bounded family of order zero pseudo-differential operators, are uniformly bounded on $L^{p'}(M)$. Thus
    %
    \begin{equation}
    \begin{split}
        \| T^{II} u \|_{L^{p'}(M)} &= \Big\| Q_R \circ \Big( \int_{I_0} b^{II}(t) e^{2 \pi i tP} (Q_R u) \Big) \Big\|_{L^{p'}(M)}\\
        &\lesssim \Big\| \Big( \int_{I_0} b^{II}(t) e^{2 \pi i tP} (Q_R u) \Big) \Big\|_{L^{p'}(M)}.
    \end{split}
    \end{equation}
    %
    Applying H\"{o}lder and Minkowski's inequalities, we find that
    %
    \begin{equation}
    \begin{split}
        \| T^{II}u \|_{L^{p'}(M)} &\leq \| b^{II} \|_{L^p(\RR)} \Big\| \Big( \int_{I_0} |e^{2 \pi i t P} (Q_R u)|^{p'} \Big)^{1/p'} \Big\|_{L^{p'}(M)}.
    \end{split}
    \end{equation}
    %
    Applying the endpoint local smoothing inequality \eqref{thelocalsmoothinginequality}, we conclude that
    %
    \begin{equation}
    \begin{split}
        \| T^{II} u \|_{L^{p'}(M)} &\lesssim \| b^{II} \|_{L^p(\RR)}  \| e^{2 \pi i P} (Q_R u) \|_{L^{p'}_t L^{p'}_x}\\
        &\lesssim  \| b^{II} \|_{L^p(\RR)}  \| Q_R u \|_{L^q_{\alpha(p) - 1/p'}(M)},
    \end{split}
    \end{equation}
    %
    Bernstein's inequality for compact manifolds (see \cite{Sogge}, Section 3.3) gives
    %
    \begin{equation}
        \| Q_R u \|_{L^{q}_{\alpha(p) - 1/p'}(M)} \lesssim R^{\alpha(p) - 1/p'} \| u \|_{L^p(M)}.
    \end{equation}
    %
    Thus we conclude that
    %
    \begin{equation}
        \| T^{II}u \|_{L^{p'}(M)} \lesssim R^{\alpha(p) - 1/p'} \| b^{II} \|_{L^p(I_0)} \| u \|_{L^{p'}(M)},
    \end{equation}
    %
    which completes the proof.
\end{proof}

Combining Lemma \ref{regime1Lemma} and Lemma \ref{LocalSmoothingLargeTimesTheorem} completes the proof of Proposition \ref{TjbLemma}, and thus of inequality \eqref{dyadicMainReulst}. Since \eqref{TrivialLowFrequencyBound} was already proven as a consequence of Lemma \ref{lowjLemma}, this completes the proof of Theorem \ref{CpVersionOfTheorem}, and thus the main results of the paper.

\section{Appendix}

In this appendix, we provide proofs of Lemmas \ref{decompositionLemma} and \ref{pseudodifferentialCoordinateLemma}.

%\setcounter{theorem}{5}

\begin{proof} [Proof of Lemma \ref{decompositionLemma}]
    The intervals $\{ I_{t_0} : t_0 \in \mathcal{T}_R \}$ cover $[-\varepsilon,\varepsilon]$, and so we may consider an associated partition $\mathbb{I}_{[-\varepsilon,\varepsilon]} = \sum_{t_0} \chi_{t_0}$ where $\text{supp}(\chi_{t_0}) \subset I_{t_0}$ and $|\chi_{t_0}| \leq 1$. Define $b_{t_0}^I = \chi_{t_0} b_j$ and $b^{II} = (1 - \mathbb{I}_{[-\varepsilon,\varepsilon]} ) b$. Then $b = \sum_{t_0} b_{t_0}^I + b^{II}$, and the support assumptions are satisfied. It remains to prove the required norm bounds for these choices. For each $n \in \ZZ$, define a function $b_{n}: I_0 \to \CC$ by setting $b_{n}(t) = R \widehat{m}(R (t + n))$. Then $b = \sum_n b_{n}$. Moreover,
    %
    \begin{align} \label{translationlpcalculation}
    \begin{split}
        &\left( \sum_{n \neq 0} \left[ \langle R n \rangle^{\alpha(p)} \| b_{n} \|_{L^p(I_0)} \right]^p \right)^{1/p}\\
        %&\quad\quad\quad \sim \left( \int_{-1/2}^{1/2} \sum_{n \neq 0} \left[ \langle R(t + n) \rangle^{\alpha(p)} |R \widehat{m}(R ( t + n ))| \right]^p\; dt \right)^{1/p}\\
        &\quad\quad\quad \sim \left( \int_{|t| \geq 1/2} \left[ \langle R t \rangle^{\alpha(p)} |R \widehat{m}(R t)| \right]^p \right)^{1/p}\\
        &\quad\quad\quad = R^{1/p'} \left( \int_{|t| \geq R/2} \left[ |t|^{\alpha(p)} \widehat{m}(t) \right]^p \right)^{1/p} \leq R^{1/p'} C_p(m).
    \end{split}
    \end{align}
    %
    Write $b_{t_0}^I = \sum_n b_{t_0,n}^I$ and $b^{II} = \sum_n b_{n}^{II}$, where $b_{t_0,n}^I = \chi_{t_0} b_n$ and $b_n^{II} = \mathbb{I}_{I_0 \smallsetminus [-\varepsilon, \varepsilon] } b_{n}$. Then
    %
    \begin{align} \label{zerobj0iicalculation}
    \begin{split}
        \| b_{0}^{II} \|_{L^p(I_0)} &= \left( \int_{\varepsilon \leq |t| \leq 1/2} |R \widehat{m}(R t)|^p \right)^{1/p}\\
        &= R^{1/p'} \left( \int_{R \varepsilon \leq |t| \leq R/2} |\widehat{m}(t)|^p \right)^{1/p} \lesssim R^{1/p' - \alpha(p)} C_p(m).
    \end{split}
    \end{align}
    %
    Using \eqref{translationlpcalculation}, \eqref{zerobj0iicalculation}, and H\"{o}lder's inequality, we conclude that
    %
    \begin{align}
    \begin{split}
        \|  b^{II} \|_{L^p(I_0)} &\leq \sum\nolimits_n \| b_{n}^{II} \|_{L^p(I_0)}\\
        &\leq \| b_{0}^{II} \|_{L^p(I_0)} + \sum\nolimits_{n \neq 0} \left[ |R n|^{\alpha(p)} \| b_{n}^{II} \|_{L^p(I_0)} \right] \frac{1}{|R n|^{\alpha(p)}}\\
        &\leq \| b_{0}^{II} \|_{L^p(I_0)} + R^{-\alpha(p)} \left( \sum\nolimits_{n \neq 0} \left[ |R n|^{\alpha(p)} \| b_{n} \|_{L^p(I_0)} \right]^p \right)^{1/p}\\ % \left( \sum_{n \neq 0} \frac{1}{|R n |^{\alpha(p) p'}} \right)^{1/p'}\\
        &\lesssim R^{1/p' - \alpha(p)} C_p(m).
    \end{split}
    \end{align}
    %
    A similar calculation shows that
    %
    \begin{align} \label{eacht0bjcalculation}
    \begin{split}
        \| b_{t_0}^I \|_{L^p(I_0)} &\leq \sum\nolimits_n \| b_{t_0,n}^I \|_{L^p(I_0)}\\
        &= \| b_{t_0,0}^I \|_{L^p(I_0)} + \sum\nolimits_{n \neq 0} \| b_{t_0,n}^I \|_{L^p(I_0)}\\
        &\lesssim \| b_{t_0,0}^I \|_{L^p(I_0)} + R^{-\alpha(p)} \Big( \sum\nolimits_{n \neq 0} |R n|^{\alpha(p)} \| b_{t_0,n}^I \|_{L^p(I_0)}^p \Big)^{1/p}\\
        &\lesssim \| b_{t_0,0}^I \|_{L^p(I_0)} + \Big( \sum\nolimits_{n \neq 0} |R n|^{\alpha(p)} \| b_{t_0,n}^I \|_{L^p(I_0)}^p \Big)^{1/p}.
    \end{split}
    \end{align}
    %
    Using \eqref{eacht0bjcalculation}, we calculate that
    %
    \begin{align} \label{bjt0Icalculation}
    \begin{split}
        &\left( \sum_{t_0 \in \mathcal{T}_R} \left[ \| b_{t_0}^I \|_{L^p(I_0)} \langle R t_0 \rangle^{\alpha(p)} \right]^p \right)^{1/p}\\
        &\quad\quad \lesssim \left( \sum_{t_0 \in \mathcal{T}_R} \left[ \| b_{t_0,0}^I \|_{L^p(I_0)} \langle R t_0 \rangle^{\alpha(p)} \right]^p + \sum_{n \neq 0} \left[ |R n|^{\alpha(p)} \| b_{t_0,n}^I \|_{L^p(I_0)} \right]^p \right)^{1/p}\\
        &\quad\quad \lesssim \left( \int_{\RR} \left[ \langle R t \rangle^{\alpha(p)} R \widehat{m}(R t) \right]^p dt \right)^{1/p}\\
        &\quad\quad \lesssim R^{1/p'} C_p(m).
    \end{split}
    \end{align}
    %
    Since each function $b_{t_0}^I$ is supported on a length $1/R$ interval, we have
    %
    \begin{equation}
        \| b_{t_0}^I \|_{L^1(I_0)} \lesssim R^{-1/p'} \| b_{t_0}^I \|_{L^p(I_0)},
    \end{equation}
    %
    and substituting this inequality into \eqref{bjt0Icalculation} completes the proof.
\end{proof}

%\setcounter{theorem}{8}

\begin{proof} [Proof of Lemma \ref{pseudodifferentialCoordinateLemma}]
    For each $\alpha$, given our choice of $\varepsilon_M$, the Lax-H\"{o}rmander Parametrix construction (see Theorem 4.1.2 of \cite{Sogge}) guarantees that we can find operators $\tilde{W}_\alpha(t)$ and $\tilde{R}_\alpha(t)$ for $|t| \leq \varepsilon_M$, such that for $u \in L^1(M)$ with $\text{supp}(u) \subset V_\alpha^*$,
    %
    \begin{equation}
        e^{2 \pi i t P} u = \tilde{W}_\alpha(t) u + \tilde{R}_\alpha(t) u,
    \end{equation}
    %
    where $\tilde{R}_\alpha(t)$ has a smooth kernel, and the kernel of $\tilde{W}_\alpha(t)$ is given in coordinates by
    %
    \begin{equation}
        \tilde{W}_\alpha(t)(x,y) = \int s_0(t,x,y,\xi) e^{2 \pi i [ \phi(x,y,\xi) + t p(y,\xi) ]}\; d\xi,
    \end{equation}
    %
    for an order zero symbol $s_0$ with
    %
    \begin{equation}
        \text{supp}_{x,y,\xi}(s_0) \subset \{ (x,y,\xi) \in U_\alpha \times V_\alpha^* \times \RR^d : d_M(x,y) \leq 1.01\varepsilon_M\ \text{and}\ |\xi| \leq 1 \},
    \end{equation}
    %
    and an order one symbol $\phi$, homogeneous in $\xi$ of order one, solving the Eikonal equation and vanishing for $x \in \Sigma_\alpha(y,\xi)$ as required by the lemma. The only difference here compared to Theorem 4.1.2 of \cite{Sogge} is that in that construction the function $\phi$ there is chosen to vanish for $x \in \tilde{\Sigma}_\alpha(y,\xi)$, where $\tilde{\Sigma}_\alpha(y,\xi) = \{ x : \xi \cdot (x - y) = 0 \}$. The only property of this choice that is used in the proof is that the perpendicular vector to $\tilde{\Sigma}_\alpha(y,\xi)$ at $y$ is $\xi$, and this is also true of the hypersurfaces $\Sigma_\alpha(y,\xi)$ that we have specified, so that there is no problem making this modification.

    In the remainder of the proof it will be convenient to fix a orthonormal basis $\{ e_k \}$ of eigenfunctions for $P$, such that $\Delta e_k = \lambda_k e_k$ for a non-decreasing sequence $\{ \lambda_k \}$. We fix $u \in L^1(M)$ with $\text{supp}(u) \subset V_\alpha^*$ and $\| u \|_{L^1(M)} \leq 1$.

    We begin by mollifying the functions $Q$. We proceed here with a similar approach to Theorem 4.3.1 of \cite{Sogge}. We fix $\rho \in C_c^\infty(\RR)$ equal to one in a neighborhood of the origin and with $\rho(t) = 0$ for $|t| \geq \varepsilon_M / 2$. We write
    %
    \begin{equation}
    \begin{split}
        Q &= \int R \widehat{q}(R t) e^{2 \pi i t P}\; dt\\
        &= \int R \widehat{q}(Rt) \Big\{ \rho(t) \tilde{W}(t) + \rho(t) \tilde{R}(t) + (1 - \rho(t)) e^{2 \pi i t P} \Big\}\; dt\\
        &= Q_I + Q_{II} + Q_{III}.
    \end{split}
    \end{equation}
    %
    The rapid decay of $\widehat{q}$ implies that the function $\psi(t) = R \widehat{q}(Rt) (1 - \rho(t))$ satisfies $\| \partial_t^N \psi \|_{L^1(\RR)} \lesssim_M R^{-M}$, and so
    %
    \begin{equation} \label{psidecaybound}
        |\widehat{\psi}(\lambda)| \lesssim_{N,M} R^{-M} \lambda^{-N}.
    \end{equation}
    %
    But since $Q_{III} = \widehat{\psi}(-P)$, we can write the kernel of $Q_{III}$ as
    %
    \begin{equation}
        Q_{III}(x,y) = \sum\nolimits_\lambda \widehat{\psi}(-\lambda_k) e_k(x) \overline{e_k(y)},
    \end{equation}
    %
    Sobolev embedding and \eqref{psidecaybound} imply that $|Q_{III}(x,y)| \lesssim_N R^{-N}$ and thus
    %
    \begin{equation} \label{QThreeBound}
        \| Q_{III} u \|_{L^\infty(M)} \lesssim_N R^{-N}.
    \end{equation}
    %
    Integration by parts, using the fact that $q$ vanishes near the origin, yields that
    %
    \begin{equation}
        \left| \int R \widehat{q}(Rt) \rho(t) \tilde{R}(t,x,y) \right| \lesssim_N R^{-N},
    \end{equation}
    %
    and thus
    %
    \begin{equation} \label{QTwoBound}
        \| Q_{II} u \|_{L^\infty(M)} \lesssim_N R^{-N}.
    \end{equation}
    %
    Now we expand
    %
    \begin{equation}
        Q_I = \iint R \widehat{q}(Rt) \rho(t) s_0(t,x,y,\xi) e^{2 \pi i [ \phi(x,y,\xi) + t p(y,\xi) ]}\; d\xi\; dt.
    \end{equation}
    %
    We perform a Fourier series expansion, writing
    %
    \begin{equation} c_n(x,y,\xi) = \int \rho(t) s_0(t,x,y,\xi) e^{-2 \pi i n t}\; dt. \end{equation}
    %
    Then the symbol estimates for $s_0$, and the compact support of $\rho$ imply that
    %
    \begin{equation} |\partial_{x,y}^\alpha \partial_\xi^\beta c_n(x,y,\xi)| \lesssim_{\alpha,\beta,N} |n|^{-N} \langle \xi \rangle^{-\beta}. \end{equation}
    %
    Using Fourier inversion we can write
    %
    \begin{equation}
    \begin{split}
        Q_I(x,y) &= \iint \sum\nolimits_n R \widehat{q}(Rt) c_n(x,y,\xi) e^{2 \pi i [ \phi(x,y,\xi) + t [ n + p(y,\xi) ] ]}\; d\xi\; dt\\
        &= \int \sum\nolimits_n q \Big( \big( n + p(y,\xi) \big) / R \Big) c_n(x,y,\xi) e^{2 \pi i \phi(x,y,\xi)}\; d\xi\\
        &= \int \tilde{\sigma}_\alpha(x,y,\xi) e^{2 \pi i \phi(x,y,\xi)}\; d\xi, 
    \end{split}
    \end{equation}
    %
    where
    %
    \begin{equation} \tilde{\sigma}_\alpha(x,y,\xi) = \sum_{n \in \ZZ} q \left( \frac{n + p(y,\xi)}{R} \right) c_n(x,y,\xi). \end{equation}
    %
    The $n$th term of this sum is supported on $R/4 - n \leq p(y,\xi) \leq 4R - n$, so in particular, if $n > 4R$ then the term vanishes. For $n \leq 4R$, we have estimates of the form
    %
    % Good range is R/4 - n >= R/8 and 4R - n <= 8R
    % so n <= R/8 and n >= -4R
    %
    \begin{equation} \left| \partial_{x,y}^\alpha \partial_\xi^\beta \left\{ q \left( \frac{n + p(y,\xi)}{R} \right) c_n(x,y,\xi) \right\} \right| \lesssim_{\alpha,\beta,N} |n|^{-N}. \end{equation}
    %
    and for $-4R \leq n \leq R/8$,
    %
    \begin{equation}
    \begin{split}
        \left| \partial_{x,y}^\alpha \partial_\xi^\beta \left\{ q \left( \frac{n + p(y,\xi)}{R} \right) c_n(x,y,\xi) \right\} \right| &\lesssim_{\alpha,\beta,N} |n|^{-N} R^{-\beta}.
    \end{split}
    \end{equation}
    %
    But this means that if we define
    %
    \begin{equation} \sigma_\alpha(x,y,\xi) = \sum\nolimits_{-4R \leq n \leq R/8} q \left( \frac{n + p(y,\xi)}{R} \right) c_n(x,y,\xi). \end{equation}
    %
    and define
    %
    \begin{equation} Q_\alpha(x,y) = \int \sigma_\alpha(x,y,\xi) e^{2 \pi i \phi(x,y,\xi)}\; d\xi \end{equation}
    %
    then
    %
    \begin{equation} \Big|\partial_{x,y}^\alpha \partial_\xi^\beta \big\{ \tilde{\sigma}_\alpha - \sigma \big\}(x,y,\xi) \Big| \lesssim_{\alpha,\beta,N,M} R^{-N} \langle \xi \rangle^{-M}, \end{equation}
    %
    and so
    %
    \begin{equation} \label{QalphaApproximation}
        \| ( Q_I - Q_\alpha ) u \|_{L^\infty(M)} \lesssim_N R^{-N}.
    \end{equation}
    %
    Combining \eqref{QThreeBound}, \eqref{QTwoBound}, and \eqref{QalphaApproximation}, we conclude that
    %
    \begin{equation} \label{QApproximationTheorem}
        \| (Q - Q_\alpha) u \|_{L^\infty(M)} \lesssim_N R^{-N}.
    \end{equation}
    %
    Since $\sigma_\alpha$ is supported on $|\xi| \sim R$, we have verified the required properties of $Q_\alpha$.

    Using the bounds on $Q - Q_\alpha$ obtained above, we see that
    %
    \begin{equation} \big\| [(Q \circ e^{2 \pi i t P} \circ Q) - (Q_\alpha \circ e^{2 \pi i t P} \circ Q_\alpha)] \{ u \} \big\|_{L^\infty(M)} \lesssim_N R^{-N}. \end{equation}
    %
    Since $\tilde{R}_\alpha(t)$ has a smooth kernel, we also see that
    %
    \begin{equation}
    \begin{split}
        &\big\| (Q_\alpha \circ (e^{2 \pi i t P} - \tilde{W}_\alpha(t)) \circ Q_\alpha) \{ u \} \big\|_{L^\infty(M)}\\
        &\quad\quad = \big\| (Q_\alpha \circ \tilde{R}_\alpha(t) \circ Q_\alpha) \{ u \} \big\|_{L^\infty(M)} \lesssim_N R^{-N}.
    \end{split}
    \end{equation}
    %
    We now write
    %
    \begin{equation}
    \begin{split}
        &(Q_\alpha \circ \tilde{W}_\alpha(T))(x,y)\\
        &\quad = \int \sigma_\alpha(x,\xi) s_0(t,z,y,\eta) e^{2 \pi i [\phi(x,z,\xi) + \phi(z,y,\eta) + t p(y,\eta) ]}\; d\xi\; d\eta\; dz.
    \end{split}
    \end{equation}
    %
    The phase of this equation has gradient in the $z$ variable with magnitude $\gtrsim R$ for $|\eta| \ll R$, and $\gtrsim R |\xi|$ if $|\eta| \gg R$. Thus, if we define $s(t,x,y,\xi) = s_0(t,x,y,\xi) \chi(\xi / R)$ where $\text{supp}(\chi) \subset [1/8,8]$, and then define
    %
    \begin{equation} W_\alpha(t)(x,y) = \int s(t,x,y,\xi) e^{2 \pi i [ \phi(x,y,\xi) + t p(y,\xi) ]}\; d\xi, \end{equation}
    %
    then we may integrate by parts in the $z$ variable to conclude that
    %
    \begin{equation} \Big|\big(Q_R \circ (\tilde{W}_\alpha(t) - W_\alpha(t)) \big)(x,y) \Big| \lesssim_N R^{-N}, \end{equation}
    %
    and thus
    %
    \begin{equation} \| (Q_R \circ (\tilde{W}_\alpha(t) - W_\alpha(t)) \circ Q_R) u \|_{L^\infty(M)} \lesssim R^{-N}. \end{equation}
    %
    This proves the required estimates for the operators $W_\alpha$.
\end{proof}

%Indeed, if we fix an arbitrary point $v_0 \in T_x^*M$, and consider the smallest closed ball $B \subset T_x^* M$ centered at $v_0$ and containing $S_x^*$, then the sphere $\partial B$ must share the same tangent plane as $S_x^*$ at some point. All principal curvatures of $\partial B$ are positive, and at this point all principal curvatures of $S_x^*$ must be greater than the principal curvatures of $\partial B$, since $S_x^*$ curves away faster than $\partial B$ in all directions. By continuity, we conclude that the principal curvatures are everywhere positive. 





\begin{comment}

\section{Necessary Conditions}

Let $m: [0,\infty) \to \CC$ be a function such that the spectral multiplier operators
%
\[ \{ m(\sqrt{-\Delta} / R) \} \]
%
are uniformly bounded on $L^p(S^d)$. Define $\psi: [0,\infty) \to [0,\infty)$ such that $\sqrt{-\Delta} = \psi(P)$ for an elliptic pseudodifferential operator $P$ commuting with $\sqrt{-\Delta}$, and with $\sigma(P) \subset \NN$. Define $m_R(\lambda) = m(\psi(\lambda)/R)$, so that 
%
\[ m(\sqrt{-\Delta} / R) = m_R(P). \]
%
If $\text{supp}(m) \subset [1/2,2]$, then $\text{supp}(m_R) \subset [R/4,4R]$ for suitably large $R$. If we define
%
\[ b_R(t) = \sum_{n = 0}^\infty m_R(n) \cos(2 \pi n t), \]
%
then we can write
%
\[ m_R(P) = 2 \int_0^{1/2} b_R(t) \cos(2 \pi P t)\; dt \]
%
Consider a normal coordinate system $\mathfrak{x}$ centered at a point $x_0 \in S^d$, and use it to define a family of bump functions $f_R: S^d \to \CC$ for $R \geq 1$ by setting $f_R = R^{d/p} f(R \mathfrak{x})$ for some fixed $f \in C_c^\infty(\RR^d)$ supported in a small neighbourhood of the origin. If we consider a partition of unity $\chi_0, \chi_1, \chi_2$ subordinate to the cover $[0,10/R)$, $(5/R, 1/2 - 5/R)$, $(1/2 - 10/R, 1/2]$, then we can write
%
\[ m_R(P) \{ f_R \} = 2(g_{R,0} + g_{R,1} + g_{R,2} + g_{R,\infty}) \]
%
where
%
\[ g_{R,j} = \int_0^{1/2} \chi_j(t) b_R(t) C_j(t) \{ f_R \}\; dt \]
%
and
%
\[ g_{R,\infty} = \int_0^{1/2} b_R(t) C_\infty(t) \{ f_R \}\; dt, \]
%
where $C_j(t)$ is a parametrix for $\cos(2 \pi P t)$, and $C_\infty(t)$ is an operator with a smooth kernel in $C^\infty([0,1/2] \times S^d \times S^d)$. Now applying Fourier series, we can write
%
\[ g_{R,\infty} = \sum_{n = 0}^\infty m_R(n) \widehat{C}_\infty(n) \{ f_R \}\; dt, \]
%
where $\widehat{C}_\infty(n)$ is rapidly decaying in $n$. But since $m_R(n)$ is supported on $[R/4,4R]$, we conclude that
%
\[ |\widehat{C}_\infty(n) \{ f_R \}| \lesssim_N R^{-N} |n|^{-10d}, \]
%
and thus
%
\[ \| g_{R,\infty} \|_{L^\infty(S^d)} \lesssim_N R^{-N} \| m \|_{L^\infty}\quad\text{for all $N \geq 0$}. \]
%
One can show (via a Fourier inversion, interpolation, and pseudodifferential operator argument) that for $x \in S^d$ with $d(x,x_0) \geq C_0 / R$,
%
\[ |g_{R,0}(x)| \lesssim_N \| m \|_{L^\infty} R^{d/p} |R d(x,x_0) |^{-d-N} \quad\text{for all $N \geq 0$}. \]
%
Similarily, if $x_0^* \in S^d$ is the antipode of $x_0$, then for $d(x,x_0^*) \geq C_0 / R$,
%
\[ |g_{R,2}(x)| \lesssim_N \| m \|_{L^\infty} R^{d/p} |R d(x,x_0^*) |^{-d-N} \quad\text{for all $N \geq 0$}. \]
%
If we let $\Omega = \{ x \in S^d: d(x,x_0) \geq C_0 / R\ \text{and}\ d(x,x_0^*) \geq C_0 / R \}$, then
%
\[ \| g_{R,0} \|_{L^p(\Omega)} + \| g_{R,2} \|_{L^p(\Omega)} \lesssim \| m \|_{L^\infty} R^{-d/p^*} \]
%
Now we introduce the Hadamard parametrix, writing
%
% \[ C_1(t,x,y) = A_d\; t \sum_{j = 0}^\infty \frac{( -1 )^j}{4^j \Gamma(j - \frac{d-1}{2})} W_j(x,y) ( d(x,y)^2 - t^2 )^{j - \frac{d+1}{2}} \]
\[ C_1(t,x,y) = t \int_0^\infty a(x,y,\tau) e^{i \tau ( d(x,y)^2 - t^2 )}\; d\tau, \]
%
where $a$ is a symbol of order $(d-1)/2$ in the $\tau$ variable, whose principal symbol is a power of the volume density on the manifold $M$. In particular, $a(x,y,\tau) > 0$ for sufficiently large $\tau$. We consider a partition of unity $\tilde{\chi}_0, \tilde{\chi}_1, \tilde{\chi}_2$ subordinate to $[0,1/5), (1/10, 10), (5,\infty)$ and use this parition to partition the regions of integration in the $\tau$ variable, writing $g_{R,1} = g_{R,1,0} + g_{R,1,1} + g_{R,1,2}$, where
%
\[ g_{R,1,j}(x) = \int \int a(x,y,\tau) \tilde{\chi}_j(\tau / R) b_R(t) t e^{i \tau ( d(x,y)^2 - t^2 ) } f_R(y)\; d\tau\; dt\; dy. \]
%
Integration by parts in the $t$ variable shows that for $j \in \{ 0, 2 \}$ $\| g_{R,1,j} \|_{L^\infty} \lesssim_N R^{-N} \| m \|_{L^\infty}$. Rescaling, writing $a_R(x,y,\tau) = R^{- \frac{d-1}{2}} a(x,y,R \tau) \tilde{\chi}_j(\tau)$ we can write
%
\begin{align*}
    g_{R,1,1}(x) &= R^{\frac{d+1}{2} + \frac{d}{p}} \iiint a_R(x,y,\tau) b_R(t) t e^{i R \tau ( d(x,y)^2 - t^2 )} f(Ry)\; d\tau\; dt\; dy.
%    &= R^{\frac{d+1}{2} + \frac{d}{p}} \iint (\mathcal{F}_\tau a_R)(x,y,R [ d(x,y)^2 - t^2 ]) b_R(t) t f(Ry)\; dt\; dy
\end{align*}
%
For our purposes (TODO: Justify Later) we may assume $d(x,y) \approx d(x,0) + y \cdot x / |x|$, and that $d(x,y)^2 \approx d(x,0)^2 + 2 d(x,0)  (y \cdot x / |x|)$. Thus
%
\[ g_{R,1,1}(x) \approx R^{\frac{d+1}{2} - \frac{d}{p'}} \iint a_R(x,\tau) b_R(t) t e^{i R \tau [d(x,0)^2 - t^2]}. \]
%
Thus
%
\[ g_{R,1,1}(x) \approx R^{\frac{d+1}{2} - \frac{d}{p'}} \int \widehat{a}_R \big(x, R[d(x,0)^2 - t^2] \big) b_R(t) t\; dt \]

\end{comment}